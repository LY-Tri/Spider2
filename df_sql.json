[
    {
        "instance_id":"sf_bq091",
        "instruction":"In which year did the assignee with the most applications in the patent category 'A61' file the most?",
        "db_id":"PATENTS",
        "external_knowledge":null,
        "sql":"WITH a61_applications AS (\n  SELECT DISTINCT p.\"application_number\"\n  FROM \"PATENTS\".\"PATENTS\".\"PUBLICATIONS\" p, LATERAL FLATTEN(input => p.\"cpc\") c\n  WHERE p.\"filing_date\" IS NOT NULL\n    AND p.\"filing_date\" > 0\n    AND c.value:\"code\"::string LIKE 'A61%'\n    AND p.\"application_number\" IS NOT NULL\n  UNION\n  SELECT DISTINCT p.\"application_number\"\n  FROM \"PATENTS\".\"PATENTS\".\"PUBLICATIONS\" p, LATERAL FLATTEN(input => p.\"ipc\") i\n  WHERE p.\"filing_date\" IS NOT NULL\n    AND p.\"filing_date\" > 0\n    AND i.value:\"code\"::string LIKE 'A61%'\n    AND p.\"application_number\" IS NOT NULL\n),\napps_with_meta AS (\n  SELECT\n    p.\"application_number\",\n    CAST(FLOOR(p.\"filing_date\" \/ 10000) AS INTEGER) AS \"filing_year\",\n    p.\"assignee_harmonized\",\n    p.\"assignee\"\n  FROM \"PATENTS\".\"PATENTS\".\"PUBLICATIONS\" p\n  JOIN a61_applications a ON a.\"application_number\" = p.\"application_number\"\n),\nassignee_apps AS (\n  SELECT\n    UPPER(TRIM(ah.value:\"name\"::string)) AS \"assignee_name\",\n    awm.\"application_number\",\n    awm.\"filing_year\"\n  FROM apps_with_meta awm,\n       LATERAL FLATTEN(input => awm.\"assignee_harmonized\") ah\n  WHERE ah.value:\"name\" IS NOT NULL\n\n  UNION ALL\n\n  SELECT\n    UPPER(TRIM(a.value::string)) AS \"assignee_name\",\n    awm.\"application_number\",\n    awm.\"filing_year\"\n  FROM apps_with_meta awm,\n       LATERAL FLATTEN(input => awm.\"assignee\") a\n  WHERE (awm.\"assignee_harmonized\" IS NULL OR ARRAY_SIZE(awm.\"assignee_harmonized\") = 0)\n    AND a.value IS NOT NULL\n),\nassignee_totals AS (\n  SELECT\n    \"assignee_name\",\n    COUNT(DISTINCT \"application_number\") AS \"total_apps\"\n  FROM assignee_apps\n  GROUP BY \"assignee_name\"\n),\ntop_assignee AS (\n  SELECT \"assignee_name\"\n  FROM assignee_totals\n  QUALIFY ROW_NUMBER() OVER (ORDER BY \"total_apps\" DESC, \"assignee_name\" ASC) = 1\n),\nyear_counts AS (\n  SELECT\n    aa.\"filing_year\" AS \"year\",\n    COUNT(DISTINCT aa.\"application_number\") AS \"cnt\"\n  FROM assignee_apps aa\n  JOIN top_assignee ta ON aa.\"assignee_name\" = ta.\"assignee_name\"\n  GROUP BY aa.\"filing_year\"\n)\nSELECT \"year\"\nFROM year_counts\nQUALIFY ROW_NUMBER() OVER (ORDER BY \"cnt\" DESC, \"year\" ASC) = 1;"
    },
    {
        "instance_id":"sf_bq099",
        "instruction":"For patent class A01B3, I want to analyze the information of the top 3 assignees based on the total number of applications. Please provide the following five pieces of information: the name of this assignee,  total number of applications, the year with the most applications, the number of applications in that year, and the country code with the most applications during that year.",
        "db_id":"PATENTS",
        "external_knowledge":null,
        "sql":"WITH \"base\" AS (\n  SELECT DISTINCT\n    UPPER(\"fa\".\"VALUE\":\"name\"::string) AS \"assignee_name\",\n    \"p\".\"application_number\" AS \"application_number\",\n    CASE WHEN \"p\".\"filing_date\" IS NOT NULL AND \"p\".\"filing_date\" > 0 THEN FLOOR(\"p\".\"filing_date\"\/10000) END AS \"filing_year\",\n    \"p\".\"country_code\" AS \"country_code\"\n  FROM \"PATENTS\".\"PATENTS\".\"PUBLICATIONS\" AS \"p\",\n       LATERAL FLATTEN(input => \"p\".\"cpc\") AS \"fc\",\n       LATERAL FLATTEN(input => \"p\".\"assignee_harmonized\") AS \"fa\"\n  WHERE UPPER(\"fc\".\"VALUE\":\"code\"::string) LIKE 'A01B3%'\n),\n\"assignee_totals\" AS (\n  SELECT\n    \"assignee_name\",\n    COUNT(DISTINCT \"application_number\") AS \"total_applications\"\n  FROM \"base\"\n  GROUP BY \"assignee_name\"\n),\n\"per_year\" AS (\n  SELECT\n    \"assignee_name\",\n    \"filing_year\",\n    COUNT(DISTINCT \"application_number\") AS \"apps_in_year\"\n  FROM \"base\"\n  WHERE \"filing_year\" IS NOT NULL\n  GROUP BY \"assignee_name\", \"filing_year\"\n),\n\"top_year\" AS (\n  SELECT\n    \"assignee_name\",\n    \"filing_year\" AS \"top_year\",\n    \"apps_in_year\"\n  FROM (\n    SELECT\n      \"assignee_name\",\n      \"filing_year\",\n      \"apps_in_year\",\n      ROW_NUMBER() OVER (PARTITION BY \"assignee_name\" ORDER BY \"apps_in_year\" DESC, \"filing_year\" ASC) AS \"rn\"\n    FROM \"per_year\"\n  )\n  WHERE \"rn\" = 1\n),\n\"per_country_year\" AS (\n  SELECT\n    \"assignee_name\",\n    \"filing_year\",\n    \"country_code\",\n    COUNT(DISTINCT \"application_number\") AS \"apps_in_year_country\"\n  FROM \"base\"\n  WHERE \"filing_year\" IS NOT NULL\n  GROUP BY \"assignee_name\", \"filing_year\", \"country_code\"\n),\n\"top_country\" AS (\n  SELECT\n    \"pc\".\"assignee_name\",\n    \"pc\".\"filing_year\",\n    \"pc\".\"country_code\" AS \"top_country_code\",\n    \"pc\".\"apps_in_year_country\"\n  FROM (\n    SELECT\n      \"assignee_name\",\n      \"filing_year\",\n      \"country_code\",\n      \"apps_in_year_country\",\n      ROW_NUMBER() OVER (PARTITION BY \"assignee_name\", \"filing_year\" ORDER BY \"apps_in_year_country\" DESC, \"country_code\" ASC) AS \"rn\"\n    FROM \"per_country_year\"\n  ) AS \"pc\"\n  WHERE \"pc\".\"rn\" = 1\n),\n\"ranked_assignees\" AS (\n  SELECT\n    \"assignee_name\",\n    \"total_applications\",\n    ROW_NUMBER() OVER (ORDER BY \"total_applications\" DESC, \"assignee_name\" ASC) AS \"assignee_rank\"\n  FROM \"assignee_totals\"\n)\nSELECT\n  \"r\".\"assignee_name\",\n  \"r\".\"total_applications\",\n  \"y\".\"top_year\",\n  \"y\".\"apps_in_year\" AS \"applications_in_top_year\",\n  \"c\".\"top_country_code\"\nFROM \"ranked_assignees\" AS \"r\"\nJOIN \"top_year\" AS \"y\"\n  ON \"r\".\"assignee_name\" = \"y\".\"assignee_name\"\nLEFT JOIN \"top_country\" AS \"c\"\n  ON \"c\".\"assignee_name\" = \"y\".\"assignee_name\"\n AND \"c\".\"filing_year\" = \"y\".\"top_year\"\nWHERE \"r\".\"assignee_rank\" <= 3\nORDER BY \"r\".\"total_applications\" DESC, \"r\".\"assignee_name\" ASC;"
    },
    {
        "instance_id":"sf_bq033",
        "instruction":"How many U.S. publications related to IoT (where the abstract includes the phrase 'internet of things') were filed each month from 2008 to 2022, including months with no filings?",
        "db_id":"PATENTS",
        "external_knowledge":null,
        "sql":"WITH date_series AS (\n  SELECT\n    DATE_FROM_PARTS(2008, 1, 1) AS month_start\n  UNION ALL\n  SELECT\n    DATEADD(MONTH, 1, month_start)\n  FROM date_series\n  WHERE month_start < '2022-12-01'\n),\nfiltered_data AS (\n  SELECT\n    TO_CHAR(TRY_TO_DATE(TO_VARCHAR(\"filing_date\"), 'YYYYMMDD'), 'YYYYMM') AS year_month,\n    \"publication_number\"\n  FROM\n    \"PATENTS\".\"PUBLICATIONS\",\n    LATERAL FLATTEN(\"abstract_localized\") abs\n  WHERE\n    \"country_code\" = 'US'\n    AND LOWER(abs.value:\"text\"::STRING) LIKE '%internet of things%'\n    AND \"filing_date\" BETWEEN 20080101 AND 20221231\n    AND \"filing_date\" != 0\n  GROUP BY year_month, \"publication_number\"\n),\nmonthly_counts AS (\n  SELECT\n    year_month,\n    COUNT(\"publication_number\") AS application_count\n  FROM filtered_data\n  GROUP BY year_month\n)\nSELECT\n  TO_CHAR(ds.month_start, 'YYYYMM') AS \"PATENT_DATE_YEARMONTH\",\n  COALESCE(mc.application_count, 0) AS \"NUMBER_OF_PATENT_APPLICATIONS\"\nFROM date_series ds\nLEFT JOIN monthly_counts mc\n  ON TO_CHAR(ds.month_start, 'YYYYMM') = mc.year_month\nORDER BY \"PATENT_DATE_YEARMONTH\""
    },
    {
        "instance_id":"sf_bq209",
        "instruction":"Can you calculate the number of utility patents that were granted in 2010 and have exactly one forward citation within a 10-year window following their application\/filing date? For this analysis, forward citations should be counted as distinct citing application numbers that cited the patent within 10 years after the patent's own filing date.",
        "db_id":"PATENTS",
        "external_knowledge":null,
        "sql":"WITH temp_view_2 AS (\nSELECT \"publication_number\", \"application_number\", \"filing_date\"\nFROM \"PATENTS\".\"PATENTS\".\"PUBLICATIONS\"\nWHERE \"application_kind\" = 'A'\nAND \"grant_date\" >= 20100101\nAND \"grant_date\" <= 20101231\n),\ncitations_parsed AS (\nSELECT \n    p.\"publication_number\" AS citing_patent,\n    c.value:publication_number::STRING AS cited_patent\nFROM \"PATENTS\".\"PATENTS\".\"PUBLICATIONS\" p,\nLATERAL FLATTEN(input => p.\"citation\") c\nWHERE p.\"citation\" IS NOT NULL\nAND c.value:publication_number::STRING IS NOT NULL\nAND c.value:publication_number::STRING != ''\n),\ntemp_view_6 AS (\nSELECT \n    cp.citing_patent,\n    cp.cited_patent,\n    tv2.\"application_number\",\n    tv2.\"filing_date\"\nFROM citations_parsed cp\nINNER JOIN temp_view_2 tv2 ON cp.cited_patent = tv2.\"publication_number\"\n),\ntemp_view_7 AS (\nSELECT \n    tv6.cited_patent,\n    COUNT(DISTINCT p_citing.\"application_number\") AS distinct_citing_applications\nFROM temp_view_6 tv6\nINNER JOIN \"PATENTS\".\"PATENTS\".\"PUBLICATIONS\" p_citing ON tv6.citing_patent = p_citing.\"publication_number\"\nWHERE p_citing.\"filing_date\" >= tv6.\"filing_date\"\nAND p_citing.\"filing_date\" <= tv6.\"filing_date\" + 100000\nGROUP BY tv6.cited_patent\n)\nSELECT COUNT(*) AS patents_with_exactly_one_citation\nFROM temp_view_7\nWHERE distinct_citing_applications = 1"
    },
    {
        "instance_id":"sf_bq210",
        "instruction":"How many US B2 patents granted between 2008 and 2018 contain claims that do not include the word 'claim'?",
        "db_id":"PATENTS",
        "external_knowledge":null,
        "sql":"SELECT COUNT(*) AS \"num_patents\"\nFROM (\n  SELECT p.\"publication_number\"\n  FROM \"PATENTS\".\"PATENTS\".\"PUBLICATIONS\" AS p,\n       LATERAL FLATTEN(input => p.\"claims_localized\") AS cl\n  WHERE p.\"country_code\" = 'US'\n    AND p.\"kind_code\" = 'B2'\n    AND p.\"grant_date\" BETWEEN 20080101 AND 20181231\n  GROUP BY p.\"publication_number\"\n  HAVING SUM(CASE WHEN REGEXP_LIKE(cl.value:\"text\"::string, '\\\\bclaim\\\\b', 'i') THEN 1 ELSE 0 END) = 0\n);"
    },
    {
        "instance_id":"sf_bq213",
        "instruction":"What is the most common 4-digit IPC code among US B2 utility patents granted from June to August in 2022?",
        "db_id":"PATENTS",
        "external_knowledge":"patents_info.md",
        "sql":"WITH \"filtered\" AS (\n  SELECT p.\"publication_number\", p.\"ipc\"\n  FROM \"PATENTS\".\"PATENTS\".\"PUBLICATIONS\" p\n  WHERE p.\"country_code\" = 'US'\n    AND p.\"kind_code\" = 'B2'\n    AND p.\"grant_date\" BETWEEN 20220601 AND 20220831\n),\n\"flat\" AS (\n  SELECT\n    fpub.\"publication_number\",\n    SUBSTRING(f.value:\"code\"::string, 1, 4) AS \"ipc4\",\n    CASE WHEN f.value:\"first\"::boolean = TRUE THEN 1 ELSE 0 END AS \"is_first\"\n  FROM \"filtered\" fpub,\n       LATERAL FLATTEN(input => fpub.\"ipc\") f\n  WHERE f.value:\"code\" IS NOT NULL\n    AND SUBSTRING(f.value:\"code\"::string, 1, 4) <> ''\n),\n\"agg\" AS (\n  SELECT\n    \"publication_number\",\n    \"ipc4\",\n    COUNT(*) AS \"cnt_all\",\n    MAX(\"is_first\") AS \"has_first\"\n  FROM \"flat\"\n  GROUP BY \"publication_number\", \"ipc4\"\n),\n\"ranked\" AS (\n  SELECT\n    \"publication_number\",\n    \"ipc4\",\n    ROW_NUMBER() OVER (\n      PARTITION BY \"publication_number\"\n      ORDER BY \"has_first\" DESC, \"cnt_all\" DESC, \"ipc4\" ASC\n    ) AS \"rn\"\n  FROM \"agg\"\n)\nSELECT\n  \"ipc4\" AS \"most_common_ipc4\",\n  COUNT(*) AS \"num_publications\"\nFROM \"ranked\"\nWHERE \"rn\" = 1\nGROUP BY \"ipc4\"\nORDER BY \"num_publications\" DESC, \"ipc4\" ASC\nLIMIT 1;"
    },
    {
        "instance_id":"sf_bq216",
        "instruction":"Identify the top five patents filed in the same year as `US-9741766-B2` that are most similar to it based on technological similarities. Please provide the publication numbers.",
        "db_id":"PATENTS_GOOGLE",
        "external_knowledge":"patents_info.md",
        "sql":"WITH target_embedding_flat AS (\n  SELECT\n    f.index,\n    f.value::FLOAT AS value\n  FROM \"PATENTS_GOOGLE\".\"PATENTS_GOOGLE\".\"ABS_AND_EMB\" AS t,\n  LATERAL FLATTEN(input => t.\"embedding_v1\") AS f\n  WHERE t.\"publication_number\" = 'US-9741766-B2'\n),\ncandidate_patents_with_year AS (\n  SELECT\n    p.\"publication_number\",\n    e.\"embedding_v1\"\n  FROM \"PATENTS_GOOGLE\".\"PATENTS_GOOGLE\".\"PUBLICATIONS\" AS p\n  JOIN \"PATENTS_GOOGLE\".\"PATENTS_GOOGLE\".\"ABS_AND_EMB\" AS e ON p.\"publication_number\" = e.\"publication_number\"\n  WHERE p.\"filing_date\" != 0\n    AND EXTRACT(YEAR FROM TO_DATE(CAST(p.\"filing_date\" AS VARCHAR), 'YYYYMMDD')) = 2016\n    AND p.\"publication_number\" != 'US-9741766-B2'\n),\ncandidate_embeddings_flat AS (\n  SELECT\n    c.\"publication_number\",\n    f.index,\n    f.value::FLOAT AS value\n  FROM candidate_patents_with_year AS c,\n  LATERAL FLATTEN(input => c.\"embedding_v1\") AS f\n)\nSELECT\n  c.\"publication_number\"\nFROM candidate_embeddings_flat AS c\nJOIN target_embedding_flat AS t ON c.index = t.index\nGROUP BY c.\"publication_number\"\nORDER BY SUM(c.value * t.value) DESC\nLIMIT 5"
    },
    {
        "instance_id":"sf_bq127",
        "instruction":"For each publication family whose earliest publication was first published in January 2015, please provide the earliest publication date, the distinct publication numbers, their country codes, the distinct CPC and IPC codes, distinct families (namely, the ids) that cite and are cited by this publication family. Please present all lists as comma-separated values, sorted alphabetically",
        "db_id":"PATENTS_GOOGLE",
        "external_knowledge":null,
        "sql":"WITH \"FAMS\" AS (\n  SELECT\n    \"family_id\",\n    MIN(\"publication_date\") AS \"earliest_publication_date_num\"\n  FROM \"PATENTS_GOOGLE\".\"PATENTS_GOOGLE\".\"PUBLICATIONS\"\n  WHERE \"family_id\" IS NOT NULL\n  GROUP BY \"family_id\"\n  HAVING MIN(\"publication_date\") BETWEEN 20150101 AND 20150131\n),\n\"FAMILY_PUBS\" AS (\n  SELECT\n    \"F\".\"family_id\",\n    \"F\".\"earliest_publication_date_num\",\n    TRIM(\"P\".\"publication_number\") AS \"publication_number\",\n    TRIM(\"P\".\"country_code\") AS \"country_code\",\n    \"P\".\"cpc\" AS \"cpc\",\n    \"P\".\"ipc\" AS \"ipc\",\n    \"P\".\"citation\" AS \"citation\"\n  FROM \"FAMS\" AS \"F\"\n  JOIN \"PATENTS_GOOGLE\".\"PATENTS_GOOGLE\".\"PUBLICATIONS\" AS \"P\"\n    ON \"P\".\"family_id\" = \"F\".\"family_id\"\n),\n\"PUBS_AGG\" AS (\n  SELECT\n    \"family_id\",\n    LISTAGG(DISTINCT \"publication_number\", ', ') WITHIN GROUP (ORDER BY \"publication_number\") AS \"publication_numbers\"\n  FROM \"FAMILY_PUBS\"\n  GROUP BY \"family_id\"\n),\n\"COUNTRIES_AGG\" AS (\n  SELECT\n    \"family_id\",\n    LISTAGG(DISTINCT \"country_code\", ', ') WITHIN GROUP (ORDER BY \"country_code\") AS \"country_codes\"\n  FROM \"FAMILY_PUBS\"\n  WHERE \"country_code\" IS NOT NULL AND \"country_code\" != ''\n  GROUP BY \"family_id\"\n),\n\"CPC_AGG\" AS (\n  SELECT\n    \"FP\".\"family_id\",\n    LISTAGG(DISTINCT TRIM(\"CPC_ITEM\".\"VALUE\":\"code\"::STRING), ', ')\n      WITHIN GROUP (ORDER BY TRIM(\"CPC_ITEM\".\"VALUE\":\"code\"::STRING)) AS \"cpc_codes\"\n  FROM \"FAMILY_PUBS\" AS \"FP\",\n       LATERAL FLATTEN(INPUT => \"FP\".\"cpc\") AS \"CPC_ITEM\"\n  WHERE \"CPC_ITEM\".\"VALUE\":\"code\" IS NOT NULL\n    AND TRIM(\"CPC_ITEM\".\"VALUE\":\"code\"::STRING) != ''\n  GROUP BY \"FP\".\"family_id\"\n),\n\"IPC_AGG\" AS (\n  SELECT\n    \"FP\".\"family_id\",\n    LISTAGG(DISTINCT TRIM(\"IPC_ITEM\".\"VALUE\":\"code\"::STRING), ', ')\n      WITHIN GROUP (ORDER BY TRIM(\"IPC_ITEM\".\"VALUE\":\"code\"::STRING)) AS \"ipc_codes\"\n  FROM \"FAMILY_PUBS\" AS \"FP\",\n       LATERAL FLATTEN(INPUT => \"FP\".\"ipc\") AS \"IPC_ITEM\"\n  WHERE \"IPC_ITEM\".\"VALUE\":\"code\" IS NOT NULL\n    AND TRIM(\"IPC_ITEM\".\"VALUE\":\"code\"::STRING) != ''\n  GROUP BY \"FP\".\"family_id\"\n),\n\"CITED_PUBS\" AS (\n  SELECT\n    \"FP\".\"family_id\" AS \"source_family_id\",\n    TRIM(\"CIT\".\"VALUE\":\"publication_number\"::STRING) AS \"cited_pubnum\"\n  FROM \"FAMILY_PUBS\" AS \"FP\",\n       LATERAL FLATTEN(INPUT => \"FP\".\"citation\") AS \"CIT\"\n  WHERE TRIM(\"CIT\".\"VALUE\":\"publication_number\"::STRING) IS NOT NULL\n    AND TRIM(\"CIT\".\"VALUE\":\"publication_number\"::STRING) != ''\n),\n\"CITED_FAMILIES\" AS (\n  SELECT DISTINCT\n    \"CP\".\"source_family_id\",\n    \"P_CITED\".\"family_id\" AS \"cited_family_id\"\n  FROM \"CITED_PUBS\" AS \"CP\"\n  JOIN \"PATENTS_GOOGLE\".\"PATENTS_GOOGLE\".\"PUBLICATIONS\" AS \"P_CITED\"\n    ON TRIM(\"P_CITED\".\"publication_number\") = \"CP\".\"cited_pubnum\"\n  WHERE \"P_CITED\".\"family_id\" IS NOT NULL\n    AND \"P_CITED\".\"family_id\" != \"CP\".\"source_family_id\"\n),\n\"CITED_FAMILIES_AGG\" AS (\n  SELECT\n    \"source_family_id\" AS \"family_id\",\n    LISTAGG(DISTINCT \"cited_family_id\", ', ')\n      WITHIN GROUP (ORDER BY \"cited_family_id\") AS \"families_cited\"\n  FROM \"CITED_FAMILIES\"\n  GROUP BY \"source_family_id\"\n),\n\"CITING_PUBS\" AS (\n  SELECT\n    \"FP\".\"family_id\" AS \"target_family_id\",\n    TRIM(\"CB\".\"VALUE\":\"publication_number\"::STRING) AS \"citing_pubnum\"\n  FROM \"FAMILY_PUBS\" AS \"FP\"\n  JOIN \"PATENTS_GOOGLE\".\"PATENTS_GOOGLE\".\"ABS_AND_EMB\" AS \"A\"\n    ON \"A\".\"publication_number\" = \"FP\".\"publication_number\",\n       LATERAL FLATTEN(INPUT => \"A\".\"cited_by\") AS \"CB\"\n  WHERE TRIM(\"CB\".\"VALUE\":\"publication_number\"::STRING) IS NOT NULL\n    AND TRIM(\"CB\".\"VALUE\":\"publication_number\"::STRING) != ''\n),\n\"CITING_FAMILIES\" AS (\n  SELECT DISTINCT\n    \"CP\".\"target_family_id\",\n    \"P_ALL\".\"family_id\" AS \"citing_family_id\"\n  FROM \"CITING_PUBS\" AS \"CP\"\n  JOIN \"PATENTS_GOOGLE\".\"PATENTS_GOOGLE\".\"PUBLICATIONS\" AS \"P_ALL\"\n    ON TRIM(\"P_ALL\".\"publication_number\") = \"CP\".\"citing_pubnum\"\n  WHERE \"P_ALL\".\"family_id\" IS NOT NULL\n    AND \"P_ALL\".\"family_id\" != \"CP\".\"target_family_id\"\n),\n\"CITING_FAMILIES_AGG\" AS (\n  SELECT\n    \"target_family_id\" AS \"family_id\",\n    LISTAGG(DISTINCT \"citing_family_id\", ', ')\n      WITHIN GROUP (ORDER BY \"citing_family_id\") AS \"families_citing\"\n  FROM \"CITING_FAMILIES\"\n  GROUP BY \"target_family_id\"\n)\nSELECT\n  \"F\".\"family_id\",\n  TO_DATE(\"F\".\"earliest_publication_date_num\"::STRING, 'YYYYMMDD') AS \"earliest_publication_date\",\n  COALESCE(\"PA\".\"publication_numbers\", '') AS \"publication_numbers\",\n  COALESCE(\"CA\".\"country_codes\", '') AS \"country_codes\",\n  COALESCE(\"CC\".\"cpc_codes\", '') AS \"cpc_codes\",\n  COALESCE(\"IC\".\"ipc_codes\", '') AS \"ipc_codes\",\n  COALESCE(\"CFA\".\"families_cited\", '') AS \"families_cited\",\n  COALESCE(\"CIA\".\"families_citing\", '') AS \"families_citing\"\nFROM \"FAMS\" AS \"F\"\nLEFT JOIN \"PUBS_AGG\" AS \"PA\" ON \"PA\".\"family_id\" = \"F\".\"family_id\"\nLEFT JOIN \"COUNTRIES_AGG\" AS \"CA\" ON \"CA\".\"family_id\" = \"F\".\"family_id\"\nLEFT JOIN \"CPC_AGG\" AS \"CC\" ON \"CC\".\"family_id\" = \"F\".\"family_id\"\nLEFT JOIN \"IPC_AGG\" AS \"IC\" ON \"IC\".\"family_id\" = \"F\".\"family_id\"\nLEFT JOIN \"CITED_FAMILIES_AGG\" AS \"CFA\" ON \"CFA\".\"family_id\" = \"F\".\"family_id\"\nLEFT JOIN \"CITING_FAMILIES_AGG\" AS \"CIA\" ON \"CIA\".\"family_id\" = \"F\".\"family_id\"\nORDER BY \"F\".\"family_id\";"
    },
    {
        "instance_id":"sf_bq222",
        "instruction":"Find the CPC technology areas in Germany that had the highest exponential moving average (smoothing factor 0.1) of patent filings per year, specifically for patents granted in December 2016. For each CPC group at level 4, show the full title, CPC group, and the year with the highest exponential moving average of patent filings.",
        "db_id":"PATENTS",
        "external_knowledge":"sliding_windows_calculation_cpc.md",
        "sql":"WITH december_2016_grants AS (\n    SELECT DISTINCT \n        SUBSTRING(f.value:\"code\"::STRING, 1, 4) AS cpc_level4\n    FROM \"PATENTS\".\"PATENTS\".\"PUBLICATIONS\" p,\n    LATERAL FLATTEN(input => p.\"cpc\") f\n    WHERE p.\"country_code\" = 'DE' \n    AND p.\"grant_date\" >= 20161201 \n    AND p.\"grant_date\" <= 20161231\n),\nall_german_patents AS (\n    SELECT \n        \"publication_number\",\n        \"filing_date\",\n        FLOOR(\"filing_date\" \/ 10000) AS filing_year,\n        f.value:\"code\"::STRING AS cpc_code,\n        SUBSTRING(cpc_code, 1, 4) AS cpc_level4\n    FROM \"PATENTS\".\"PATENTS\".\"PUBLICATIONS\" p,\n    LATERAL FLATTEN(input => p.\"cpc\") f\n    WHERE p.\"country_code\" = 'DE' \n    AND p.\"filing_date\" IS NOT NULL\n    AND p.\"filing_date\" > 0\n    AND cpc_level4 IN (SELECT cpc_level4 FROM december_2016_grants)\n),\nyearly_counts AS (\n    SELECT \n        cpc_level4,\n        filing_year,\n        COUNT(DISTINCT \"publication_number\") AS patent_count\n    FROM all_german_patents\n    WHERE filing_year > 0\n    GROUP BY cpc_level4, filing_year\n),\nordered_counts AS (\n    SELECT \n        cpc_level4,\n        filing_year,\n        patent_count,\n        ROW_NUMBER() OVER (PARTITION BY cpc_level4 ORDER BY filing_year) AS year_rank\n    FROM yearly_counts\n),\nema_calculated AS (\n    SELECT \n        cpc_level4,\n        filing_year,\n        patent_count,\n        patent_count AS ema,\n        year_rank\n    FROM ordered_counts\n    WHERE year_rank = 1\n    \n    UNION ALL\n    \n    SELECT \n        oc.cpc_level4,\n        oc.filing_year,\n        oc.patent_count,\n        ROUND(0.1 * oc.patent_count + 0.9 * ec.ema, 2) AS ema,\n        oc.year_rank\n    FROM ordered_counts oc\n    JOIN ema_calculated ec ON oc.cpc_level4 = ec.cpc_level4 AND oc.year_rank = ec.year_rank + 1\n),\nmax_ema_per_cpc AS (\n    SELECT \n        cpc_level4,\n        filing_year,\n        ema,\n        ROW_NUMBER() OVER (PARTITION BY cpc_level4 ORDER BY ema DESC, filing_year DESC) as ema_rank\n    FROM ema_calculated\n)\nSELECT \n    m.cpc_level4 AS cpc_group,\n    cd.\"titleFull\" AS full_title,\n    m.filing_year AS year,\n    m.ema AS highest_exponential_moving_average\nFROM max_ema_per_cpc m\nLEFT JOIN \"PATENTS\".\"PATENTS\".\"CPC_DEFINITION\" cd \n    ON m.cpc_level4 = cd.\"symbol\"\nWHERE m.ema_rank = 1\nORDER BY m.ema DESC"
    },
    {
        "instance_id":"sf_bq221",
        "instruction":"Identify the CPC technology areas with the highest exponential moving average of patent filings each year (with a smoothing factor of 0.2), considering only the first CPC code for each patent that has a valid filing date and a non-empty application number, and report the full CPC title along with the best year associated with the highest exponential moving average for each CPC group at level 5.",
        "db_id":"PATENTS",
        "external_knowledge":"sliding_windows_calculation_cpc.md",
        "sql":"WITH RECURSIVE\nbase AS (\n  SELECT\n    p.\"application_number\",\n    FLOOR(p.\"filing_date\"\/10000) AS \"filing_year\",\n    f.\"VALUE\":\"code\"::string AS \"cpc_code\",\n    f.\"INDEX\" AS \"idx\",\n    ROW_NUMBER() OVER (PARTITION BY p.\"application_number\" ORDER BY f.\"INDEX\") AS \"rn\"\n  FROM \"PATENTS\".\"PATENTS\".\"PUBLICATIONS\" p,\n       LATERAL FLATTEN(input => p.\"cpc\") f\n  WHERE p.\"application_number\" IS NOT NULL\n    AND p.\"application_number\" != ''\n    AND p.\"filing_date\" IS NOT NULL\n    AND p.\"filing_date\" > 0\n    AND f.\"VALUE\":\"first\"::boolean = true\n),\npub_first AS (\n  SELECT\n    \"application_number\",\n    \"filing_year\",\n    \"cpc_code\"\n  FROM base\n  WHERE \"rn\" = 1\n),\nmap_group AS (\n  SELECT\n    pf.\"application_number\",\n    pf.\"filing_year\",\n    anc.\"symbol\" AS \"group_symbol\",\n    anc.\"titleFull\" AS \"group_title\"\n  FROM pub_first pf\n  JOIN \"PATENTS\".\"PATENTS\".\"CPC_DEFINITION\" cd\n    ON cd.\"symbol\" = pf.\"cpc_code\"\n  , LATERAL FLATTEN(input => ARRAY_CAT(ARRAY_CONSTRUCT(cd.\"symbol\"), cd.\"parents\")) par\n  JOIN \"PATENTS\".\"PATENTS\".\"CPC_DEFINITION\" anc\n    ON anc.\"symbol\" = par.\"VALUE\"::string\n   AND anc.\"level\" = 5\n),\nyearly_counts AS (\n  SELECT\n    \"group_symbol\",\n    \"group_title\",\n    \"filing_year\" AS \"year\",\n    COUNT(DISTINCT \"application_number\") AS \"filings\"\n  FROM map_group\n  GROUP BY 1,2,3\n),\ngroup_years AS (\n  SELECT\n    yc.\"group_symbol\",\n    yc.\"group_title\",\n    MIN(yc.\"year\") AS \"min_year\",\n    MAX(yc.\"year\") AS \"max_year\"\n  FROM yearly_counts yc\n  GROUP BY 1,2\n),\nseries AS (\n  SELECT\n    gy.\"group_symbol\",\n    gy.\"group_title\",\n    v.\"VALUE\"::int AS \"year\"\n  FROM group_years gy,\n       LATERAL FLATTEN(input => ARRAY_GENERATE_RANGE(gy.\"min_year\", gy.\"max_year\" + 1)) v\n),\nseries_counts AS (\n  SELECT\n    s.\"group_symbol\",\n    s.\"group_title\",\n    s.\"year\",\n    COALESCE(yc.\"filings\", 0) AS \"filings\"\n  FROM series s\n  LEFT JOIN yearly_counts yc\n    ON yc.\"group_symbol\" = s.\"group_symbol\"\n   AND yc.\"year\" = s.\"year\"\n),\nordered AS (\n  SELECT\n    \"group_symbol\",\n    \"group_title\",\n    \"year\",\n    \"filings\",\n    ROW_NUMBER() OVER (PARTITION BY \"group_symbol\" ORDER BY \"year\") AS \"ord\"\n  FROM series_counts\n),\nr AS (\n  SELECT\n    o.\"group_symbol\",\n    o.\"group_title\",\n    o.\"year\",\n    o.\"filings\",\n    o.\"filings\"::float AS \"ema\",\n    o.\"ord\"\n  FROM ordered o\n  WHERE o.\"ord\" = 1\n  UNION ALL\n  SELECT\n    o.\"group_symbol\",\n    o.\"group_title\",\n    o.\"year\",\n    o.\"filings\",\n    0.2 * o.\"filings\" + 0.8 * r.\"ema\" AS \"ema\",\n    o.\"ord\"\n  FROM ordered o\n  JOIN r\n    ON o.\"group_symbol\" = r.\"group_symbol\"\n   AND o.\"ord\" = r.\"ord\" + 1\n),\nbest_year AS (\n  SELECT\n    \"group_symbol\",\n    \"group_title\",\n    \"year\" AS \"best_year\",\n    \"ema\" AS \"max_ema\",\n    ROW_NUMBER() OVER (PARTITION BY \"group_symbol\" ORDER BY \"ema\" DESC, \"year\" ASC) AS \"rn\"\n  FROM r\n)\nSELECT\n  \"group_symbol\",\n  \"group_title\",\n  \"best_year\",\n  \"max_ema\"\nFROM best_year\nWHERE \"rn\" = 1\nORDER BY \"max_ema\" DESC, \"group_symbol\" ASC;"
    },
    {
        "instance_id":"sf_bq223",
        "instruction":"Which assignees, excluding DENSO CORP itself, have cited patents assigned to DENSO CORP, and what are the titles of the primary CPC subclasses associated with these citations? Provide the name of each citing assignee (excluding DENSO CORP), the full title of the primary CPC subclass (based on the first CPC code), and the count of citations grouped by the citing assignee and the CPC subclass title. Ensure that only citations of patents with valid filing dates are considered, and focus on the first CPC code for each citing patent. The results should specifically exclude DENSO CORP as a citing assignee.",
        "db_id":"PATENTS",
        "external_knowledge":"patents_info.md",
        "sql":"\/*\nMain Question: Identify non-DENSO assignees that have cited patents assigned to DENSO CORP (with valid filing dates) and list the CPC subclass titles (from the first CPC code) together with the count of such citing patents.\n\nHigh-level approach\n1. \"denso_pubs\" \u2013 all publications whose assignee list contains any form of \u201cDENSO\u201d *and* have a non-NULL filing date.\n2. \"citing_pubs\" \u2013 all publications that cite at least one patent in \"denso_pubs\" and themselves have a non-NULL filing date.\n3. \"citing_infos\" \u2013 for every publication in \"citing_pubs\":\n      \u2022 flatten assignees and keep only those **not** containing \u201cdenso\u201d;\n      \u2022 take the first CPC code (cpc.value:first = TRUE) and reduce it to its 4-character subclass symbol.\n   Each row now represents     (citing_pub , clean_assignee_name , cpc_subclass_symbol).\n4. Join to PATENTS.PATENTS.CPC_DEFINITION to obtain the full subclass title and aggregate.\n   We count DISTINCT citing publications to avoid double-counting the same patent when it cites multiple DENSO patents.\n*\/\n\nWITH \"denso_pubs\" AS (\n    \/* Patents assigned to DENSO (any variant in the name) with a valid filing date *\/\n    SELECT DISTINCT \"p\".\"publication_number\" AS \"denso_pub\"\n    FROM \"PATENTS\".\"PATENTS\".\"PUBLICATIONS\" \"p\",\n         LATERAL FLATTEN(input => \"p\".\"assignee_harmonized\") \"ah\"\n    WHERE \"p\".\"filing_date\" IS NOT NULL\n      AND UPPER(\"ah\".value:\"name\"::string) LIKE '%DENSO%'\n),\n\"citing_pubs\" AS (\n    \/* Publications that cite any of the DENSO patents and also have a valid filing date *\/\n    SELECT DISTINCT \"cp\".\"publication_number\" AS \"citing_pub\"\n    FROM \"PATENTS\".\"PATENTS\".\"PUBLICATIONS\" \"cp\",\n         LATERAL FLATTEN(input => \"cp\".\"citation\") \"cite\"\n         JOIN \"denso_pubs\" \"d\"\n           ON \"cite\".value:\"publication_number\"::string = \"d\".\"denso_pub\"\n    WHERE \"cp\".\"filing_date\" IS NOT NULL\n),\n\"citing_infos\" AS (\n    \/* Extract non-DENSO assignee name and first CPC subclass for each citing publication *\/\n    SELECT DISTINCT\n           \"cp\".\"citing_pub\",\n           TRIM(LOWER(\"ass\".value:\"name\"::string))            AS \"assignee_name\",\n           SUBSTR(\"cpc\".value:\"code\"::string , 1 , 4)         AS \"cpc_subclass_symbol\"\n    FROM \"citing_pubs\" \"cp\"\n         JOIN \"PATENTS\".\"PATENTS\".\"PUBLICATIONS\" \"pub\"\n           ON \"pub\".\"publication_number\" = \"cp\".\"citing_pub\"\n         , LATERAL FLATTEN(input => \"pub\".\"assignee_harmonized\") \"ass\"\n         , LATERAL FLATTEN(input => \"pub\".\"cpc\") \"cpc\"\n    WHERE \"ass\".value:\"name\" IS NOT NULL\n      AND LOWER(\"ass\".value:\"name\"::string) NOT LIKE '%denso%'\n      AND \"cpc\".value:\"first\"::boolean = TRUE\n)\n\/* Final aggregation: count distinct citing publications per (assignee , CPC subclass title) *\/\nSELECT\n       \"ci\".\"assignee_name\"                AS \"citing_assignee\",\n       \"cd\".\"titleFull\"                    AS \"cpc_subclass_title\",\n       COUNT(DISTINCT \"ci\".\"citing_pub\")   AS \"citation_count\"\nFROM \"citing_infos\" \"ci\"\n     JOIN \"PATENTS\".\"PATENTS\".\"CPC_DEFINITION\" \"cd\"\n       ON \"cd\".\"symbol\" = \"ci\".\"cpc_subclass_symbol\"\nGROUP BY\n       \"ci\".\"assignee_name\",\n       \"cd\".\"titleFull\"\nORDER BY\n       \"citation_count\" DESC,\n       \"citing_assignee\",\n       \"cpc_subclass_title\";"
    },
    {
        "instance_id":"sf_bq128",
        "instruction":"Retrieve the following information for U.S. patents filed between January 1, 2014, and February 1, 2014. The patent title and abstract. The publication date of the patent. The number of backward citations for each patent (i.e., the number of patents cited by the current patent before its filing date). The number of forward citations for each patent within the first 5 years of its publication (i.e., the number of patents that cited the current patent within 5 years after its publication). For each patent, ensure the forward citations are counted only for citations within 5 years after the publication date, and backward citations are counted for citations before the filing date.",
        "db_id":"PATENTSVIEW",
        "external_knowledge":"forward_backward_citation.md",
        "sql":"WITH \"filings\" AS (\n    SELECT\n        \"patent_id\",\n        MIN(TRY_TO_DATE(\"date\")) AS \"filing_date\"\n    FROM \"PATENTSVIEW\".\"PATENTSVIEW\".\"APPLICATION\"\n    WHERE \"country\" = 'US'\n      AND TRY_TO_DATE(\"date\") IS NOT NULL\n    GROUP BY \"patent_id\"\n),\n\"qualified_cpc\" AS (\n    SELECT DISTINCT\n        \"patent_id\"\n    FROM \"PATENTSVIEW\".\"PATENTSVIEW\".\"CPC_CURRENT\"\n    WHERE \"subsection_id\" IN ('C05','C06','C07','C08','C09','C10','C11','C12','C13')\n       OR \"group_id\" IN ('A01G','A01H','A61K','A61P','A61Q','B01F','B01J','B81B','B82B','B82Y','G01N','G16H')\n),\n\"base_pats\" AS (\n    SELECT DISTINCT\n        p.\"id\" AS \"patent_id\",\n        p.\"title\",\n        p.\"abstract\",\n        TRY_TO_DATE(p.\"date\") AS \"publication_date\",\n        f.\"filing_date\"\n    FROM \"PATENTSVIEW\".\"PATENTSVIEW\".\"PATENT\" p\n    JOIN \"filings\" f ON p.\"id\" = f.\"patent_id\"\n    JOIN \"qualified_cpc\" qc ON p.\"id\" = qc.\"patent_id\"\n    WHERE p.\"country\" = 'US'\n      AND f.\"filing_date\" BETWEEN TO_DATE('2014-01-01') AND TO_DATE('2014-02-01')\n      AND TRY_TO_DATE(p.\"date\") IS NOT NULL\n),\n\"backward_counts\" AS (\n    SELECT\n        bc.\"patent_id\",\n        COUNT(DISTINCT bc.\"citation_id\") AS \"backward_citations\"\n    FROM \"PATENTSVIEW\".\"PATENTSVIEW\".\"USPATENTCITATION\" bc\n    JOIN \"base_pats\" b ON bc.\"patent_id\" = b.\"patent_id\"\n    WHERE TRY_TO_DATE(bc.\"date\") IS NOT NULL\n      AND TRY_TO_DATE(bc.\"date\") < b.\"filing_date\"\n    GROUP BY bc.\"patent_id\"\n),\n\"forward_counts\" AS (\n    SELECT\n        fc.\"citation_id\" AS \"patent_id\",\n        COUNT(DISTINCT fc.\"patent_id\") AS \"forward_citations\"\n    FROM \"PATENTSVIEW\".\"PATENTSVIEW\".\"USPATENTCITATION\" fc\n    JOIN \"base_pats\" b ON fc.\"citation_id\" = b.\"patent_id\"\n    JOIN \"PATENTSVIEW\".\"PATENTSVIEW\".\"PATENT\" pc ON pc.\"id\" = fc.\"patent_id\"\n    WHERE TRY_TO_DATE(pc.\"date\") IS NOT NULL\n      AND TRY_TO_DATE(pc.\"date\") >= b.\"publication_date\"\n      AND TRY_TO_DATE(pc.\"date\") <= DATEADD(year, 5, b.\"publication_date\")\n    GROUP BY fc.\"citation_id\"\n)\nSELECT\n    b.\"title\",\n    b.\"abstract\",\n    b.\"publication_date\",\n    COALESCE(back.\"backward_citations\", 0) AS \"backward_citation_count\",\n    COALESCE(fwd.\"forward_citations\", 0) AS \"forward_citation_count\"\nFROM \"base_pats\" b\nLEFT JOIN \"backward_counts\" back ON back.\"patent_id\" = b.\"patent_id\"\nLEFT JOIN \"forward_counts\" fwd ON fwd.\"patent_id\" = b.\"patent_id\"\nORDER BY b.\"publication_date\", b.\"title\""
    },
    {
        "instance_id":"sf_bq246",
        "instruction":"Retrieve U.S. patents with the number of forward citations within the first 3 years after the patent application date (i.e., patents citing the current patent within 3 years). Only include patents with both backward citations within 1 year before the application date and forward citations within 1 year after the application date. The query should focus on specific CPC categories, sort results by backward citations in descending order, and return the patent with the most backward citations, limiting to one result.",
        "db_id":"PATENTSVIEW",
        "external_knowledge":null,
        "sql":"WITH \"cte_app\" AS (\n  SELECT\n    a.\"patent_id\",\n    MIN(TRY_TO_DATE(a.\"date\")) AS \"app_date\"\n  FROM \"PATENTSVIEW\".\"PATENTSVIEW\".\"APPLICATION\" a\n  WHERE TRY_TO_DATE(a.\"date\") IS NOT NULL\n  GROUP BY a.\"patent_id\"\n),\n\"base\" AS (\n  SELECT DISTINCT\n    p.\"id\" AS \"patent_id\",\n    p.\"number\" AS \"patent_number\",\n    ca.\"app_date\" AS \"application_date\"\n  FROM \"PATENTSVIEW\".\"PATENTSVIEW\".\"PATENT\" p\n  JOIN \"cte_app\" ca\n    ON ca.\"patent_id\" = p.\"id\"\n  JOIN \"PATENTSVIEW\".\"PATENTSVIEW\".\"CPC_CURRENT\" cpc\n    ON cpc.\"patent_id\" = p.\"id\"\n  WHERE p.\"country\" = 'US'\n    AND cpc.\"category\" = 'inventional'\n),\n\"back_1y\" AS (\n  SELECT\n    b.\"patent_id\",\n    COUNT(DISTINCT b.\"citation_id\") AS \"back_1y_count\"\n  FROM \"PATENTSVIEW\".\"PATENTSVIEW\".\"USPATENTCITATION\" b\n  JOIN \"base\" ba\n    ON ba.\"patent_id\" = b.\"patent_id\"\n  JOIN \"cte_app\" app_cited\n    ON app_cited.\"patent_id\" = b.\"citation_id\"\n  WHERE app_cited.\"app_date\" >= DATEADD(year, -1, ba.\"application_date\")\n    AND app_cited.\"app_date\" < ba.\"application_date\"\n  GROUP BY b.\"patent_id\"\n),\n\"fwd_1y\" AS (\n  SELECT\n    f.\"citation_id\" AS \"patent_id\",\n    COUNT(DISTINCT f.\"patent_id\") AS \"fwd_1y_count\"\n  FROM \"PATENTSVIEW\".\"PATENTSVIEW\".\"USPATENTCITATION\" f\n  JOIN \"base\" ba\n    ON ba.\"patent_id\" = f.\"citation_id\"\n  JOIN \"cte_app\" app_citing\n    ON app_citing.\"patent_id\" = f.\"patent_id\"\n  WHERE app_citing.\"app_date\" >= ba.\"application_date\"\n    AND app_citing.\"app_date\" <= DATEADD(year, 1, ba.\"application_date\")\n  GROUP BY f.\"citation_id\"\n),\n\"fwd_3y\" AS (\n  SELECT\n    f.\"citation_id\" AS \"patent_id\",\n    COUNT(DISTINCT f.\"patent_id\") AS \"fwd_3y_count\"\n  FROM \"PATENTSVIEW\".\"PATENTSVIEW\".\"USPATENTCITATION\" f\n  JOIN \"base\" ba\n    ON ba.\"patent_id\" = f.\"citation_id\"\n  JOIN \"cte_app\" app_citing\n    ON app_citing.\"patent_id\" = f.\"patent_id\"\n  WHERE app_citing.\"app_date\" >= ba.\"application_date\"\n    AND app_citing.\"app_date\" <= DATEADD(year, 3, ba.\"application_date\")\n  GROUP BY f.\"citation_id\"\n)\nSELECT\n  b.\"patent_id\",\n  b.\"patent_number\",\n  b.\"application_date\",\n  COALESCE(bk.\"back_1y_count\", 0) AS \"backward_citations_within_1y_before_app\",\n  COALESCE(f1.\"fwd_1y_count\", 0) AS \"forward_citations_within_1y_after_app\",\n  COALESCE(f3.\"fwd_3y_count\", 0) AS \"forward_citations_within_3y_after_app\"\nFROM \"base\" b\nLEFT JOIN \"back_1y\" bk\n  ON bk.\"patent_id\" = b.\"patent_id\"\nLEFT JOIN \"fwd_1y\" f1\n  ON f1.\"patent_id\" = b.\"patent_id\"\nLEFT JOIN \"fwd_3y\" f3\n  ON f3.\"patent_id\" = b.\"patent_id\"\nWHERE COALESCE(bk.\"back_1y_count\", 0) > 0\n  AND COALESCE(f1.\"fwd_1y_count\", 0) > 0\nORDER BY bk.\"back_1y_count\" DESC\nLIMIT 1;"
    },
    {
        "instance_id":"sf_bq052",
        "instruction":"Retrieve the following information for U.S. patents: The patent ID, title, and application date. The number of backward citations within 1 month before the application date (i.e., patents that cited the current patent before its application). The number of forward citations within 1 month after the application date (i.e., patents that cited the current patent after its application). The abstract text of the patent. Only include patents that belong to specific CPC categories, such as subsection 'C05' or group 'A01G'. The query should filter patents to include only those that have at least one backward citation or one forward citation in the 1-month period specified. Sort the results by application date and return all matching records.",
        "db_id":"PATENTSVIEW",
        "external_knowledge":null,
        "sql":"WITH CpcFilteredPatents AS (\n    SELECT DISTINCT\n        p.\"id\",\n        p.\"title\",\n        p.\"abstract\"\n    FROM \"PATENTSVIEW\".\"PATENTSVIEW\".\"PATENT\" AS p\n    JOIN \"PATENTSVIEW\".\"PATENTSVIEW\".\"CPC_CURRENT\" AS cpc ON p.\"id\" = cpc.\"patent_id\"\n    WHERE p.\"country\" = 'US' AND (cpc.\"subsection_id\" = 'C05' OR cpc.\"group_id\" = 'A01G')\n)\nSELECT\n    cfp.\"id\" AS patent_id,\n    cfp.\"title\",\n    core_app.\"date\" AS application_date,\n    COUNT(DISTINCT CASE\n        WHEN citing_app.\"date\"::DATE >= DATEADD(month, -1, core_app.\"date\"::DATE) AND citing_app.\"date\"::DATE < core_app.\"date\"::DATE\n        THEN usc.\"patent_id\"\n        ELSE NULL\n    END) AS num_backward_citations,\n    COUNT(DISTINCT CASE\n        WHEN citing_app.\"date\"::DATE > core_app.\"date\"::DATE AND citing_app.\"date\"::DATE <= DATEADD(month, 1, core_app.\"date\"::DATE)\n        THEN usc.\"patent_id\"\n        ELSE NULL\n    END) AS num_forward_citations,\n    cfp.\"abstract\"\nFROM CpcFilteredPatents AS cfp\nJOIN \"PATENTSVIEW\".\"PATENTSVIEW\".\"APPLICATION\" AS core_app ON cfp.\"id\" = core_app.\"patent_id\"\nLEFT JOIN \"PATENTSVIEW\".\"PATENTSVIEW\".\"USPATENTCITATION\" AS usc ON cfp.\"id\" = usc.\"citation_id\"\nLEFT JOIN \"PATENTSVIEW\".\"PATENTSVIEW\".\"APPLICATION\" AS citing_app ON usc.\"patent_id\" = citing_app.\"patent_id\"\nGROUP BY\n    cfp.\"id\",\n    cfp.\"title\",\n    core_app.\"date\",\n    cfp.\"abstract\"\nHAVING\n    num_backward_citations > 0 OR num_forward_citations > 0\nORDER BY\n    application_date"
    },
    {
        "instance_id":"sf_bq182",
        "instruction":"Which primary programming languages, determined by the highest number of bytes in each repository, had at least 5 PullRequestEvents on January 18, 2023 across all their repositories?",
        "db_id":"GITHUB_REPOS_DATE",
        "external_knowledge":null,
        "sql":"WITH primary_languages AS (\n    SELECT \n        \"repo_name\",\n        lang_data.value:\"name\"::string as primary_language,\n        ROW_NUMBER() OVER (PARTITION BY \"repo_name\" ORDER BY lang_data.value:\"bytes\"::int DESC) as rn\n    FROM GITHUB_REPOS_DATE.GITHUB_REPOS.LANGUAGES,\n         TABLE(FLATTEN(PARSE_JSON(\"language\"))) as lang_data\n    WHERE \"language\" != '[]'\n),\nprimary_lang_only AS (\n    SELECT \"repo_name\", primary_language\n    FROM primary_languages\n    WHERE rn = 1\n),\npull_requests_jan18 AS (\n    SELECT \n        \"repo\":\"name\"::string as repo_name,\n        COUNT(*) as pr_count\n    FROM GITHUB_REPOS_DATE.YEAR._2023 \n    WHERE \"type\" = 'PullRequestEvent'\n        AND \"created_at\" >= 1674009600000000  -- 2023-01-18 00:00:00 UTC\n        AND \"created_at\" < 1674096000000000   -- 2023-01-19 00:00:00 UTC\n    GROUP BY \"repo\":\"name\"::string\n)\nSELECT \n    plo.primary_language,\n    SUM(pr.pr_count) as total_pr_events\nFROM primary_lang_only plo\nJOIN pull_requests_jan18 pr ON plo.\"repo_name\" = pr.repo_name\nGROUP BY plo.primary_language\nHAVING total_pr_events >= 5\nORDER BY total_pr_events DESC"
    },
    {
        "instance_id":"sf_bq224",
        "instruction":"Which repository with an approved license in `licenses.md` had the highest combined total of forks, issues, and watches in April 2022?",
        "db_id":"GITHUB_REPOS_DATE",
        "external_knowledge":null,
        "sql":"WITH approved_repos AS ( SELECT DISTINCT T1.\"repo_name\" FROM GITHUB_REPOS_DATE.GITHUB_REPOS.LICENSES T1 JOIN GITHUB_REPOS_DATE.GITHUB_REPOS.LICENSES T2 ON LOWER(T1.\"license\") = LOWER(T2.\"license\") ), events_agg AS ( SELECT TRY_PARSE_JSON(E.\"repo\"):\"name\"::STRING AS repo_name, SUM(CASE WHEN E.\"type\" = 'ForkEvent'  THEN 1 ELSE 0 END) AS fork_events, SUM(CASE WHEN E.\"type\" = 'IssuesEvent' THEN 1 ELSE 0 END) AS issues_events, SUM(CASE WHEN E.\"type\" = 'WatchEvent' THEN 1 ELSE 0 END) AS watch_events FROM GITHUB_REPOS_DATE.MONTH._202204 E WHERE TO_TIMESTAMP(E.\"created_at\" \/ 1000000) >= '2022-04-01' AND TO_TIMESTAMP(E.\"created_at\" \/ 1000000) < '2022-05-01' AND E.\"type\" IN ('ForkEvent','IssuesEvent','WatchEvent') GROUP BY TRY_PARSE_JSON(E.\"repo\"):\"name\"::STRING ), totals AS ( SELECT repo_name, fork_events + issues_events + watch_events AS total_events FROM events_agg ) SELECT T.repo_name FROM totals T JOIN approved_repos A ON T.repo_name = A.\"repo_name\" ORDER BY T.total_events DESC LIMIT 1;"
    },
    {
        "instance_id":"sf_bq233",
        "instruction":"Can you analyze the joined data from github repos files and github_repos contents, focusing only on files ending with '.py' or '.r', then extract Python modules from 'import' or 'from ... import' lines and R libraries from 'library(...)' lines, count their occurrences, and finally list the results sorted by language and by the number of occurrences in descending order?",
        "db_id":"GITHUB_REPOS",
        "external_knowledge":null,
        "sql":"WITH python_files AS (\n  SELECT \n    sf.\"path\",\n    sc.\"content\",\n    'Python' as language\n  FROM GITHUB_REPOS.GITHUB_REPOS.SAMPLE_FILES sf \n  JOIN GITHUB_REPOS.GITHUB_REPOS.SAMPLE_CONTENTS sc ON sf.\"id\" = sc.\"id\" \n  WHERE sc.\"binary\" = FALSE \n  AND sf.\"path\" ILIKE '%.py'\n),\npython_import_lines AS (\n  SELECT \n    language,\n    \"path\",\n    TRIM(line.value) as line_content\n  FROM python_files,\n  LATERAL SPLIT_TO_TABLE(\"content\", '\\n') line\n  WHERE TRIM(line.value) LIKE 'import %' \n     OR TRIM(line.value) LIKE 'from %import%'\n),\nparsed_python_imports AS (\n  SELECT \n    language,\n    CASE \n      -- Handle \"import module\" statements - extract first module name\n      WHEN line_content LIKE 'import %' AND line_content NOT LIKE 'from %' THEN\n        TRIM(SPLIT_PART(SPLIT_PART(line_content, 'import ', 2), ',', 1))\n      -- Handle \"from module import ...\" statements - extract the main module\n      WHEN line_content LIKE 'from %import%' THEN\n        TRIM(SPLIT_PART(SPLIT_PART(line_content, 'from ', 2), ' import', 1))\n      ELSE NULL\n    END as full_module_name\n  FROM python_import_lines\n  WHERE line_content IS NOT NULL AND line_content != ''\n),\npython_base_modules AS (\n  SELECT \n    language,\n    -- Extract the base module name (first part before dot)\n    SPLIT_PART(full_module_name, '.', 1) as module_name\n  FROM parsed_python_imports\n  WHERE full_module_name IS NOT NULL AND full_module_name != ''\n),\npython_results AS (\n  SELECT \n    language,\n    module_name,\n    COUNT(*) as occurrence_count\n  FROM python_base_modules\n  WHERE module_name IS NOT NULL AND module_name != ''\n  GROUP BY language, module_name\n),\nr_files AS (\n  SELECT \n    \"sample_path\" as r_path,\n    \"content\",\n    'R' as language\n  FROM GITHUB_REPOS.GITHUB_REPOS.SAMPLE_CONTENTS \n  WHERE \"binary\" = FALSE \n  AND (\"sample_path\" ILIKE '%.r' OR \"sample_path\" ILIKE '%.R' OR \"sample_path\" ILIKE '%.Rmd')\n),\nr_import_lines AS (\n  SELECT \n    language,\n    r_path,\n    TRIM(line.value) as line_content\n  FROM r_files,\n  LATERAL SPLIT_TO_TABLE(\"content\", '\\n') line\n  WHERE TRIM(line.value) LIKE 'library(%' \n     OR TRIM(line.value) LIKE 'require(%'\n),\nparsed_r_imports AS (\n  SELECT \n    language,\n    CASE \n      -- Handle library() statements\n      WHEN line_content LIKE 'library(%' THEN\n        TRIM(REPLACE(REPLACE(SPLIT_PART(SPLIT_PART(line_content, 'library(', 2), ')', 1), '\"', ''), '\\\\', ''))\n      -- Handle require() statements  \n      WHEN line_content LIKE 'require(%' THEN\n        TRIM(REPLACE(REPLACE(SPLIT_PART(SPLIT_PART(line_content, 'require(', 2), ')', 1), '\"', ''), '\\\\', ''))\n      ELSE NULL\n    END as library_name\n  FROM r_import_lines\n  WHERE line_content IS NOT NULL AND line_content != ''\n),\nr_results AS (\n  SELECT \n    language,\n    library_name as module_name,\n    COUNT(*) as occurrence_count\n  FROM parsed_r_imports\n  WHERE library_name IS NOT NULL AND library_name != ''\n  GROUP BY language, library_name\n),\ncombined_results AS (\n  SELECT language, module_name, occurrence_count FROM python_results\n  UNION ALL\n  SELECT language, module_name, occurrence_count FROM r_results\n)\nSELECT \n  language,\n  module_name,\n  occurrence_count\nFROM combined_results\nORDER BY language, occurrence_count DESC, module_name"
    },
    {
        "instance_id":"sf_bq248",
        "instruction":"Among all repositories that do not use any programming language whose name (case-insensitively) includes the substring \"python,\" what is the proportion of files whose paths include \"readme.md\" and whose contents contain the phrase \"Copyright (c)\"?",
        "db_id":"GITHUB_REPOS",
        "external_knowledge":null,
        "sql":"-- Answer: proportion of README.md files (in non-Python repositories) whose contents include \"Copyright (c)\"\n-- 1) Filter SAMPLE_CONTENTS to files whose path contains \"readme.md\" (case-insensitive).\n-- 2) Exclude any repository that uses a language whose name contains the substring \"python\" (case-insensitive).\n--    To detect such repositories, FLATTEN the VARIANT array \"language\" and look at the \"name\" field.\n-- 3) Compute the proportion = (# files with the phrase) \/ (total # README.md files) as a floating value.\n\nSELECT\n    \/* numerator: files whose content contains the phrase *\/\n    COUNT_IF(LOWER(sc.\"content\") LIKE '%copyright (c)%')::DOUBLE\n    \/\n    \/* denominator: total README.md files in non-Python repos *\/\n    NULLIF(COUNT(*), 0)                                       AS \"proportion\"\nFROM \"GITHUB_REPOS\".\"GITHUB_REPOS\".\"SAMPLE_CONTENTS\" sc\nWHERE LOWER(sc.\"sample_path\") LIKE '%readme.md%'\n  AND NOT EXISTS (\n        SELECT 1\n        FROM \"GITHUB_REPOS\".\"GITHUB_REPOS\".\"LANGUAGES\" l,\n             LATERAL FLATTEN(input => l.\"language\") f\n        WHERE l.\"repo_name\" = sc.\"sample_repo_name\"\n          AND LOWER(f.value:\"name\"::string) LIKE '%python%'\n  );"
    },
    {
        "instance_id":"sf_bq193",
        "instruction":"Retrieve all non-empty, non-commented lines from `README.md` files in GitHub repositories, excluding lines that are comments (either starting with `#` for Markdown or `\/\/` for code comments). For each line, calculate how often each unique line appears across all repositories and return a comma-separated list of the programming languages used in each repository containing that line, sorted alphabetically, with the results ordered by the frequency of occurrence in descending order.",
        "db_id":"GITHUB_REPOS",
        "external_knowledge":null,
        "sql":"WITH readme_lines AS (\n  SELECT\n    sc.\"sample_repo_name\" AS \"repo_name\",\n    TRIM(REPLACE(l.value::string, '\\r', '')) AS \"line_clean\"\n  FROM \"GITHUB_REPOS\".\"GITHUB_REPOS\".\"SAMPLE_CONTENTS\" sc,\n       LATERAL FLATTEN(input => SPLIT(sc.\"content\", '\\n')) l\n  WHERE sc.\"sample_path\" ILIKE '%README.md'\n    AND NVL(sc.\"binary\", FALSE) = FALSE\n    AND sc.\"content\" IS NOT NULL\n),\nfiltered_lines AS (\n  SELECT\n    rl.\"repo_name\",\n    rl.\"line_clean\"\n  FROM readme_lines rl\n  WHERE rl.\"line_clean\" != ''\n    AND LTRIM(rl.\"line_clean\") NOT LIKE '#%'\n    AND LTRIM(rl.\"line_clean\") NOT LIKE '\/\/%'\n),\nrepo_languages AS (\n  SELECT\n    lg.\"repo_name\",\n    TRIM(fl.value:\"name\"::string) AS \"language_name\"\n  FROM \"GITHUB_REPOS\".\"GITHUB_REPOS\".\"LANGUAGES\" lg,\n       LATERAL FLATTEN(input => lg.\"language\") fl\n  WHERE fl.value:\"name\" IS NOT NULL\n    AND TRIM(fl.value:\"name\"::string) != ''\n)\nSELECT\n  f.\"line_clean\" AS \"line\",\n  COUNT(DISTINCT f.\"repo_name\") AS \"frequency\",\n  LISTAGG(DISTINCT rl.\"language_name\", ', ') WITHIN GROUP (ORDER BY rl.\"language_name\") AS \"languages\"\nFROM filtered_lines f\nLEFT JOIN repo_languages rl\n  ON rl.\"repo_name\" = f.\"repo_name\"\nGROUP BY f.\"line_clean\"\nORDER BY \"frequency\" DESC, \"line\" ASC"
    },
    {
        "instance_id":"sf_bq295",
        "instruction":"Using the 2017 GitHub Archive data for watch events, which three repositories that include at least one Python file (with a .py extension) smaller than 15,000 bytes and containing the substring \"def \" in its content have the highest total number of watch events for that year?",
        "db_id":"GITHUB_REPOS_DATE",
        "external_knowledge":null,
        "sql":"SELECT \n    w.\"repo\":name::VARCHAR as repo_name,\n    COUNT(*) as watch_count\nFROM \"GITHUB_REPOS_DATE\".\"YEAR\".\"_2017\" w\nWHERE w.\"type\" = 'WatchEvent'\nAND w.\"repo\" IS NOT NULL\nAND EXISTS (\n    SELECT 1 \n    FROM \"GITHUB_REPOS_DATE\".\"GITHUB_REPOS\".\"SAMPLE_CONTENTS\" c\n    WHERE c.\"sample_repo_name\" = w.\"repo\":name::VARCHAR\n    AND c.\"sample_path\" LIKE '%.py'\n    AND c.\"size\" < 15000\n    AND c.\"content\" LIKE '%def %'\n)\nGROUP BY repo_name\nORDER BY watch_count DESC\nLIMIT 3;"
    },
    {
        "instance_id":"sf_bq255",
        "instruction":"How many commit messages are there in repositories that use the 'Shell' programming language and 'apache-2.0' license, where the length of the commit message is more than 5 characters but less than 10,000 characters, and the messages do not start with the word 'merge', 'update' or 'test'?",
        "db_id":"GITHUB_REPOS",
        "external_knowledge":null,
        "sql":"SELECT\n    COUNT(*) AS \"commit_count\"\nFROM \"GITHUB_REPOS\".\"GITHUB_REPOS\".\"SAMPLE_COMMITS\" AS \"sc\"\nINNER JOIN (\n    SELECT DISTINCT \"l\".\"repo_name\"\n    FROM \"GITHUB_REPOS\".\"GITHUB_REPOS\".\"LANGUAGES\" AS \"l\",\n         LATERAL FLATTEN(INPUT => \"l\".\"language\") AS \"lang\"\n    WHERE LOWER(\"lang\".\"VALUE\":\"name\"::STRING) = 'shell'\n) AS \"shell_repos\"\n    ON \"sc\".\"repo_name\" = \"shell_repos\".\"repo_name\"\nINNER JOIN (\n    SELECT DISTINCT \"repo_name\"\n    FROM \"GITHUB_REPOS\".\"GITHUB_REPOS\".\"LICENSES\"\n    WHERE LOWER(\"license\") = 'apache-2.0'\n) AS \"licensed_repos\"\n    ON \"sc\".\"repo_name\" = \"licensed_repos\".\"repo_name\"\nWHERE \"sc\".\"message\" IS NOT NULL\n  AND LENGTH(\"sc\".\"message\") > 5\n  AND LENGTH(\"sc\".\"message\") < 10000\n  AND NOT REGEXP_LIKE(LOWER(LTRIM(\"sc\".\"message\")), '^(merge|update|test)\\b');"
    },
    {
        "instance_id":"sf_bq377",
        "instruction":"Extract and count the frequency of all package names listed in the require section of JSON-formatted content",
        "db_id":"GITHUB_REPOS",
        "external_knowledge":null,
        "sql":"WITH json_content AS (\n  SELECT \n    \"content\",\n    \"sample_ref\",\n    \"sample_path\"\n  FROM \"GITHUB_REPOS\".\"GITHUB_REPOS\".\"SAMPLE_CONTENTS\"\n  WHERE \"sample_path\" ILIKE '%.json'\n    AND \"content\" IS NOT NULL\n    AND \"content\" != ''\n),\nparsed_json AS (\n  SELECT \n    \"sample_ref\",\n    \"sample_path\",\n    TRY_PARSE_JSON(\"content\") AS \"parsed_content\"\n  FROM json_content\n  WHERE TRY_PARSE_JSON(\"content\") IS NOT NULL\n),\nrequire_sections AS (\n  SELECT \n    \"sample_ref\",\n    \"sample_path\",\n    \"parsed_content\":\"require\" AS \"require_obj\"\n  FROM parsed_json\n  WHERE \"parsed_content\":\"require\" IS NOT NULL\n),\nflattened_packages AS (\n  SELECT \n    r.\"sample_ref\",\n    r.\"sample_path\",\n    f.key::STRING AS \"package_name\"\n  FROM require_sections r,\n  LATERAL FLATTEN(input => r.\"require_obj\") f\n)\nSELECT \n  \"package_name\",\n  COUNT(*) AS \"frequency\"\nFROM flattened_packages\nWHERE \"package_name\" IS NOT NULL\nGROUP BY \"package_name\"\nORDER BY \"frequency\" DESC, \"package_name\""
    },
    {
        "instance_id":"sf_bq359",
        "instruction":"List the repository names and commit counts for the top two GitHub repositories with JavaScript as the primary language and the highest number of commits.",
        "db_id":"GITHUB_REPOS",
        "external_knowledge":null,
        "sql":"WITH js_flattened AS (\n  SELECT\n    \"repo_name\",\n    f.value:\"name\"::STRING AS \"lang\",\n    TRY_CAST(f.value:\"bytes\"::STRING AS NUMBER) AS \"bytes\"\n  FROM GITHUB_REPOS.GITHUB_REPOS.LANGUAGES,\n       LATERAL FLATTEN(INPUT => \"language\") f\n),\nprimary_lang AS (\n  SELECT\n    \"repo_name\",\n    \"lang\",\n    \"bytes\",\n    ROW_NUMBER() OVER (\n      PARTITION BY \"repo_name\"\n      ORDER BY \"bytes\" DESC NULLS LAST, \"lang\" ASC\n    ) AS \"rn\"\n  FROM js_flattened\n),\njs_primary_repos AS (\n  SELECT\n    \"repo_name\"\n  FROM primary_lang\n  WHERE \"rn\" = 1\n    AND LOWER(\"lang\") = 'javascript'\n),\ncommits_agg AS (\n  SELECT\n    \"repo_name\",\n    COUNT(DISTINCT \"commit\") AS \"commit_count\"\n  FROM GITHUB_REPOS.GITHUB_REPOS.SAMPLE_COMMITS\n  WHERE \"repo_name\" IS NOT NULL\n  GROUP BY \"repo_name\"\n)\nSELECT\n  j.\"repo_name\" AS \"repo_name\",\n  c.\"commit_count\" AS \"commit_count\"\nFROM js_primary_repos j\nJOIN commits_agg c\n  ON j.\"repo_name\" = c.\"repo_name\"\nORDER BY\n  c.\"commit_count\" DESC NULLS LAST,\n  j.\"repo_name\" ASC\nLIMIT 2;"
    },
    {
        "instance_id":"sf_bq252",
        "instruction":"Could you please find the name of the repository that contains the most copied non-binary Swift file in the dataset, ensuring each file is uniquely identified by its ID?",
        "db_id":"GITHUB_REPOS",
        "external_knowledge":null,
        "sql":"\nSELECT f.\"repo_name\" AS repository_name\nFROM \"GITHUB_REPOS\".\"GITHUB_REPOS\".\"SAMPLE_CONTENTS\" c\nJOIN \"GITHUB_REPOS\".\"GITHUB_REPOS\".\"SAMPLE_FILES\" f\nON c.\"id\" = f.\"id\"\nWHERE c.\"sample_path\" ILIKE '%.swift' AND c.\"binary\" = FALSE\nORDER BY c.\"copies\" DESC NULLS LAST\nLIMIT 1;\n"
    },
    {
        "instance_id":"sf_bq236",
        "instruction":"What are the top 5 zip codes of the areas in the United States that have experienced the most hail storm events in the past 10 years? Don't use data from hail reports table.",
        "db_id":"NOAA_DATA_PLUS",
        "external_knowledge":"functions_st_within.md",
        "sql":"WITH threshold AS (\n    SELECT DATEADD(year, -10, CURRENT_DATE()) AS min_event_date\n),\nhail_events AS (\n    SELECT TO_TIMESTAMP_NTZ(\"event_begin_time\" \/ 1000000) AS event_ts,\n           \"event_latitude\" AS event_lat,\n           \"event_longitude\" AS event_lon\n    FROM \"NOAA_DATA_PLUS\".\"NOAA_HISTORIC_SEVERE_STORMS\".\"STORMS_2014\"\n    WHERE UPPER(\"event_type\") = 'HAIL' AND \"event_latitude\" IS NOT NULL AND \"event_longitude\" IS NOT NULL\n    UNION ALL\n    SELECT TO_TIMESTAMP_NTZ(\"event_begin_time\" \/ 1000000), \"event_latitude\", \"event_longitude\"\n    FROM \"NOAA_DATA_PLUS\".\"NOAA_HISTORIC_SEVERE_STORMS\".\"STORMS_2015\"\n    WHERE UPPER(\"event_type\") = 'HAIL' AND \"event_latitude\" IS NOT NULL AND \"event_longitude\" IS NOT NULL\n    UNION ALL\n    SELECT TO_TIMESTAMP_NTZ(\"event_begin_time\" \/ 1000000), \"event_latitude\", \"event_longitude\"\n    FROM \"NOAA_DATA_PLUS\".\"NOAA_HISTORIC_SEVERE_STORMS\".\"STORMS_2016\"\n    WHERE UPPER(\"event_type\") = 'HAIL' AND \"event_latitude\" IS NOT NULL AND \"event_longitude\" IS NOT NULL\n    UNION ALL\n    SELECT TO_TIMESTAMP_NTZ(\"event_begin_time\" \/ 1000000), \"event_latitude\", \"event_longitude\"\n    FROM \"NOAA_DATA_PLUS\".\"NOAA_HISTORIC_SEVERE_STORMS\".\"STORMS_2017\"\n    WHERE UPPER(\"event_type\") = 'HAIL' AND \"event_latitude\" IS NOT NULL AND \"event_longitude\" IS NOT NULL\n    UNION ALL\n    SELECT TO_TIMESTAMP_NTZ(\"event_begin_time\" \/ 1000000), \"event_latitude\", \"event_longitude\"\n    FROM \"NOAA_DATA_PLUS\".\"NOAA_HISTORIC_SEVERE_STORMS\".\"STORMS_2018\"\n    WHERE UPPER(\"event_type\") = 'HAIL' AND \"event_latitude\" IS NOT NULL AND \"event_longitude\" IS NOT NULL\n    UNION ALL\n    SELECT TO_TIMESTAMP_NTZ(\"event_begin_time\" \/ 1000000), \"event_latitude\", \"event_longitude\"\n    FROM \"NOAA_DATA_PLUS\".\"NOAA_HISTORIC_SEVERE_STORMS\".\"STORMS_2019\"\n    WHERE UPPER(\"event_type\") = 'HAIL' AND \"event_latitude\" IS NOT NULL AND \"event_longitude\" IS NOT NULL\n    UNION ALL\n    SELECT TO_TIMESTAMP_NTZ(\"event_begin_time\" \/ 1000000), \"event_latitude\", \"event_longitude\"\n    FROM \"NOAA_DATA_PLUS\".\"NOAA_HISTORIC_SEVERE_STORMS\".\"STORMS_2020\"\n    WHERE UPPER(\"event_type\") = 'HAIL' AND \"event_latitude\" IS NOT NULL AND \"event_longitude\" IS NOT NULL\n    UNION ALL\n    SELECT TO_TIMESTAMP_NTZ(\"event_begin_time\" \/ 1000000), \"event_latitude\", \"event_longitude\"\n    FROM \"NOAA_DATA_PLUS\".\"NOAA_HISTORIC_SEVERE_STORMS\".\"STORMS_2021\"\n    WHERE UPPER(\"event_type\") = 'HAIL' AND \"event_latitude\" IS NOT NULL AND \"event_longitude\" IS NOT NULL\n    UNION ALL\n    SELECT TO_TIMESTAMP_NTZ(\"event_begin_time\" \/ 1000000), \"event_latitude\", \"event_longitude\"\n    FROM \"NOAA_DATA_PLUS\".\"NOAA_HISTORIC_SEVERE_STORMS\".\"STORMS_2022\"\n    WHERE UPPER(\"event_type\") = 'HAIL' AND \"event_latitude\" IS NOT NULL AND \"event_longitude\" IS NOT NULL\n    UNION ALL\n    SELECT TO_TIMESTAMP_NTZ(\"event_begin_time\" \/ 1000000), \"event_latitude\", \"event_longitude\"\n    FROM \"NOAA_DATA_PLUS\".\"NOAA_HISTORIC_SEVERE_STORMS\".\"STORMS_2023\"\n    WHERE UPPER(\"event_type\") = 'HAIL' AND \"event_latitude\" IS NOT NULL AND \"event_longitude\" IS NOT NULL\n    UNION ALL\n    SELECT TO_TIMESTAMP_NTZ(\"event_begin_time\" \/ 1000000), \"event_latitude\", \"event_longitude\"\n    FROM \"NOAA_DATA_PLUS\".\"NOAA_HISTORIC_SEVERE_STORMS\".\"STORMS_2024\"\n    WHERE UPPER(\"event_type\") = 'HAIL' AND \"event_latitude\" IS NOT NULL AND \"event_longitude\" IS NOT NULL\n),\nrecent_hail AS (\n    SELECT e.event_ts, e.event_lat, e.event_lon\n    FROM hail_events e\n    JOIN threshold t ON e.event_ts >= t.min_event_date\n),\nhail_with_zip AS (\n    SELECT z.\"zip_code\",\n           COUNT(*) AS hail_event_count\n    FROM recent_hail r\n    JOIN \"NOAA_DATA_PLUS\".\"GEO_US_BOUNDARIES\".\"ZIP_CODES\" z\n      ON ST_WITHIN(ST_POINT(r.event_lon, r.event_lat), TO_GEOGRAPHY(z.\"zip_code_geom\"))\n    GROUP BY z.\"zip_code\"\n)\nSELECT \"zip_code\", hail_event_count\nFROM hail_with_zip\nORDER BY hail_event_count DESC, \"zip_code\"\nFETCH FIRST 5 ROWS ONLY;"
    },
    {
        "instance_id":"sf_bq358",
        "instruction":"Can you tell me which bike trip in New York City on July 15, 2015, started and ended in ZIP Code areas with the highest average temperature for that day, as recorded by the Central Park weather station (WBAN '94728')? If there's more than one trip that meets these criteria, I'd like to know about the one that starts in the smallest ZIP Code and ends in the largest ZIP Code. Please return the starting and ending ZIP Codes of this trip.",
        "db_id":"NEW_YORK_CITIBIKE_1",
        "external_knowledge":"functions_st_within.md",
        "sql":"WITH weather_check AS (\n    SELECT \"temp\"\n    FROM NEW_YORK_CITIBIKE_1.NOAA_GSOD.GSOD2015\n    WHERE \"wban\" = '94728'\n      AND \"mo\" = '07'\n      AND \"da\" = '15'\n),\nnyc_zips AS (\n    SELECT \"zip_code\", TO_GEOGRAPHY(\"zip_code_geom\") AS geom\n    FROM NEW_YORK_CITIBIKE_1.GEO_US_BOUNDARIES.ZIP_CODES\n    WHERE \"state_code\" = 'NY' AND \"city\" ILIKE '%New York%'\n),\njul15_trips AS (\n    SELECT\n        t.\"bikeid\",\n        ST_POINT(t.\"start_station_longitude\", t.\"start_station_latitude\") AS start_pt,\n        ST_POINT(t.\"end_station_longitude\", t.\"end_station_latitude\") AS end_pt\n    FROM NEW_YORK_CITIBIKE_1.NEW_YORK_CITIBIKE.CITIBIKE_TRIPS t\n    CROSS JOIN weather_check w\n    WHERE TO_TIMESTAMP(t.\"starttime\" \/ 1000000)::DATE = '2015-07-15'\n)\nSELECT\n    sz.\"zip_code\" AS start_zip,\n    ez.\"zip_code\" AS end_zip\nFROM jul15_trips jt\nJOIN nyc_zips sz ON ST_CONTAINS(sz.geom, jt.start_pt)\nJOIN nyc_zips ez ON ST_CONTAINS(ez.geom, jt.end_pt)\nORDER BY start_zip ASC, end_zip DESC\nLIMIT 1;"
    },
    {
        "instance_id":"sf_bq050",
        "instruction":"I want to analyze bike trips in New York City for 2014 by linking trip data with weather information to understand how weather conditions (temperature, wind speed, and precipitation) affect bike trips between neighborhoods. For each combination of starting and ending neighborhoods, I need the following: 1. Total number of bike trips between the neighborhoods. 2. Average trip duration in minutes (rounded to 1 decimal). 3. Average temperature at the start of the trip (rounded to 1 decimal). 4. Average wind speed at the start (in meters per second, rounded to 1 decimal). 5. Average precipitation at the start (in centimeters, rounded to 1 decimal). 6. The month with the most trips (e.g., `4` for April). The data should be grouped by the starting and ending neighborhoods, with:`zip_codes` in `geo_us_boundaries` used to map the bike trip locations based on latitude and longitude. `zip_codes` in `cyclistic` used to obtain the borough and neighborhood names. Using weather data from the Central Park station for the trip date, covering all trips in 2014.",
        "db_id":"NEW_YORK_CITIBIKE_1",
        "external_knowledge":"functions_st_within.md",
        "sql":"WITH \"trips_2014\" AS (\n  SELECT\n    t.*,\n    TO_TIMESTAMP_NTZ(\"starttime\", 6) AS \"start_ts\"\n  FROM \"NEW_YORK_CITIBIKE_1\".\"NEW_YORK_CITIBIKE\".\"CITIBIKE_TRIPS\" t\n  WHERE YEAR(TO_TIMESTAMP_NTZ(\"starttime\", 6)) = 2014\n),\n\"trip_with_zips\" AS (\n  SELECT\n    t.*,\n    CAST(szs.\"zip_code\" AS NUMBER) AS \"start_zip\",\n    CAST(sze.\"zip_code\" AS NUMBER) AS \"end_zip\"\n  FROM \"trips_2014\" t\n  JOIN \"NEW_YORK_CITIBIKE_1\".\"GEO_US_BOUNDARIES\".\"ZIP_CODES\" szs\n    ON szs.\"state_code\" = 'NY'\n   AND ST_WITHIN(ST_POINT(t.\"start_station_longitude\", t.\"start_station_latitude\"), TO_GEOGRAPHY(szs.\"zip_code_geom\"))\n  JOIN \"NEW_YORK_CITIBIKE_1\".\"GEO_US_BOUNDARIES\".\"ZIP_CODES\" sze\n    ON sze.\"state_code\" = 'NY'\n   AND ST_WITHIN(ST_POINT(t.\"end_station_longitude\", t.\"end_station_latitude\"), TO_GEOGRAPHY(sze.\"zip_code_geom\"))\n),\n\"trip_neighborhoods\" AS (\n  SELECT\n    twz.*,\n    czs.\"borough\" AS \"start_borough\",\n    czs.\"neighborhood\" AS \"start_neighborhood\",\n    cze.\"borough\" AS \"end_borough\",\n    cze.\"neighborhood\" AS \"end_neighborhood\"\n  FROM \"trip_with_zips\" twz\n  JOIN \"NEW_YORK_CITIBIKE_1\".\"CYCLISTIC\".\"ZIP_CODES\" czs\n    ON czs.\"zip\" = twz.\"start_zip\"\n  JOIN \"NEW_YORK_CITIBIKE_1\".\"CYCLISTIC\".\"ZIP_CODES\" cze\n    ON cze.\"zip\" = twz.\"end_zip\"\n),\n\"weather_central_park\" AS (\n  SELECT\n    w.\"year\",\n    w.\"mo\",\n    w.\"da\",\n    w.\"wban\",\n    NULLIF(w.\"temp\", 9999.9) AS \"temp_f\",\n    CAST(NULLIF(w.\"wdsp\", '999.9') AS FLOAT) AS \"wdsp_knots\",\n    NULLIF(w.\"prcp\", 99.99) AS \"prcp_inches\"\n  FROM \"NEW_YORK_CITIBIKE_1\".\"NOAA_GSOD\".\"GSOD2014\" w\n  WHERE w.\"wban\" = '94728' AND w.\"year\" = '2014'\n),\n\"trip_with_weather\" AS (\n  SELECT\n    tn.*,\n    wc.\"temp_f\",\n    wc.\"wdsp_knots\",\n    wc.\"prcp_inches\"\n  FROM \"trip_neighborhoods\" tn\n  LEFT JOIN \"weather_central_park\" wc\n    ON wc.\"mo\" = LPAD(CAST(EXTRACT(MONTH FROM tn.\"start_ts\") AS VARCHAR), 2, '0')\n   AND wc.\"da\" = LPAD(CAST(EXTRACT(DAY FROM tn.\"start_ts\") AS VARCHAR), 2, '0')\n   AND wc.\"year\" = '2014'\n),\n\"aggregated\" AS (\n  SELECT\n    \"start_borough\",\n    \"start_neighborhood\",\n    \"end_borough\",\n    \"end_neighborhood\",\n    COUNT(*) AS \"total_trips\",\n    ROUND(AVG(\"tripduration\")\/60, 1) AS \"avg_trip_duration_min\",\n    ROUND(AVG(\"temp_f\"), 1) AS \"avg_temp\",\n    ROUND(AVG(\"wdsp_knots\" * 0.514444), 1) AS \"avg_wind_speed_ms\",\n    ROUND(AVG(\"prcp_inches\" * 2.54), 1) AS \"avg_precip_cm\"\n  FROM \"trip_with_weather\"\n  GROUP BY \"start_borough\",\"start_neighborhood\",\"end_borough\",\"end_neighborhood\"\n),\n\"monthly_counts\" AS (\n  SELECT\n    \"start_neighborhood\",\n    \"end_neighborhood\",\n    EXTRACT(MONTH FROM \"start_ts\") AS \"month_num\",\n    COUNT(*) AS \"trips_in_month\"\n  FROM \"trip_with_weather\"\n  GROUP BY \"start_neighborhood\",\"end_neighborhood\",EXTRACT(MONTH FROM \"start_ts\")\n),\n\"top_month\" AS (\n  SELECT\n    \"start_neighborhood\",\n    \"end_neighborhood\",\n    \"month_num\"\n  FROM (\n    SELECT\n      \"start_neighborhood\",\n      \"end_neighborhood\",\n      \"month_num\",\n      \"trips_in_month\",\n      ROW_NUMBER() OVER (\n        PARTITION BY \"start_neighborhood\",\"end_neighborhood\"\n        ORDER BY \"trips_in_month\" DESC, \"month_num\"\n      ) AS \"rn\"\n    FROM \"monthly_counts\"\n  )\n  WHERE \"rn\" = 1\n)\nSELECT\n  a.\"start_borough\",\n  a.\"start_neighborhood\",\n  a.\"end_borough\",\n  a.\"end_neighborhood\",\n  a.\"total_trips\",\n  a.\"avg_trip_duration_min\",\n  a.\"avg_temp\",\n  a.\"avg_wind_speed_ms\",\n  a.\"avg_precip_cm\",\n  tm.\"month_num\" AS \"month_with_most_trips\"\nFROM \"aggregated\" a\nJOIN \"top_month\" tm\n  ON a.\"start_neighborhood\" = tm.\"start_neighborhood\" AND a.\"end_neighborhood\" = tm.\"end_neighborhood\"\nORDER BY a.\"start_borough\", a.\"start_neighborhood\", a.\"end_borough\", a.\"end_neighborhood\";"
    },
    {
        "instance_id":"sf_bq291",
        "instruction":"Can you provide a daily weather summary for July 2019 within a 5 km radius of latitude 26.75 and longitude 51.5? I need the maximum, minimum, and average temperatures; total precipitation; average cloud cover between 10 AM and 5 PM; total snowfall (when average temperature is below 32\u00b0F); and total rainfall (when average temperature is 32\u00b0F or above) for each forecast date. The data should correspond to forecasts created in July 2019 for the following day.",
        "db_id":"NOAA_GLOBAL_FORECAST_SYSTEM",
        "external_knowledge":"functions_st_within.md",
        "sql":"WITH \"pts\" AS (\n  SELECT\n    \"creation_time\",\n    TO_DATE(TO_TIMESTAMP_NTZ(\"creation_time\" \/ 1000000)) AS \"creation_date\",\n    \"forecast\",\n    \"geography\",\n    \"geography_polygon\"\n  FROM \"NOAA_GLOBAL_FORECAST_SYSTEM\".\"NOAA_GLOBAL_FORECAST_SYSTEM\".\"NOAA_GFS0P25\"\n  WHERE TO_DATE(TO_TIMESTAMP_NTZ(\"creation_time\" \/ 1000000)) BETWEEN '2019-07-01' AND '2019-07-31'\n    AND (\n      (TO_GEOGRAPHY(\"geography\") IS NOT NULL AND (\n         ST_DISTANCE(TO_GEOGRAPHY(\"geography\"), TO_GEOGRAPHY('POINT(51.5 26.75)')) <= 5000\n         OR ST_DISTANCE(TO_GEOGRAPHY(\"geography\"), TO_GEOGRAPHY('POINT(26.75 51.5)')) <= 5000\n       ))\n      OR (TO_GEOGRAPHY(\"geography_polygon\") IS NOT NULL AND (\n         ST_CONTAINS(TO_GEOGRAPHY(\"geography_polygon\"), TO_GEOGRAPHY('POINT(51.5 26.75)'))\n         OR ST_CONTAINS(TO_GEOGRAPHY(\"geography_polygon\"), TO_GEOGRAPHY('POINT(26.75 51.5)'))\n       ))\n    )\n),\n\"flat\" AS (\n  SELECT\n    p.\"creation_date\" AS \"creation_date\",\n    TO_DATE(TO_TIMESTAMP_NTZ((\"f\".\"VALUE\":\"time\")::NUMBER \/ 1000000)) AS \"forecast_date\",\n    EXTRACT(HOUR FROM TO_TIMESTAMP_NTZ((\"f\".\"VALUE\":\"time\")::NUMBER \/ 1000000)) AS \"forecast_hour\",\n    (((\"f\".\"VALUE\":\"temperature_2m_above_ground\")::FLOAT) * 9.0\/5.0) + 32 AS \"temp_f\",\n    COALESCE((\"f\".\"VALUE\":\"total_precipitation_surface\")::FLOAT, 0) AS \"precip\",\n    (\"f\".\"VALUE\":\"total_cloud_cover_entire_atmosphere\")::FLOAT AS \"cloud_cover\"\n  FROM \"pts\" p,\n       LATERAL FLATTEN(input => p.\"forecast\") AS \"f\"\n),\n\"daily\" AS (\n  SELECT\n    \"forecast_date\",\n    MAX(\"temp_f\") AS \"max_temp_f\",\n    MIN(\"temp_f\") AS \"min_temp_f\",\n    AVG(\"temp_f\") AS \"avg_temp_f\",\n    SUM(\"precip\") AS \"total_precipitation\",\n    AVG(CASE WHEN \"forecast_hour\" BETWEEN 10 AND 17 THEN \"cloud_cover\" END) AS \"avg_cloud_cover_10am_5pm\"\n  FROM \"flat\"\n  WHERE \"forecast_date\" = DATEADD('day', 1, \"creation_date\")\n    AND \"forecast_date\" BETWEEN '2019-07-01' AND '2019-07-31'\n  GROUP BY \"forecast_date\"\n)\nSELECT\n  \"forecast_date\",\n  \"max_temp_f\" AS \"max_temperature_f\",\n  \"min_temp_f\" AS \"min_temperature_f\",\n  \"avg_temp_f\" AS \"avg_temperature_f\",\n  \"total_precipitation\",\n  \"avg_cloud_cover_10am_5pm\",\n  CASE WHEN \"avg_temp_f\" < 32 THEN \"total_precipitation\" ELSE 0 END AS \"total_snowfall\",\n  CASE WHEN \"avg_temp_f\" >= 32 THEN \"total_precipitation\" ELSE 0 END AS \"total_rainfall\"\nFROM \"daily\"\nORDER BY \"forecast_date\" ASC;"
    },
    {
        "instance_id":"sf_bq017",
        "instruction":"What are the five longest types of highways within the multipolygon boundary of Denmark (as defined by Wikidata ID 'Q35') by total length, analyzed through planet features?",
        "db_id":"GEO_OPENSTREETMAP",
        "external_knowledge":"functions_st_dwithin.md",
        "sql":"with \"denmark\" as (\n    select st_collect(to_geography(\"PF\".\"geometry\")) as \"geom\"\n    from \"GEO_OPENSTREETMAP\".\"GEO_OPENSTREETMAP\".\"PLANET_FEATURES\" as \"PF\",\n         lateral flatten(input => \"PF\".\"all_tags\") as \"TAG\"\n    where \"PF\".\"feature_type\" = 'multipolygons'\n      and \"TAG\".\"VALUE\":\"key\"::string = 'wikidata'\n      and \"TAG\".\"VALUE\":\"value\"::string = 'Q35'\n),\n\"highway_lines_raw\" as (\n    select\n        \"PF\".\"osm_id\" as \"osm_id\",\n        \"PF\".\"geometry\" as \"geometry\",\n        max(case when \"TAG\".\"VALUE\":\"key\"::string = 'highway' then \"TAG\".\"VALUE\":\"value\"::string end) as \"highway_type\"\n    from \"GEO_OPENSTREETMAP\".\"GEO_OPENSTREETMAP\".\"PLANET_FEATURES\" as \"PF\",\n         lateral flatten(input => \"PF\".\"all_tags\") as \"TAG\"\n    where \"PF\".\"feature_type\" = 'lines'\n    group by \"PF\".\"osm_id\", \"PF\".\"geometry\"\n    having max(case when \"TAG\".\"VALUE\":\"key\"::string = 'highway' then \"TAG\".\"VALUE\":\"value\"::string end) is not null\n),\n\"highway_lines\" as (\n    select\n        \"osm_id\",\n        to_geography(\"geometry\") as \"geom\",\n        \"highway_type\"\n    from \"highway_lines_raw\"\n),\n\"lengths\" as (\n    select\n        \"HL\".\"highway_type\" as \"highway_type\",\n        st_length(st_intersection(\"D\".\"geom\", \"HL\".\"geom\")) as \"length_m\"\n    from \"highway_lines\" as \"HL\"\n    cross join \"denmark\" as \"D\"\n    where st_intersects(\"D\".\"geom\", \"HL\".\"geom\")\n)\nselect\n    \"highway_type\",\n    sum(\"length_m\") as \"total_length_m\"\nfrom \"lengths\"\ngroup by \"highway_type\"\norder by \"total_length_m\" desc\nlimit 5;"
    },
    {
        "instance_id":"sf_bq349",
        "instruction":"Which OpenStreetMap ID from the planet features table corresponds to an administrative boundary, represented as multipolygons, whose total number of 'amenity'-tagged Points of Interest (POIs), as derived from the planet nodes table, is closest to the median count among all such boundaries?",
        "db_id":"GEO_OPENSTREETMAP",
        "external_knowledge":"functions_st_dwithin.md",
        "sql":"WITH admin_candidates AS (\n  SELECT\n    pf.\"osm_id\",\n    pf.\"geometry\",\n    pf.\"osm_timestamp\"\n  FROM GEO_OPENSTREETMAP.GEO_OPENSTREETMAP.PLANET_FEATURES AS pf,\n       LATERAL FLATTEN(INPUT => pf.\"all_tags\") tag\n  WHERE pf.\"feature_type\" = 'multipolygons'\n    AND tag.value:\"key\"::STRING = 'boundary'\n    AND LOWER(TRIM(tag.value:\"value\"::STRING)) = 'administrative'\n    AND pf.\"osm_id\" IS NOT NULL\n    AND pf.\"geometry\" IS NOT NULL\n),\nadmin_polygons AS (\n  SELECT \"osm_id\", \"geometry\"\n  FROM (\n    SELECT ac.*,\n           ROW_NUMBER() OVER (PARTITION BY ac.\"osm_id\" ORDER BY ac.\"osm_timestamp\" DESC NULLS LAST, ac.\"osm_id\" ASC) AS rn\n    FROM admin_candidates ac\n  ) WHERE rn = 1\n),\npolygons_geog AS (\n  SELECT\n    \"osm_id\",\n    ST_GEOGRAPHYFROMWKB(\"geometry\") AS \"geom\"\n  FROM admin_polygons\n),\namenity_nodes AS (\n  SELECT DISTINCT\n    pn.\"id\" AS \"node_id\",\n    pn.\"latitude\" AS \"latitude\",\n    pn.\"longitude\" AS \"longitude\"\n  FROM GEO_OPENSTREETMAP.GEO_OPENSTREETMAP.PLANET_NODES AS pn,\n       LATERAL FLATTEN(INPUT => pn.\"all_tags\") tag\n  WHERE tag.value:\"key\"::STRING = 'amenity'\n    AND pn.\"latitude\" IS NOT NULL\n    AND pn.\"longitude\" IS NOT NULL\n),\nnodes_geog AS (\n  SELECT\n    \"node_id\",\n    ST_MAKEPOINT(\"longitude\", \"latitude\") AS \"pt\"\n  FROM amenity_nodes\n),\ncounts_per_polygon AS (\n  SELECT\n    p.\"osm_id\" AS \"osm_id\",\n    COALESCE(COUNT(DISTINCT n.\"node_id\"), 0) AS \"cnt\"\n  FROM polygons_geog p\n  LEFT JOIN nodes_geog n\n    ON ST_DWITHIN(p.\"geom\", n.\"pt\", 0.0)\n  GROUP BY p.\"osm_id\"\n),\nmedian_val AS (\n  SELECT\n    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY \"cnt\") AS \"median_cnt\"\n  FROM counts_per_polygon\n),\ndistances AS (\n  SELECT\n    c.\"osm_id\",\n    c.\"cnt\",\n    ABS(c.\"cnt\" - m.\"median_cnt\") AS \"dist\"\n  FROM counts_per_polygon c\n  CROSS JOIN median_val m\n)\nSELECT \"osm_id\"\nFROM distances\nORDER BY \"dist\" ASC NULLS LAST, \"osm_id\" ASC\nLIMIT 1;"
    },
    {
        "instance_id":"sf_bq429",
        "instruction":"Which are the top five states with the greatest average difference in median income between 2015 and 2018 at the ZIP code level, and what is the corresponding average number of vulnerable employees across wholesale trade, natural resources and construction, arts and entertainment, information, and retail trade industries in 2017 according to the ACS Five-Year Estimates and ZIP code boundaries data?",
        "db_id":"CENSUS_BUREAU_ACS_2",
        "external_knowledge":"avg_vulnerable_weights.md",
        "sql":"WITH income_diff_by_zip AS (\n    SELECT\n        T1.\"geo_id\",\n        ABS(T2.\"median_income\" - T1.\"median_income\") AS income_difference\n    FROM \"CENSUS_BUREAU_ACS_2\".\"CENSUS_BUREAU_ACS\".\"ZCTA5_2015_5YR\" AS T1\n    JOIN \"CENSUS_BUREAU_ACS_2\".\"CENSUS_BUREAU_ACS\".\"ZCTA5_2018_5YR\" AS T2\n        ON T1.\"geo_id\" = T2.\"geo_id\"\n    WHERE T1.\"median_income\" IS NOT NULL AND T2.\"median_income\" IS NOT NULL\n),\nvulnerable_pop_by_zip AS (\n    SELECT\n        \"geo_id\",\n        (\"employed_wholesale_trade\" * 0.38423645320197042 +\n         (\"employed_agriculture_forestry_fishing_hunting_mining\" + \"employed_construction\") * 0.48071410777129553 +\n         \"employed_arts_entertainment_recreation_accommodation_food\" * 0.89455676291236841 +\n         \"employed_information\" * 0.31315240083507306 +\n         \"employed_retail_trade\" * 0.51) AS vulnerable_employees\n    FROM \"CENSUS_BUREAU_ACS_2\".\"CENSUS_BUREAU_ACS\".\"ZCTA5_2017_5YR\"\n    WHERE\n        \"employed_wholesale_trade\" IS NOT NULL AND\n        \"employed_agriculture_forestry_fishing_hunting_mining\" IS NOT NULL AND\n        \"employed_construction\" IS NOT NULL AND\n        \"employed_arts_entertainment_recreation_accommodation_food\" IS NOT NULL AND\n        \"employed_information\" IS NOT NULL AND\n        \"employed_retail_trade\" IS NOT NULL\n),\njoined_data AS (\n    SELECT\n        T3.\"state_name\",\n        T1.income_difference,\n        T2.vulnerable_employees\n    FROM income_diff_by_zip AS T1\n    JOIN vulnerable_pop_by_zip AS T2\n        ON T1.\"geo_id\" = T2.\"geo_id\"\n    JOIN \"CENSUS_BUREAU_ACS_2\".\"GEO_US_BOUNDARIES\".\"ZIP_CODES\" AS T3\n        ON T1.\"geo_id\" = T3.\"zip_code\"\n)\nSELECT\n    \"state_name\",\n    AVG(income_difference) AS avg_income_diff,\n    AVG(vulnerable_employees) AS avg_vulnerable_employees\nFROM joined_data\nGROUP BY \"state_name\"\nORDER BY avg_income_diff DESC\nLIMIT 5"
    },
    {
        "instance_id":"sf_bq254",
        "instruction":"Among all multipolygons located within the same geographic area as the multipolygon associated with Wikidata item Q191, but lacking a 'wikidata' tag themselves, which two rank highest by the number of points that lie within their boundaries, and what are their names?",
        "db_id":"GEO_OPENSTREETMAP",
        "external_knowledge":"functions_st_dwithin.md",
        "sql":"\/*\nAssumptions & reasoning (see guidelines comments requirement):\n1.  The multipolygon representing Wikidata Q191 (Estonia) is stored in\n    GEO_OPENSTREETMAP.PLANET_FEATURES with feature_type = 'multipolygons'.\n    We fetch its (single) geometry as our target area.\n2.  \u201cSame geographic area\u201d is interpreted as geometries that intersect\n    the Estonia multipolygon \u2013 ST_INTERSECTS is therefore used.\n3.  Candidates are all multipolygons from the same table that  (a) have\n    a non-NULL geometry, (b) do NOT carry any tag with key = 'wikidata'.\n4.  \u2006all_tags is a JSON array; to avoid expensive LATERAL FLATTEN scans\n    on the full table we first pre-filter with NOT ILIKE '%\"wikidata\"%'.\n    The small subset that passes the spatial test is then flattened once\n    to extract the *name* tag.\n5.  Point features are taken from PLANET_FEATURES_POINTS.  Only points\n    with a non-NULL geometry are considered.  ST_CONTAINS(g_poly , g_pt)\n    gives the per-polygon contained-point count.\n6.  We order by descending point count and return the top two with their\n    names (may be NULL if no name tag exists).\n*\/\nWITH target_estonia AS (\n    \/* geometry of the multipolygon tagged wikidata = Q191 (Estonia) *\/\n    SELECT  TO_GEOGRAPHY(pf.\"geometry\") AS geom\n    FROM    \"GEO_OPENSTREETMAP\".\"GEO_OPENSTREETMAP\".\"PLANET_FEATURES\" pf\n            ,LATERAL FLATTEN(input => pf.\"all_tags\") tag\n    WHERE   pf.\"feature_type\" = 'multipolygons'\n      AND   tag.value:\"key\"::string  = 'wikidata'\n      AND   tag.value:\"value\"::string = 'Q191'\n    LIMIT 1\n), candidate_raw AS (\n    \/* multipolygons without a wikidata tag, having geometry, intersecting Estonia *\/\n    SELECT  COALESCE(pf.\"osm_way_id\", pf.\"osm_id\")      AS id,\n            pf.\"geometry\"                                 AS geom_bin,\n            pf.\"all_tags\"                                 AS tags\n    FROM    \"GEO_OPENSTREETMAP\".\"GEO_OPENSTREETMAP\".\"PLANET_FEATURES\" pf\n            CROSS JOIN target_estonia t\n    WHERE   pf.\"feature_type\" = 'multipolygons'\n      AND   pf.\"geometry\" IS NOT NULL\n      AND   LOWER(TO_VARCHAR(pf.\"all_tags\")) NOT ILIKE '%\"wikidata\"%'\n      AND   ST_INTERSECTS(TO_GEOGRAPHY(pf.\"geometry\"), t.geom)\n), name_tags AS (\n    \/* extract (optional) name tag for every candidate *\/\n    SELECT  cr.id,\n            MAX(tag.value:\"value\"::string) AS name_tag\n    FROM    candidate_raw cr,\n            LATERAL FLATTEN(input => cr.tags) tag\n    WHERE   tag.value:\"key\"::string = 'name'\n    GROUP BY cr.id\n), geom_candidates AS (\n    SELECT  id, TO_GEOGRAPHY(geom_bin) AS geom\n    FROM    candidate_raw\n), point_counts AS (\n    SELECT  gc.id                         AS multipolygon_id,\n            nt.name_tag                   AS name,\n            COUNT(*)                      AS point_cnt\n    FROM        geom_candidates gc\n    LEFT JOIN   name_tags nt   ON nt.id = gc.id\n    JOIN        \"GEO_OPENSTREETMAP\".\"GEO_OPENSTREETMAP\".\"PLANET_FEATURES_POINTS\" p\n                ON p.\"geometry\" IS NOT NULL\n               AND ST_CONTAINS(gc.geom, TO_GEOGRAPHY(p.\"geometry\"))\n    GROUP BY gc.id, nt.name_tag\n)\nSELECT   multipolygon_id AS id,\n         name,\n         point_cnt\nFROM     point_counts\nORDER BY point_cnt DESC, name\nLIMIT 2;"
    },
    {
        "instance_id":"sf_bq289",
        "instruction":"Can you find the shortest distance between any two amenities (either a library, place of worship, or community center) located within Philadelphia, analyzed through pennsylvania table and planet features points?",
        "db_id":"GEO_OPENSTREETMAP_CENSUS_PLACES",
        "external_knowledge":"functions_st_contains.md",
        "sql":"WITH philly_amenities AS (\n    SELECT\n        \"p\".\"osm_id\",\n        TO_GEOGRAPHY(\"p\".\"geometry\") AS \"geo\"\n    FROM\n        \"GEO_OPENSTREETMAP_CENSUS_PLACES\".\"GEO_US_CENSUS_PLACES\".\"PLACES_PENNSYLVANIA\" AS \"pa\"\n    JOIN\n        \"GEO_OPENSTREETMAP_CENSUS_PLACES\".\"GEO_OPENSTREETMAP\".\"PLANET_FEATURES_POINTS\" AS \"p\"\n        ON ST_CONTAINS(TO_GEOGRAPHY(\"pa\".\"place_geom\"), TO_GEOGRAPHY(\"p\".\"geometry\"))\n    , TABLE(FLATTEN(\"p\".\"all_tags\")) AS \"t\"\n    WHERE\n        \"pa\".\"place_name\" = 'Philadelphia'\n        AND \"t\".\"VALUE\":\"key\"::STRING = 'amenity'\n        AND \"t\".\"VALUE\":\"value\"::STRING IN ('library', 'place_of_worship', 'community_centre')\n)\nSELECT\n    MIN(ST_DISTANCE(\"a1\".\"geo\", \"a2\".\"geo\"))\nFROM\n    philly_amenities AS \"a1\"\nCROSS JOIN\n    philly_amenities AS \"a2\"\nWHERE\n    \"a1\".\"osm_id\" < \"a2\".\"osm_id\""
    },
    {
        "instance_id":"sf_bq250",
        "instruction":"Based on the most recent 1km population grid data in Singapore before January 2023, using ST_CONVEXHULL to aggregate all population grid centroids into a bounding region and ST_INTERSECTS to identify hospitals from OpenStreetMap\u2019s planet layer (layer_code in (2110, 2120)) that fall within this region, then calculating the distance from each grid cell to its nearest hospital, what is the total population of the grid cell that is farthest from any hospital?",
        "db_id":"GEO_OPENSTREETMAP_WORLDPOP",
        "external_knowledge":"OpenStreetMap_data_in_layered_GIS_format.md",
        "sql":"WITH singapore_hospitals AS (SELECT pl.\"osm_id\", CASE WHEN pl.\"gdal_type\" = 'points' THEN ST_GEOGFROMWKB(pl.\"geometry\") ELSE ST_CENTROID(ST_GEOGFROMWKB(pl.\"geometry\")) END as hospital_point FROM GEO_OPENSTREETMAP_WORLDPOP.GEO_OPENSTREETMAP.PLANET_LAYERS pl WHERE pl.\"layer_class\" = 'poi_health' AND pl.\"layer_name\" = 'hospital' AND pl.\"geometry\" IS NOT NULL AND ST_X(CASE WHEN pl.\"gdal_type\" = 'points' THEN ST_GEOGFROMWKB(pl.\"geometry\") ELSE ST_CENTROID(ST_GEOGFROMWKB(pl.\"geometry\")) END) BETWEEN 103.64 AND 103.99 AND ST_Y(CASE WHEN pl.\"gdal_type\" = 'points' THEN ST_GEOGFROMWKB(pl.\"geometry\") ELSE ST_CENTROID(ST_GEOGFROMWKB(pl.\"geometry\")) END) BETWEEN 1.27 AND 1.45), singapore_population_grids AS (SELECT \"geo_id\", \"population\", \"latitude_centroid\", \"longitude_centroid\", ST_GEOGFROMWKB(\"geog\") as grid_point FROM GEO_OPENSTREETMAP_WORLDPOP.WORLDPOP.POPULATION_GRID_1KM WHERE \"alpha_3_code\" = 'SGP' AND \"last_updated\" = '2020-01-01' AND \"population\" > 0), grid_distances AS (SELECT spg.\"geo_id\", spg.\"population\", MIN(ST_DISTANCE(spg.grid_point, sh.hospital_point)) as min_distance_to_hospital FROM singapore_population_grids spg CROSS JOIN singapore_hospitals sh GROUP BY spg.\"geo_id\", spg.\"population\"), max_distance AS (SELECT MAX(min_distance_to_hospital) as maximum_distance FROM grid_distances), farthest_grid AS (SELECT gd.\"geo_id\", gd.\"population\", gd.min_distance_to_hospital FROM grid_distances gd, max_distance md WHERE gd.min_distance_to_hospital = md.maximum_distance) SELECT SUM(\"population\") as total_population FROM farthest_grid"
    },
    {
        "instance_id":"sf_bq083",
        "instruction":"Can you calculate the daily change in the market value of USDC tokens (address `0xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48`) for 2023, based on Ethereum transactions? The change should be computed from minting (input pattern `0x40c10f19%`) and burning (input pattern `0x42966c68%`) operations. For each transaction, minting should be positive and burning negative. Extract the relevant amount from the 'input' field as a hexadecimal, convert it to millions, express it in USD format. Group the results by date and order them in descending order.",
        "db_id":"CRYPTO",
        "external_knowledge":"Total_Market_Value_Change.md",
        "sql":"SELECT\n    TO_DATE(TO_TIMESTAMP_NTZ(\"block_timestamp\" \/ 1000000)) AS \"Date\",\n    TO_CHAR(\n        SUM(\n            (CASE WHEN \"input\" LIKE '0x40c10f19%' THEN 1 ELSE -1 END) *\n            CAST('0x' || LTRIM(\n                SUBSTRING(\"input\", CASE WHEN \"input\" LIKE '0x40c10f19%' THEN 75 ELSE 11 END, 64),\n                '0'\n            ) AS FLOAT) \/ 1000000\n        ),\n        '$9,999,999,999.00'\n    ) AS \"\u0394 Total Market Value\"\nFROM \"CRYPTO\".\"CRYPTO_ETHEREUM\".\"TRANSACTIONS\"\nWHERE\n    \"to_address\" = '0xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48'\n    AND (\"input\" LIKE '0x40c10f19%' OR \"input\" LIKE '0x42966c68%')\n    AND \"block_timestamp\" >= 1672531200000000 AND \"block_timestamp\" < 1704067200000000\nGROUP BY\n    \"Date\"\nORDER BY\n    \"Date\" DESC;"
    },
    {
        "instance_id":"sf_bq341",
        "instruction":"Which Ethereum address has the top 3 smallest positive balance from transactions involving the token at address \"0xa92a861fc11b99b24296af880011b47f9cafb5ab\"?",
        "db_id":"CRYPTO",
        "external_knowledge":null,
        "sql":"WITH address_balances AS (\n    SELECT \n        address,\n        SUM(CASE WHEN direction = 'in' THEN value_numeric ELSE -value_numeric END) AS balance\n    FROM (\n        SELECT \n            \"to_address\" AS address,\n            CAST(\"value\" AS NUMBER(38,0)) AS value_numeric,\n            'in' AS direction\n        FROM CRYPTO.CRYPTO_ETHEREUM.TOKEN_TRANSFERS \n        WHERE \"token_address\" = '0xa92a861fc11b99b24296af880011b47f9cafb5ab'\n        \n        UNION ALL\n        \n        SELECT \n            \"from_address\" AS address,\n            CAST(\"value\" AS NUMBER(38,0)) AS value_numeric,\n            'out' AS direction\n        FROM CRYPTO.CRYPTO_ETHEREUM.TOKEN_TRANSFERS \n        WHERE \"token_address\" = '0xa92a861fc11b99b24296af880011b47f9cafb5ab'\n    )\n    WHERE address != '0x0000000000000000000000000000000000000000'\n    GROUP BY address\n)\nSELECT address\nFROM address_balances\nWHERE balance > 0\nORDER BY balance ASC\nLIMIT 3"
    },
    {
        "instance_id":"sf_bq444",
        "instruction":"Can you pull the blockchain timestamp, block number, and transaction hash for the first five mint and burn events from Ethereum logs for the address '0x8ad599c3a0ff1de082011efddc58f1908eb6e6d8'? Please include mint events identified by the topic '0x7a53080ba414158be7ec69b987b5fb7d07dee101fe85488f0853ae16239d0bde' and burn events by '0x0c396cd989a39f4459b5fa1aed6a9a8dcdbc45908acfd67e028cd568da98982c', and order them by block timestamp from the oldest to the newest.",
        "db_id":"CRYPTO",
        "external_knowledge":"ethereum_logs_and_events_overview.md",
        "sql":"WITH events AS (\n    SELECT\n        CASE\n            WHEN \"topics\"[0]::STRING = '0x7a53080ba414158be7ec69b987b5fb7d07dee101fe85488f0853ae16239d0bde' THEN 'MINT'\n            ELSE 'BURN'\n        END AS \"event_type\",\n        \"block_timestamp\",\n        \"block_number\",\n        \"transaction_hash\",\n        \"log_index\"\n    FROM \"CRYPTO\".\"CRYPTO_ETHEREUM\".\"LOGS\"\n    WHERE LOWER(\"address\") = '0x8ad599c3a0ff1de082011efddc58f1908eb6e6d8'\n      AND \"topics\"[0]::STRING IN (\n          '0x7a53080ba414158be7ec69b987b5fb7d07dee101fe85488f0853ae16239d0bde',\n          '0x0c396cd989a39f4459b5fa1aed6a9a8dcdbc45908acfd67e028cd568da98982c'\n      )\n),\nranked_events AS (\n    SELECT\n        \"event_type\",\n        \"block_timestamp\",\n        \"block_number\",\n        \"transaction_hash\",\n        ROW_NUMBER() OVER (\n            PARTITION BY \"event_type\"\n            ORDER BY \"block_timestamp\", \"block_number\", \"log_index\"\n        ) AS \"rn\"\n    FROM events\n)\nSELECT\n    \"event_type\",\n    TO_TIMESTAMP_NTZ(\"block_timestamp\" \/ 1000000) AS \"block_timestamp\",\n    \"block_number\",\n    \"transaction_hash\"\nFROM ranked_events\nWHERE \"rn\" <= 5\nORDER BY \"block_timestamp\", \"block_number\", \"event_type\", \"transaction_hash\""
    },
    {
        "instance_id":"sf_bq334",
        "instruction":"Calculate the annual differences in Bitcoin output value averages between two methods: Merged input\/output records: Combine the inputs and outputs tables, filter to only output records, and calculate yearly averages. Transactions table: Directly use the output_value field from the transactions table for yearly averages. Show the difference (merged outputs average minus transactions average) only for years with data in both methods.",
        "db_id":"CRYPTO",
        "external_knowledge":null,
        "sql":"WITH \"merged_union\" AS (\n  SELECT\n    \"block_timestamp\",\n    \"value\",\n    'output' AS \"origin\"\n  FROM \"CRYPTO\".\"CRYPTO_BITCOIN\".\"OUTPUTS\"\n  UNION ALL\n  SELECT\n    \"block_timestamp\",\n    \"value\",\n    'input' AS \"origin\"\n  FROM \"CRYPTO\".\"CRYPTO_BITCOIN\".\"INPUTS\"\n),\n\"merged_outputs_avg\" AS (\n  SELECT\n    DATE_PART(year, TO_TIMESTAMP_NTZ(\"block_timestamp\", 6)) AS \"year\",\n    AVG(\"value\") AS \"merged_avg\"\n  FROM \"merged_union\"\n  WHERE \"origin\" = 'output'\n    AND \"value\" IS NOT NULL\n    AND \"block_timestamp\" IS NOT NULL\n  GROUP BY 1\n),\n\"transactions_avg\" AS (\n  SELECT\n    DATE_PART(year, TO_TIMESTAMP_NTZ(\"block_timestamp\", 6)) AS \"year\",\n    AVG(\"output_value\") AS \"tx_avg\"\n  FROM \"CRYPTO\".\"CRYPTO_BITCOIN\".\"TRANSACTIONS\"\n  WHERE \"output_value\" IS NOT NULL\n    AND \"block_timestamp\" IS NOT NULL\n  GROUP BY 1\n)\nSELECT\n  m.\"year\",\n  m.\"merged_avg\" - t.\"tx_avg\" AS \"difference\"\nFROM \"merged_outputs_avg\" m\nJOIN \"transactions_avg\" t\n  ON m.\"year\" = t.\"year\"\nORDER BY m.\"year\";"
    },
    {
        "instance_id":"sf_bq057",
        "instruction":"Which month (e.g., 3 for March) in 2021 witnessed the highest percentage of Bitcoin transaction volume occurring in CoinJoin transactions (defined as transactions with >2 outputs, output value \u2264 input value, and having multiple equal-value outputs)? Also provide the percentage of all Bitcoin transactions that were CoinJoins, the percentage of UTXOs involved in CoinJoin transactions (average of input and output percentages), and the percentage of total Bitcoin volume that occurred in CoinJoin transactions for that month. Round all percentages to 1 decimal place.",
        "db_id":"CRYPTO",
        "external_knowledge":null,
        "sql":"WITH monthly_transactions AS (\n    SELECT \n        EXTRACT(MONTH FROM TO_TIMESTAMP(\"block_timestamp\"\/1000000)) as month_num,\n        \"hash\",\n        \"input_count\",\n        \"output_count\", \n        \"input_value\",\n        \"output_value\",\n        \"outputs\"\n    FROM CRYPTO.CRYPTO_BITCOIN.TRANSACTIONS \n    WHERE EXTRACT(YEAR FROM TO_TIMESTAMP(\"block_timestamp\"\/1000000)) = 2021\n      AND \"is_coinbase\" = FALSE\n),\npotential_coinjoins AS (\n    SELECT \n        month_num,\n        \"hash\",\n        \"input_count\",\n        \"output_count\",\n        \"input_value\",\n        \"output_value\",\n        \"outputs\"\n    FROM monthly_transactions\n    WHERE \"output_count\" > 2\n      AND \"output_value\" <= \"input_value\"\n),\ntransactions_with_output_values AS (\n    SELECT \n        t.month_num,\n        t.\"hash\",\n        t.\"input_count\",\n        t.\"output_count\",\n        t.\"input_value\", \n        t.\"output_value\",\n        output_flat.value:\"value\"::NUMBER as output_val\n    FROM potential_coinjoins t,\n    LATERAL FLATTEN(input => t.\"outputs\") output_flat\n),\nequal_value_groups AS (\n    SELECT \n        month_num,\n        \"hash\",\n        \"input_count\",\n        \"output_count\",\n        \"input_value\",\n        \"output_value\",\n        output_val,\n        COUNT(*) OVER (PARTITION BY \"hash\", output_val) as same_value_count\n    FROM transactions_with_output_values\n    WHERE output_val > 0\n),\ncoinjoin_transactions AS (\n    SELECT DISTINCT \n        month_num,\n        \"hash\",\n        \"input_count\", \n        \"output_count\",\n        \"input_value\",\n        \"output_value\"\n    FROM equal_value_groups\n    WHERE same_value_count >= 2\n),\nmonthly_coinjoin_stats AS (\n    SELECT \n        month_num,\n        COUNT(*) as coinjoin_count,\n        SUM(\"input_count\") as total_coinjoin_inputs,\n        SUM(\"output_count\") as total_coinjoin_outputs,\n        SUM(\"output_value\") as total_coinjoin_volume\n    FROM coinjoin_transactions\n    GROUP BY month_num\n),\nmonthly_total_stats AS (\n    SELECT \n        month_num,\n        COUNT(*) as total_transactions,\n        SUM(\"input_count\") as total_inputs,\n        SUM(\"output_count\") as total_outputs,\n        SUM(\"output_value\") as total_volume\n    FROM monthly_transactions\n    GROUP BY month_num\n),\nmonthly_percentages AS (\n    SELECT \n        t.month_num,\n        COALESCE(c.coinjoin_count, 0) as coinjoin_count,\n        t.total_transactions,\n        COALESCE(c.total_coinjoin_inputs, 0) as total_coinjoin_inputs,\n        COALESCE(c.total_coinjoin_outputs, 0) as total_coinjoin_outputs,\n        t.total_inputs,\n        t.total_outputs,\n        COALESCE(c.total_coinjoin_volume, 0) as total_coinjoin_volume,\n        t.total_volume,\n        ROUND((COALESCE(c.total_coinjoin_volume, 0)::DECIMAL \/ t.total_volume) * 100, 1) as pct_volume_coinjoin\n    FROM monthly_total_stats t\n    LEFT JOIN monthly_coinjoin_stats c ON t.month_num = c.month_num\n),\nhighest_month AS (\n    SELECT \n        month_num,\n        pct_volume_coinjoin,\n        ROW_NUMBER() OVER (ORDER BY pct_volume_coinjoin DESC) as rank\n    FROM monthly_percentages\n)\nSELECT \n    mp.month_num as month_with_highest_coinjoin_pct,\n    ROUND((mp.coinjoin_count::DECIMAL \/ mp.total_transactions) * 100, 1) as pct_transactions_coinjoin,\n    ROUND(((mp.total_coinjoin_inputs::DECIMAL \/ mp.total_inputs) + (mp.total_coinjoin_outputs::DECIMAL \/ mp.total_outputs)) * 50, 1) as pct_utxos_coinjoin,\n    mp.pct_volume_coinjoin\nFROM monthly_percentages mp\nJOIN highest_month hm ON mp.month_num = hm.month_num\nWHERE hm.rank = 1"
    },
    {
        "instance_id":"sf_bq068",
        "instruction":"Using double-entry bookkeeping principles by treating transaction inputs as debits (negative values) and outputs as credits (positive values) for all Bitcoin Cash transactions between 2014-03-01 and 2014-04-01, how can we calculate the maximum and minimum final balances grouped by address type from these transactions?",
        "db_id":"CRYPTO",
        "external_knowledge":null,
        "sql":"WITH date_range AS (\n    SELECT \n        DATE_PART(EPOCH_MICROSECOND, '2014-03-01'::TIMESTAMP) AS start_timestamp,\n        DATE_PART(EPOCH_MICROSECOND, '2014-04-01'::TIMESTAMP) AS end_timestamp\n),\naddress_transactions AS (\n    -- Get all debits (inputs) within the date range\n    SELECT \n        FLATTENED.value::STRING AS address,\n        i.\"type\" AS address_type,\n        -i.\"value\" AS amount,\n        i.\"block_timestamp\"\n    FROM \"CRYPTO\".\"CRYPTO_BITCOIN_CASH\".\"INPUTS\" i,\n    LATERAL FLATTEN(input => PARSE_JSON(i.\"addresses\")) FLATTENED\n    WHERE i.\"block_timestamp\" BETWEEN (SELECT start_timestamp FROM date_range) AND (SELECT end_timestamp FROM date_range)\n    \n    UNION ALL\n    \n    -- Get all credits (outputs) within the date range\n    SELECT \n        FLATTENED.value::STRING AS address,\n        o.\"type\" AS address_type,\n        o.\"value\" AS amount,\n        o.\"block_timestamp\"\n    FROM \"CRYPTO\".\"CRYPTO_BITCOIN_CASH\".\"OUTPUTS\" o,\n    LATERAL FLATTEN(input => PARSE_JSON(o.\"addresses\")) FLATTENED\n    WHERE o.\"block_timestamp\" BETWEEN (SELECT start_timestamp FROM date_range) AND (SELECT end_timestamp FROM date_range)\n),\naddress_balances AS (\n    SELECT \n        address,\n        address_type,\n        SUM(amount) AS final_balance\n    FROM address_transactions\n    GROUP BY address, address_type\n)\nSELECT \n    address_type,\n    MAX(final_balance) AS max_final_balance,\n    MIN(final_balance) AS min_final_balance\nFROM address_balances\nGROUP BY address_type\nORDER BY address_type;"
    },
    {
        "instance_id":"sf_bq093",
        "instruction":"What were the maximum and minimum net balance changes for Ethereum Classic addresses on October 14, 2016? Calculate these by summing all transactions where addresses received funds (debits), sent funds (credits), and paid or received gas fees. Only include successful status transactions and exclude internal calls of types. For gas fees, consider both the fees paid by transaction senders and received by miners, calculated as multiplied by the gas price for both miners and senders",
        "db_id":"CRYPTO",
        "external_knowledge":null,
        "sql":"WITH \"FILTERED_TX\" AS (\n    SELECT\n        t.\"hash\",\n        t.\"from_address\",\n        COALESCE(t.\"to_address\", t.\"receipt_contract_address\") AS \"to_address\",\n        COALESCE(t.\"value\", 0) AS \"value\",\n        COALESCE(t.\"receipt_gas_used\", 0) AS \"receipt_gas_used\",\n        COALESCE(t.\"gas_price\", 0) AS \"gas_price\",\n        COALESCE(t.\"receipt_gas_used\", 0) * COALESCE(t.\"gas_price\", 0) AS \"gas_fee\",\n        b.\"miner\"\n    FROM\n        CRYPTO.CRYPTO_ETHEREUM_CLASSIC.\"TRANSACTIONS\" t\n        JOIN CRYPTO.CRYPTO_ETHEREUM_CLASSIC.\"BLOCKS\" b\n            ON t.\"block_number\" = b.\"number\"\n    WHERE\n        t.\"receipt_status\" = 1\n        AND TO_DATE(TO_TIMESTAMP_NTZ(t.\"block_timestamp\" \/ 1000000)) = '2016-10-14'\n),\n\"ADDRESS_CHANGES\" AS (\n    SELECT\n        \"address\",\n        SUM(\"amount\") AS \"net_change\"\n    FROM (\n        SELECT\n            t.\"from_address\" AS \"address\",\n            -CAST(t.\"value\" AS NUMBER(38, 9)) AS \"amount\"\n        FROM\n            \"FILTERED_TX\" t\n        UNION ALL\n        SELECT\n            t.\"to_address\" AS \"address\",\n            CAST(t.\"value\" AS NUMBER(38, 9)) AS \"amount\"\n        FROM\n            \"FILTERED_TX\" t\n        WHERE\n            t.\"to_address\" IS NOT NULL\n        UNION ALL\n        SELECT\n            t.\"from_address\" AS \"address\",\n            -CAST(t.\"gas_fee\" AS NUMBER(38, 9)) AS \"amount\"\n        FROM\n            \"FILTERED_TX\" t\n        UNION ALL\n        SELECT\n            t.\"miner\" AS \"address\",\n            CAST(t.\"gas_fee\" AS NUMBER(38, 9)) AS \"amount\"\n        FROM\n            \"FILTERED_TX\" t\n    ) AS contributions\n    WHERE\n        \"address\" IS NOT NULL\n    GROUP BY\n        \"address\"\n),\n\"MAX_ADDR\" AS (\n    SELECT\n        \"address\",\n        \"net_change\"\n    FROM\n        \"ADDRESS_CHANGES\"\n    QUALIFY ROW_NUMBER() OVER (ORDER BY \"net_change\" DESC, \"address\") = 1\n),\n\"MIN_ADDR\" AS (\n    SELECT\n        \"address\",\n        \"net_change\"\n    FROM\n        \"ADDRESS_CHANGES\"\n    QUALIFY ROW_NUMBER() OVER (ORDER BY \"net_change\" ASC, \"address\") = 1\n)\nSELECT\n    metrics.\"metric\",\n    metrics.\"address\",\n    metrics.\"net_change\"\nFROM (\n    SELECT\n        'MAX' AS \"metric\",\n        max_addr.\"address\",\n        COALESCE(max_addr.\"net_change\", stats.\"max_change\", 0) AS \"net_change\"\n    FROM\n        (SELECT MAX(\"net_change\") AS \"max_change\" FROM \"ADDRESS_CHANGES\") stats\n        LEFT JOIN \"MAX_ADDR\" max_addr ON 1 = 1\n    UNION ALL\n    SELECT\n        'MIN' AS \"metric\",\n        min_addr.\"address\",\n        COALESCE(min_addr.\"net_change\", stats.\"min_change\", 0) AS \"net_change\"\n    FROM\n        (SELECT MIN(\"net_change\") AS \"min_change\" FROM \"ADDRESS_CHANGES\") stats\n        LEFT JOIN \"MIN_ADDR\" min_addr ON 1 = 1\n) metrics\nORDER BY\n    metrics.\"metric\" DESC;"
    },
    {
        "instance_id":"sf_bq037",
        "instruction":"About the refined human genetic variations collected in phase 3 on 2015-02-20, I want to know the minimum and maximum start positions as well as the proportions of these two respectively for reference bases 'AT' and 'TA'.",
        "db_id":"HUMAN_GENOME_VARIANTS",
        "external_knowledge":null,
        "sql":"WITH min_max AS (\n    SELECT \n        \"reference_bases\",\n        MIN(\"start_position\") as min_start,\n        MAX(\"start_position\") as max_start,\n        COUNT(*) as total_count\n    FROM HUMAN_GENOME_VARIANTS.HUMAN_GENOME_VARIANTS._1000_GENOMES_PHASE_3_OPTIMIZED_SCHEMA_VARIANTS_20150220\n    WHERE \"reference_bases\" IN ('AT', 'TA')\n    GROUP BY \"reference_bases\"\n),\nmin_counts AS (\n    SELECT \n        v.\"reference_bases\",\n        v.\"start_position\",\n        COUNT(*) as extreme_count\n    FROM HUMAN_GENOME_VARIANTS.HUMAN_GENOME_VARIANTS._1000_GENOMES_PHASE_3_OPTIMIZED_SCHEMA_VARIANTS_20150220 v\n    INNER JOIN min_max mm ON v.\"reference_bases\" = mm.\"reference_bases\"\n    WHERE v.\"start_position\" = mm.min_start\n    GROUP BY v.\"reference_bases\", v.\"start_position\"\n),\nmax_counts AS (\n    SELECT \n        v.\"reference_bases\",\n        v.\"start_position\",\n        COUNT(*) as extreme_count\n    FROM HUMAN_GENOME_VARIANTS.HUMAN_GENOME_VARIANTS._1000_GENOMES_PHASE_3_OPTIMIZED_SCHEMA_VARIANTS_20150220 v\n    INNER JOIN min_max mm ON v.\"reference_bases\" = mm.\"reference_bases\"\n    WHERE v.\"start_position\" = mm.max_start\n    GROUP BY v.\"reference_bases\", v.\"start_position\"\n)\nSELECT \n    mm.\"reference_bases\",\n    mm.min_start,\n    mm.max_start,\n    COALESCE(mc.extreme_count, 0) * 1.0 \/ mm.total_count AS prop_min,\n    COALESCE(mx.extreme_count, 0) * 1.0 \/ mm.total_count AS prop_max\nFROM min_max mm\nLEFT JOIN min_counts mc ON mm.\"reference_bases\" = mc.\"reference_bases\"\nLEFT JOIN max_counts mx ON mm.\"reference_bases\" = mx.\"reference_bases\"\nORDER BY mm.\"reference_bases\";"
    },
    {
        "instance_id":"sf_bq012",
        "instruction":"Calculate the average balance (in quadrillions, 10^15) of the top 10 Ethereum addresses by net balance, including incoming and outgoing transfers from traces (only successful transactions and excluding call types like delegatecall, callcode, and staticcall), miner rewards (sum of gas fees per block), and sender gas fee deductions. Exclude null addresses and round the result to two decimal places.",
        "db_id":"ETHEREUM_BLOCKCHAIN",
        "external_knowledge":null,
        "sql":"WITH tx_fees AS (\n  SELECT\n    t.\"from_address\" AS sender,\n    b.\"miner\" AS miner,\n    (t.\"receipt_gas_used\" * t.\"gas_price\") AS fee\n  FROM \"ETHEREUM_BLOCKCHAIN\".\"ETHEREUM_BLOCKCHAIN\".\"TRANSACTIONS\" t\n  JOIN \"ETHEREUM_BLOCKCHAIN\".\"ETHEREUM_BLOCKCHAIN\".\"BLOCKS\" b\n    ON t.\"block_hash\" = b.\"hash\"\n  WHERE t.\"receipt_status\" = 1\n),\nminer_fee_income AS (\n  SELECT miner AS address, SUM(fee) AS amount\n  FROM tx_fees\n  WHERE miner IS NOT NULL\n  GROUP BY miner\n),\nsender_fee_deduction AS (\n  SELECT sender AS address, -SUM(fee) AS amount\n  FROM tx_fees\n  WHERE sender IS NOT NULL\n  GROUP BY sender\n),\ntrace_inflows AS (\n  SELECT\n    tr.\"to_address\" AS address,\n    SUM(tr.\"value\") AS amount\n  FROM \"ETHEREUM_BLOCKCHAIN\".\"ETHEREUM_BLOCKCHAIN\".\"TRACES\" tr\n  WHERE tr.\"status\" = 1\n    AND tr.\"to_address\" IS NOT NULL\n    AND (tr.\"trace_type\" IS NULL OR LOWER(tr.\"trace_type\") != 'reward')\n    AND (tr.\"call_type\" IS NULL OR LOWER(tr.\"call_type\") NOT IN ('delegatecall','callcode','staticcall'))\n  GROUP BY tr.\"to_address\"\n),\ntrace_outflows AS (\n  SELECT\n    tr.\"from_address\" AS address,\n    -SUM(tr.\"value\") AS amount\n  FROM \"ETHEREUM_BLOCKCHAIN\".\"ETHEREUM_BLOCKCHAIN\".\"TRACES\" tr\n  WHERE tr.\"status\" = 1\n    AND tr.\"from_address\" IS NOT NULL\n    AND (tr.\"trace_type\" IS NULL OR LOWER(tr.\"trace_type\") != 'reward')\n    AND (tr.\"call_type\" IS NULL OR LOWER(tr.\"call_type\") NOT IN ('delegatecall','callcode','staticcall'))\n  GROUP BY tr.\"from_address\"\n),\nall_flows AS (\n  SELECT address, amount FROM trace_inflows\n  UNION ALL\n  SELECT address, amount FROM trace_outflows\n  UNION ALL\n  SELECT address, amount FROM miner_fee_income\n  UNION ALL\n  SELECT address, amount FROM sender_fee_deduction\n),\nnet_balances AS (\n  SELECT address, SUM(amount) AS net_balance\n  FROM all_flows\n  WHERE address IS NOT NULL\n  GROUP BY address\n),\ntop10 AS (\n  SELECT address, net_balance\n  FROM net_balances\n  ORDER BY net_balance DESC\n  LIMIT 10\n)\nSELECT ROUND(AVG(net_balance \/ POWER(10, 15)), 2) AS avg_balance_quadrillions\nFROM top10;"
    },
    {
        "instance_id":"sf_bq187",
        "instruction":"Calculate the total circulating supply of 'BNB' tokens (in units divided by 10^18) by summing balances of all non-zero addresses, where each address\u2019s balance equals its total received BNB minus sent BNB. Exclude transactions involving the zero address (0x000...) for both senders and receivers.",
        "db_id":"ETHEREUM_BLOCKCHAIN",
        "external_knowledge":null,
        "sql":"WITH inflows AS (\n  SELECT \n    \"to_address\" AS address,\n    SUM(CAST(\"value\" AS NUMBER)) AS total_received\n  FROM \"ETHEREUM_BLOCKCHAIN\".\"ETHEREUM_BLOCKCHAIN\".\"TOKEN_TRANSFERS\"\n  WHERE \"token_address\" = '0xb8c77482e45f1f44de1745f52c74426c631bdd52'\n    AND \"to_address\" != '0x0000000000000000000000000000000000000000'\n  GROUP BY \"to_address\"\n),\noutflows AS (\n  SELECT \n    \"from_address\" AS address,\n    SUM(CAST(\"value\" AS NUMBER)) AS total_sent\n  FROM \"ETHEREUM_BLOCKCHAIN\".\"ETHEREUM_BLOCKCHAIN\".\"TOKEN_TRANSFERS\"\n  WHERE \"token_address\" = '0xb8c77482e45f1f44de1745f52c74426c631bdd52'\n    AND \"from_address\" != '0x0000000000000000000000000000000000000000'\n  GROUP BY \"from_address\"\n),\nall_addresses AS (\n  SELECT address FROM inflows\n  UNION\n  SELECT address FROM outflows\n),\nnet_balances AS (\n  SELECT \n    aa.address,\n    COALESCE(i.total_received, 0) - COALESCE(o.total_sent, 0) AS balance\n  FROM all_addresses aa\n  LEFT JOIN inflows i ON aa.address = i.address\n  LEFT JOIN outflows o ON aa.address = o.address\n)\nSELECT \n  SUM(CASE WHEN balance > 0 THEN balance ELSE 0 END) \/ POWER(10, 18) AS total_circulating_supply\nFROM net_balances"
    },
    {
        "instance_id":"sf_bq294",
        "instruction":"Could you provide the details of the top 5 longest bike share trips that started between July 1, 2017, and December 31, 2017, including the trip ID, duration in seconds, start date, start station name, route (derived from start station name to end station name), bike number, subscriber type, member's birth year, the member's current age (calculated using the current year), an age classification based on whether the member is younger than 40, between 40 and 60, or older than 60, the member's gender, and the name of the region of the start station? Please exclude any trips where the start station name, member's birth year, or member's gender is not specified.",
        "db_id":"SAN_FRANCISCO_PLUS",
        "external_knowledge":"trip_info.md",
        "sql":"SELECT\n  t.\"trip_id\" AS \"trip_id\",\n  t.\"duration_sec\" AS \"duration_sec\",\n  TO_TIMESTAMP_NTZ(t.\"start_date\" \/ 1000000) AS \"start_date\",\n  t.\"start_station_name\" AS \"start_station_name\",\n  (t.\"start_station_name\" || ' - ' || t.\"end_station_name\") AS \"route\",\n  t.\"bike_number\" AS \"bike_number\",\n  t.\"subscriber_type\" AS \"subscriber_type\",\n  CAST(t.\"member_birth_year\" AS INT) AS \"member_birth_year\",\n  (EXTRACT(YEAR FROM CURRENT_DATE) - CAST(t.\"member_birth_year\" AS INT)) AS \"age\",\n  CASE\n    WHEN (EXTRACT(YEAR FROM CURRENT_DATE) - CAST(t.\"member_birth_year\" AS INT)) < 40 THEN 'Young (<40 Y.O)'\n    WHEN (EXTRACT(YEAR FROM CURRENT_DATE) - CAST(t.\"member_birth_year\" AS INT)) BETWEEN 40 AND 60 THEN 'Adult (40-60 Y.O)'\n    ELSE 'Senior Adult (>60 Y.O)'\n  END AS \"age_class\",\n  t.\"member_gender\" AS \"member_gender\",\n  r.\"name\" AS \"region_name\"\nFROM \"SAN_FRANCISCO_PLUS\".\"SAN_FRANCISCO_BIKESHARE\".\"BIKESHARE_TRIPS\" t\nLEFT JOIN \"SAN_FRANCISCO_PLUS\".\"SAN_FRANCISCO_BIKESHARE\".\"BIKESHARE_STATION_INFO\" si\n  ON si.\"station_id\" = CAST(t.\"start_station_id\" AS VARCHAR)\nLEFT JOIN \"SAN_FRANCISCO_PLUS\".\"SAN_FRANCISCO_BIKESHARE\".\"BIKESHARE_REGIONS\" r\n  ON r.\"region_id\" = si.\"region_id\"\nWHERE\n  t.\"start_station_name\" IS NOT NULL AND TRIM(t.\"start_station_name\") != ''\n  AND t.\"member_birth_year\" IS NOT NULL\n  AND t.\"member_gender\" IS NOT NULL AND TRIM(t.\"member_gender\") != ''\n  AND TO_TIMESTAMP_NTZ(t.\"start_date\" \/ 1000000) BETWEEN TO_TIMESTAMP_NTZ('2017-07-01 00:00:00') AND TO_TIMESTAMP_NTZ('2017-12-31 23:59:59')\nORDER BY t.\"duration_sec\" DESC\nLIMIT 5;"
    },
    {
        "instance_id":"sf_bq260",
        "instruction":"From January 1, 2019, to April 30, 2022, how many users are at the youngest age and how many users are at the oldest age for each gender in the e-commerce platform, counting both youngest and oldest users separately for each gender?",
        "db_id":"THELOOK_ECOMMERCE",
        "external_knowledge":null,
        "sql":"WITH FilteredUsers AS (\n  SELECT\n    \"gender\",\n    \"age\"\n  FROM \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"USERS\"\n  WHERE\n    TO_TIMESTAMP_NTZ(\"created_at\" \/ 1000000) >= '2019-01-01' AND TO_TIMESTAMP_NTZ(\"created_at\" \/ 1000000) < '2022-05-01'\n), AgeBounds AS (\n  SELECT\n    \"gender\",\n    MIN(\"age\") AS min_age,\n    MAX(\"age\") AS max_age\n  FROM FilteredUsers\n  GROUP BY\n    \"gender\"\n)\nSELECT\n  T1.\"gender\",\n  COUNT(CASE WHEN T1.\"age\" = T2.min_age THEN 1 END) AS youngest_count,\n  COUNT(CASE WHEN T1.\"age\" = T2.max_age THEN 1 END) AS oldest_count\nFROM FilteredUsers AS T1\nJOIN AgeBounds AS T2\n  ON T1.\"gender\" = T2.\"gender\"\nWHERE\n  T1.\"age\" = T2.min_age OR T1.\"age\" = T2.max_age\nGROUP BY\n  T1.\"gender\";"
    },
    {
        "instance_id":"sf_bq263",
        "instruction":"Please create a month-by-month report for the year 2023 that focuses on the 'Sleep & Lounge' category, showing for each month the total sales, total cost, number of complete orders, total profit, and the profit-to-cost ratio, ensuring that the order is marked as 'Complete,' the creation date is between January 1, 2023, and December 31, 2023, and the cost data is accurately associated with the corresponding product through the order items. ",
        "db_id":"THELOOK_ECOMMERCE",
        "external_knowledge":null,
        "sql":"SELECT\n  DATE_TRUNC('month', TO_TIMESTAMP_NTZ(\"O\".\"created_at\" \/ 1000000)) AS \"month\",\n  SUM(\"OI\".\"sale_price\") AS \"total_sales\",\n  SUM(\"P\".\"cost\") AS \"total_cost\",\n  COUNT(DISTINCT \"O\".\"order_id\") AS \"complete_orders\",\n  SUM(\"OI\".\"sale_price\" - \"P\".\"cost\") AS \"total_profit\",\n  CASE WHEN SUM(\"P\".\"cost\") = 0 THEN NULL ELSE SUM(\"OI\".\"sale_price\" - \"P\".\"cost\") \/ SUM(\"P\".\"cost\") END AS \"profit_to_cost_ratio\"\nFROM \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"ORDER_ITEMS\" AS \"OI\"\nJOIN \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"ORDERS\" AS \"O\"\n  ON \"OI\".\"order_id\" = \"O\".\"order_id\"\nJOIN \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"PRODUCTS\" AS \"P\"\n  ON \"OI\".\"product_id\" = \"P\".\"id\"\nWHERE \"O\".\"status\" = 'Complete'\n  AND \"P\".\"category\" = 'Sleep & Lounge'\n  AND TO_TIMESTAMP_NTZ(\"O\".\"created_at\" \/ 1000000) >= '2023-01-01'\n  AND TO_TIMESTAMP_NTZ(\"O\".\"created_at\" \/ 1000000) < '2024-01-01'\nGROUP BY 1\nORDER BY 1;"
    },
    {
        "instance_id":"sf_bq264",
        "instruction":"Identify the difference in the number of the oldest and youngest users registered between January 1, 2019, and April 30, 2022, from our e-commerce platform data.",
        "db_id":"THELOOK_ECOMMERCE",
        "external_knowledge":null,
        "sql":"WITH MinMaxAge AS (\n  SELECT\n    MIN(\"age\") AS min_age,\n    MAX(\"age\") AS max_age\n  FROM \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"USERS\"\n  WHERE\n    CAST(TO_TIMESTAMP_NTZ(\"created_at\" \/ 1000000) AS DATE) BETWEEN '2019-01-01' AND '2022-04-30'\n)\nSELECT\n  SUM(CASE WHEN T.\"age\" = M.max_age THEN 1 ELSE 0 END) - SUM(CASE WHEN T.\"age\" = M.min_age THEN 1 ELSE 0 END)\nFROM \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"USERS\" AS T\nCROSS JOIN MinMaxAge AS M\nWHERE\n  CAST(TO_TIMESTAMP_NTZ(T.\"created_at\" \/ 1000000) AS DATE) BETWEEN '2019-01-01' AND '2022-04-30'"
    },
    {
        "instance_id":"sf_bq265",
        "instruction":"Can you list the email addresses of the top 10 users who registered in 2019 and made purchases in 2019, ranking them by their highest average order value, where average order value is calculated by multiplying the number of items in each order by the sale price, summing this total across all orders for each user, and then dividing by the total number of orders?",
        "db_id":"THELOOK_ECOMMERCE",
        "external_knowledge":null,
        "sql":"WITH users_2019 AS (\n  SELECT \"id\" AS \"user_id\", \"email\"\n  FROM THELOOK_ECOMMERCE.THELOOK_ECOMMERCE.USERS\n  WHERE TO_TIMESTAMP_NTZ(\"created_at\" \/ 1000000.0) >= TO_TIMESTAMP_NTZ('2019-01-01')\n    AND TO_TIMESTAMP_NTZ(\"created_at\" \/ 1000000.0) < TO_TIMESTAMP_NTZ('2020-01-01')\n),\norders_2019 AS (\n  SELECT \"order_id\", \"user_id\"\n  FROM THELOOK_ECOMMERCE.THELOOK_ECOMMERCE.ORDERS\n  WHERE TO_TIMESTAMP_NTZ(\"created_at\" \/ 1000000.0) >= TO_TIMESTAMP_NTZ('2019-01-01')\n    AND TO_TIMESTAMP_NTZ(\"created_at\" \/ 1000000.0) < TO_TIMESTAMP_NTZ('2020-01-01')\n    AND \"status\" NOT IN ('Cancelled','Returned')\n),\norder_items_valid AS (\n  SELECT \"order_id\", \"sale_price\"\n  FROM THELOOK_ECOMMERCE.THELOOK_ECOMMERCE.ORDER_ITEMS\n  WHERE \"status\" NOT IN ('Cancelled','Returned')\n    AND \"sale_price\" IS NOT NULL\n),\nper_order_revenue AS (\n  SELECT o.\"order_id\", o.\"user_id\",\n         SUM(oi.\"sale_price\") AS \"order_revenue\"\n  FROM orders_2019 o\n  JOIN order_items_valid oi\n    ON o.\"order_id\" = oi.\"order_id\"\n  GROUP BY o.\"order_id\", o.\"user_id\"\n),\nper_user AS (\n  SELECT por.\"user_id\",\n         SUM(por.\"order_revenue\") AS \"total_revenue\",\n         COUNT(DISTINCT por.\"order_id\") AS \"order_count\",\n         SUM(por.\"order_revenue\") \/ NULLIF(COUNT(DISTINCT por.\"order_id\"), 0) AS \"aov\"\n  FROM per_order_revenue por\n  GROUP BY por.\"user_id\"\n)\nSELECT u.\"email\" AS \"email\"\nFROM users_2019 u\nJOIN per_user p\n  ON u.\"user_id\" = p.\"user_id\"\nORDER BY p.\"aov\" DESC NULLS LAST, u.\"email\" ASC\nLIMIT 10;"
    },
    {
        "instance_id":"sf_bq271",
        "instruction":"Please generate a report that, for each month in 2021, provides the number of orders, the number of unique purchasers, and the profit (calculated as the sum of product retail prices minus the sum of product costs), where the orders were placed during 2021 by users who registered in 2021 for inventory items created in 2021, and group the results by the users' country, product department, and product category.",
        "db_id":"THELOOK_ECOMMERCE",
        "external_knowledge":null,
        "sql":"SELECT\n  TO_CHAR(DATE_TRUNC('month', TO_TIMESTAMP_LTZ(\"ORDERS\".\"created_at\" \/ 1000000)), 'YYYY-MM') AS \"order_month\",\n  \"USERS\".\"country\" AS \"country\",\n  \"INVENTORY_ITEMS\".\"product_department\" AS \"product_department\",\n  \"INVENTORY_ITEMS\".\"product_category\" AS \"product_category\",\n  COUNT(DISTINCT \"ORDERS\".\"order_id\") AS \"num_orders\",\n  COUNT(DISTINCT \"ORDERS\".\"user_id\") AS \"unique_purchasers\",\n  SUM(COALESCE(\"INVENTORY_ITEMS\".\"product_retail_price\",0)) - SUM(COALESCE(\"INVENTORY_ITEMS\".\"cost\",0)) AS \"profit\"\nFROM \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"ORDERS\" \"ORDERS\"\nJOIN \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"USERS\" \"USERS\"\n  ON \"ORDERS\".\"user_id\" = \"USERS\".\"id\"\nJOIN \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"ORDER_ITEMS\" \"ORDER_ITEMS\"\n  ON \"ORDERS\".\"order_id\" = \"ORDER_ITEMS\".\"order_id\"\nJOIN \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"INVENTORY_ITEMS\" \"INVENTORY_ITEMS\"\n  ON \"ORDER_ITEMS\".\"inventory_item_id\" = \"INVENTORY_ITEMS\".\"id\"\nWHERE\n  \"ORDERS\".\"created_at\" >= 1609459200000000\n  AND \"ORDERS\".\"created_at\" < 1640995200000000\n  AND \"USERS\".\"created_at\" >= 1609459200000000\n  AND \"USERS\".\"created_at\" < 1640995200000000\n  AND \"INVENTORY_ITEMS\".\"created_at\" >= 1609459200000000\n  AND \"INVENTORY_ITEMS\".\"created_at\" < 1640995200000000\nGROUP BY\n  DATE_TRUNC('month', TO_TIMESTAMP_LTZ(\"ORDERS\".\"created_at\" \/ 1000000)),\n  \"USERS\".\"country\",\n  \"INVENTORY_ITEMS\".\"product_department\",\n  \"INVENTORY_ITEMS\".\"product_category\"\nORDER BY\n  \"order_month\",\n  \"country\",\n  \"product_department\",\n  \"product_category\";"
    },
    {
        "instance_id":"sf_bq273",
        "instruction":"Can you list the top 5 months from August 2022 to November 2023 where the profit from Facebook-sourced completed orders showed the largest month-over-month increase? Calculate profit as sales minus costs, group by delivery month, and include only orders created between August 2022 and November 2023. Compare each month's profit to its previous month to find the largest increases.",
        "db_id":"THELOOK_ECOMMERCE",
        "external_knowledge":null,
        "sql":"WITH \"order_profit\" AS (\n    SELECT\n        CAST(DATE_TRUNC('month', TO_TIMESTAMP(\"o\".\"delivered_at\" \/ 1000000)) AS DATE) AS \"delivery_month\",\n        \"o\".\"order_id\",\n        SUM(\"oi\".\"sale_price\" - COALESCE(\"ii\".\"cost\", 0)) AS \"order_profit\"\n    FROM \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"ORDERS\" AS \"o\"\n    JOIN \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"USERS\" AS \"u\"\n        ON \"o\".\"user_id\" = \"u\".\"id\"\n    JOIN \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"ORDER_ITEMS\" AS \"oi\"\n        ON \"o\".\"order_id\" = \"oi\".\"order_id\"\n    LEFT JOIN \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"INVENTORY_ITEMS\" AS \"ii\"\n        ON \"oi\".\"inventory_item_id\" = \"ii\".\"id\"\n    WHERE \"o\".\"status\" = 'Complete'\n      AND \"u\".\"traffic_source\" = 'Facebook'\n      AND TO_TIMESTAMP(\"o\".\"created_at\" \/ 1000000) >= TO_TIMESTAMP('2022-08-01')\n      AND TO_TIMESTAMP(\"o\".\"created_at\" \/ 1000000) < TO_TIMESTAMP('2023-12-01')\n      AND \"o\".\"delivered_at\" IS NOT NULL\n    GROUP BY 1, 2\n),\n\"monthly_profit\" AS (\n    SELECT\n        \"delivery_month\",\n        SUM(\"order_profit\") AS \"monthly_profit\"\n    FROM \"order_profit\"\n    WHERE \"delivery_month\" >= DATE '2022-08-01'\n      AND \"delivery_month\" <= DATE '2023-11-01'\n    GROUP BY 1\n),\n\"monthly_changes\" AS (\n    SELECT\n        \"delivery_month\",\n        \"monthly_profit\",\n        \"monthly_profit\" - LAG(\"monthly_profit\") OVER (ORDER BY \"delivery_month\") AS \"profit_increase\"\n    FROM \"monthly_profit\"\n)\nSELECT\n    \"delivery_month\",\n    \"monthly_profit\",\n    \"profit_increase\"\nFROM \"monthly_changes\"\nWHERE \"profit_increase\" IS NOT NULL\nORDER BY \"profit_increase\" DESC, \"delivery_month\"\nLIMIT 5"
    },
    {
        "instance_id":"sf_bq028",
        "instruction":"Considering only the latest release versions of NPM package, which packages are the top 8 most popular based on the Github star number, as well as their versions?",
        "db_id":"DEPS_DEV_V1",
        "external_knowledge":null,
        "sql":"SELECT\n    q.\"package_name\",\n    q.\"version\",\n    q.\"github_stars\"\nFROM (\n    WITH latest_release AS (\n        SELECT\n            pv.\"Name\",\n            pv.\"Version\"\n        FROM \"DEPS_DEV_V1\".\"DEPS_DEV_V1\".\"PACKAGEVERSIONS\" pv\n        WHERE pv.\"System\" = 'NPM'\n          AND pv.\"Name\" NOT LIKE '%>%'\n          AND COALESCE((pv.\"VersionInfo\":\"IsRelease\")::BOOLEAN, FALSE)\n        QUALIFY ROW_NUMBER() OVER (\n            PARTITION BY pv.\"Name\"\n            ORDER BY COALESCE(pv.\"UpstreamPublishedAt\", pv.\"SnapshotAt\") DESC, pv.\"Version\" DESC\n        ) = 1\n    ),\n    latest_project AS (\n        SELECT\n            pr.\"Name\" AS \"ProjectName\",\n            pr.\"StarsCount\"\n        FROM \"DEPS_DEV_V1\".\"DEPS_DEV_V1\".\"PROJECTS\" pr\n        WHERE pr.\"Type\" = 'GITHUB'\n        QUALIFY ROW_NUMBER() OVER (\n            PARTITION BY pr.\"Name\"\n            ORDER BY pr.\"SnapshotAt\" DESC\n        ) = 1\n    ),\n    package_project AS (\n        SELECT DISTINCT\n            pvtp.\"Name\",\n            pvtp.\"Version\",\n            pvtp.\"ProjectName\"\n        FROM \"DEPS_DEV_V1\".\"DEPS_DEV_V1\".\"PACKAGEVERSIONTOPROJECT\" pvtp\n        WHERE pvtp.\"System\" = 'NPM'\n          AND pvtp.\"ProjectType\" = 'GITHUB'\n          AND pvtp.\"RelationType\" = 'SOURCE_REPO_TYPE'\n    )\n    SELECT\n        lr.\"Name\" AS \"package_name\",\n        lr.\"Version\" AS \"version\",\n        lp.\"StarsCount\" AS \"github_stars\",\n        ROW_NUMBER() OVER (\n            PARTITION BY lr.\"Name\"\n            ORDER BY lp.\"StarsCount\" DESC, mp.\"ProjectName\"\n        ) AS rn\n    FROM latest_release lr\n    JOIN package_project mp\n      ON mp.\"Name\" = lr.\"Name\"\n     AND mp.\"Version\" = lr.\"Version\"\n    JOIN latest_project lp\n      ON lp.\"ProjectName\" = mp.\"ProjectName\"\n) q\nWHERE q.rn = 1\nORDER BY q.\"github_stars\" DESC, q.\"package_name\"\nLIMIT 8"
    },
    {
        "instance_id":"sf_bq104",
        "instruction":"Based on the most recent refresh date, identify the top-ranked rising search term for the week that is exactly one year prior to the latest available week in the dataset.",
        "db_id":"GOOGLE_TRENDS",
        "external_knowledge":null,
        "sql":"WITH base AS (\n  SELECT\n    tr.\"term\",\n    tr.\"week\",\n    tr.\"score\",\n    tr.\"rank\",\n    tr.\"percent_gain\",\n    tr.\"refresh_date\"\n  FROM GOOGLE_TRENDS.GOOGLE_TRENDS.TOP_RISING_TERMS tr\n  WHERE tr.\"week\" IS NOT NULL\n\n  UNION ALL\n\n  SELECT\n    ir.\"term\",\n    ir.\"week\",\n    ir.\"score\",\n    ir.\"rank\",\n    ir.\"percent_gain\",\n    ir.\"refresh_date\"\n  FROM GOOGLE_TRENDS.GOOGLE_TRENDS.INTERNATIONAL_TOP_RISING_TERMS ir\n  WHERE ir.\"week\" IS NOT NULL\n),\nlatest_refresh AS (\n  SELECT MAX(\"refresh_date\") AS \"refresh_date\"\n  FROM base\n),\nlatest_week_in_latest AS (\n  SELECT MAX(b.\"week\") AS \"week\"\n  FROM base b\n  JOIN latest_refresh lr ON b.\"refresh_date\" = lr.\"refresh_date\"\n),\ntarget AS (\n  -- Use 52 weeks prior to align with weekly cadence for \"exactly one year\"\n  SELECT DATEADD('week', -52, lw.\"week\") AS \"target_week\"\n  FROM latest_week_in_latest lw\n),\ncand_exact_latest AS (  -- priority 1: exact target week within latest refresh\n  SELECT 1 AS prio, b.\"refresh_date\", b.\"week\"\n  FROM base b\n  JOIN latest_refresh lr ON b.\"refresh_date\" = lr.\"refresh_date\"\n  JOIN target t ON b.\"week\" = t.\"target_week\"\n  GROUP BY b.\"refresh_date\", b.\"week\"\n),\ncand_exact_any AS (     -- priority 2: exact target week across any refresh (choose latest refresh that has it)\n  SELECT 2 AS prio, x.\"refresh_date\", x.\"week\"\n  FROM (\n    SELECT b.\"refresh_date\", b.\"week\",\n           ROW_NUMBER() OVER (PARTITION BY b.\"week\" ORDER BY b.\"refresh_date\" DESC) AS rn\n    FROM base b\n    JOIN target t ON b.\"week\" = t.\"target_week\"\n  ) x\n  WHERE x.rn = 1\n),\ncandidates AS (\n  SELECT * FROM cand_exact_latest\n  UNION ALL\n  SELECT * FROM cand_exact_any\n),\nselected_context AS (\n  SELECT \"refresh_date\", \"week\"\n  FROM (\n    SELECT \"refresh_date\", \"week\",\n           ROW_NUMBER() OVER (ORDER BY prio) AS rn\n    FROM candidates\n  )\n  WHERE rn = 1\n),\nrows_in_context AS (\n  SELECT b.*\n  FROM base b\n  JOIN selected_context sc\n    ON b.\"refresh_date\" = sc.\"refresh_date\"\n   AND b.\"week\" = sc.\"week\"\n),\nterms_aggregated AS (\n  SELECT\n    \"term\",\n    MIN(\"rank\") AS best_rank,\n    MAX(\"percent_gain\") AS best_percent_gain,\n    MAX(\"score\") AS best_score\n  FROM rows_in_context\n  GROUP BY \"term\"\n)\nSELECT ta.\"term\"\nFROM terms_aggregated ta\nQUALIFY ROW_NUMBER() OVER (\n  ORDER BY ta.best_rank ASC NULLS LAST,\n           ta.best_percent_gain DESC NULLS LAST,\n           ta.best_score DESC NULLS LAST,\n           ta.\"term\"\n) = 1;"
    },
    {
        "instance_id":"sf_bq121",
        "instruction":"How do the average reputation and number of badges vary among Stack Overflow users based on the number of complete years they have been members, considering only those who joined on or before October 1, 2021?",
        "db_id":"STACKOVERFLOW",
        "external_knowledge":null,
        "sql":"WITH\n  ref AS (\n    SELECT DATE '2021-10-01' AS \"ref_date\"\n  ),\n  users_base AS (\n    SELECT\n      \"id\" AS \"user_id\",\n      CAST(TO_TIMESTAMP(\"creation_date\" \/ 1000000.0) AS DATE) AS \"creation_date_dt\",\n      \"reputation\"\n    FROM \"STACKOVERFLOW\".\"STACKOVERFLOW\".\"USERS\"\n  ),\n  users_filtered AS (\n    SELECT ub.*\n    FROM users_base ub\n    JOIN ref r ON 1=1\n    WHERE ub.\"creation_date_dt\" <= r.\"ref_date\"\n  ),\n  users_years AS (\n    SELECT\n      \"user_id\",\n      \"creation_date_dt\",\n      \"reputation\",\n      (DATEDIFF(year, \"creation_date_dt\", r.\"ref_date\")\n        - CASE WHEN DATEADD(year, DATEDIFF(year, \"creation_date_dt\", r.\"ref_date\"), \"creation_date_dt\") > r.\"ref_date\" THEN 1 ELSE 0 END\n      ) AS \"complete_years\"\n    FROM users_filtered uf\n    JOIN ref r ON 1=1\n  ),\n  badges_per_user AS (\n    SELECT\n      \"user_id\",\n      COUNT(*) AS \"num_badges\"\n    FROM \"STACKOVERFLOW\".\"STACKOVERFLOW\".\"BADGES\"\n    GROUP BY \"user_id\"\n  )\nSELECT\n  uy.\"complete_years\" AS \"complete_years\",\n  COALESCE(CAST(AVG(uy.\"reputation\") AS FLOAT), 0) AS \"avg_reputation\",\n  COALESCE(CAST(AVG(COALESCE(b.\"num_badges\", 0)) AS FLOAT), 0) AS \"avg_badges\",\n  COUNT(*) AS \"user_count\"\nFROM users_years uy\nLEFT JOIN badges_per_user b\n  ON uy.\"user_id\" = b.\"user_id\"\nGROUP BY uy.\"complete_years\"\nORDER BY uy.\"complete_years\" ASC;"
    },
    {
        "instance_id":"sf_bq345",
        "instruction":"How large are the DICOM image files with SEG or RTSTRUCT modalities and the SOP Class UID \"1.2.840.10008.5.1.4.1.1.66.4\", when grouped by collection, study, and series IDs, if they have no references to other series, images, or sources? Can you also provide a viewer URL formatted as \"https:\/\/viewer.imaging.datacommons.cancer.gov\/viewer\/\" followed by the study ID, and list these sizes in kilobytes, sorted from largest to smallest?",
        "db_id":"IDC",
        "external_knowledge":null,
        "sql":"SELECT\n  \"collection_id\",\n  \"StudyInstanceUID\" AS \"study_id\",\n  \"SeriesInstanceUID\" AS \"series_id\",\n  'https:\/\/viewer.imaging.datacommons.cancer.gov\/viewer\/' || \"StudyInstanceUID\" AS \"viewer_url\",\n  ROUND(SUM(\"instance_size\") \/ 1024) AS \"size_kb\"\nFROM \"IDC\".\"IDC_V17\".\"DICOM_ALL\"\nWHERE\n  \"Modality\" IN ('SEG','RTSTRUCT')\n  AND \"SOPClassUID\" = '1.2.840.10008.5.1.4.1.1.66.4'\n  AND COALESCE(ARRAY_SIZE(\"ReferencedSeriesSequence\"), 0) = 0\n  AND COALESCE(ARRAY_SIZE(\"ReferencedImageSequence\"), 0) = 0\n  AND COALESCE(ARRAY_SIZE(\"SourceImageSequence\"), 0) = 0\nGROUP BY\n  \"collection_id\",\n  \"StudyInstanceUID\",\n  \"SeriesInstanceUID\",\n  'https:\/\/viewer.imaging.datacommons.cancer.gov\/viewer\/' || \"StudyInstanceUID\"\nORDER BY\n  \"size_kb\" DESC;"
    },
    {
        "instance_id":"sf_bq346",
        "instruction":"In publicly accessible DICOM data where the Modality is 'SEG' and the SOPClassUID is '1.2.840.10008.5.1.4.1.1.66.4', and each segmentation references its original SOPInstanceUID, which five segmentation categories (by 'SegmentedPropertyCategory.CodeMeaning') occur most frequently?",
        "db_id":"IDC",
        "external_knowledge":null,
        "sql":"WITH segs AS (\n  SELECT d.\"SOPInstanceUID\"\n  FROM \"IDC\".\"IDC_V17\".\"DICOM_ALL\" d\n  WHERE d.\"access\" = 'Public'\n    AND d.\"Modality\" = 'SEG'\n    AND d.\"SOPClassUID\" = '1.2.840.10008.5.1.4.1.1.66.4'\n),\nrefs AS (\n  SELECT s.\"SOPInstanceUID\" AS \"seg_sop\", d.\"ReferencedSOPInstanceUID\"::string AS \"ref_sop\"\n  FROM segs s\n  JOIN \"IDC\".\"IDC_V17\".\"DICOM_ALL\" d ON d.\"SOPInstanceUID\" = s.\"SOPInstanceUID\"\n  WHERE d.\"ReferencedSOPInstanceUID\" IS NOT NULL\n  UNION ALL\n  SELECT s.\"SOPInstanceUID\", f.value:\"ReferencedSOPInstanceUID\"::string\n  FROM segs s\n  JOIN \"IDC\".\"IDC_V17\".\"DICOM_ALL\" d ON d.\"SOPInstanceUID\" = s.\"SOPInstanceUID\",\n       LATERAL FLATTEN(INPUT => d.\"SourceImageSequence\") f\n  WHERE f.value:\"ReferencedSOPInstanceUID\" IS NOT NULL\n  UNION ALL\n  SELECT s.\"SOPInstanceUID\", f.value:\"ReferencedSOPInstanceUID\"::string\n  FROM segs s\n  JOIN \"IDC\".\"IDC_V17\".\"DICOM_ALL\" d ON d.\"SOPInstanceUID\" = s.\"SOPInstanceUID\",\n       LATERAL FLATTEN(INPUT => d.\"ReferencedImageSequence\") f\n  WHERE f.value:\"ReferencedSOPInstanceUID\" IS NOT NULL\n  UNION ALL\n  SELECT s.\"SOPInstanceUID\", f2.value:\"ReferencedSOPInstanceUID\"::string\n  FROM segs s\n  JOIN \"IDC\".\"IDC_V17\".\"DICOM_ALL\" d ON d.\"SOPInstanceUID\" = s.\"SOPInstanceUID\",\n       LATERAL FLATTEN(INPUT => d.\"DerivationImageSequence\") f1,\n       LATERAL FLATTEN(INPUT => f1.value:\"SourceImageSequence\") f2\n  WHERE f2.value:\"ReferencedSOPInstanceUID\" IS NOT NULL\n  UNION ALL\n  SELECT s.\"SOPInstanceUID\", ri.value:\"ReferencedSOPInstanceUID\"::string\n  FROM segs s\n  JOIN \"IDC\".\"IDC_V17\".\"DICOM_ALL\" d ON d.\"SOPInstanceUID\" = s.\"SOPInstanceUID\",\n       LATERAL FLATTEN(INPUT => d.\"ReferencedSeriesSequence\") rs,\n       LATERAL FLATTEN(INPUT => rs.value:\"ReferencedInstanceSequence\") ri\n  WHERE ri.value:\"ReferencedSOPInstanceUID\" IS NOT NULL\n  UNION ALL\n  SELECT s.\"SOPInstanceUID\", ri.value:\"ReferencedSOPInstanceUID\"::string\n  FROM segs s\n  JOIN \"IDC\".\"IDC_V17\".\"DICOM_ALL\" d ON d.\"SOPInstanceUID\" = s.\"SOPInstanceUID\",\n       LATERAL FLATTEN(INPUT => d.\"ReferencedImageEvidenceSequence\") rie,\n       LATERAL FLATTEN(INPUT => rie.value:\"ReferencedSeriesSequence\") rs,\n       LATERAL FLATTEN(INPUT => rs.value:\"ReferencedInstanceSequence\") ri\n  WHERE ri.value:\"ReferencedSOPInstanceUID\" IS NOT NULL\n),\nseg_with_ref AS (\n  SELECT DISTINCT \"seg_sop\"\n  FROM refs\n)\nSELECT\n  seg.\"SegmentedPropertyCategory\":\"CodeMeaning\"::string AS \"SegmentedPropertyCategory_CodeMeaning\",\n  COUNT(*) AS \"count\"\nFROM \"IDC\".\"IDC_V17\".\"SEGMENTATIONS\" AS seg\nJOIN seg_with_ref r\n  ON seg.\"SOPInstanceUID\" = r.\"seg_sop\"\nGROUP BY 1\nORDER BY 2 DESC\nLIMIT 5"
    },
    {
        "instance_id":"sf_bq347",
        "instruction":"From the union of the specified MR series with SeriesInstanceUID 1.3.6.1.4.1.14519.5.2.1.3671.4754.105976129314091491952445656147 and all associated segmentation instances, which modality has the greatest number of SOP instances in total, and how many are there?",
        "db_id":"IDC",
        "external_knowledge":null,
        "sql":"WITH \"TARGET_SERIES\" AS (\n    SELECT '1.3.6.1.4.1.14519.5.2.1.3671.4754.105976129314091491952445656147' AS \"SeriesInstanceUID\"\n    UNION\n    SELECT DISTINCT \"SeriesInstanceUID\"\n    FROM \"IDC\".\"IDC_V17\".\"SEGMENTATIONS\"\n    WHERE \"segmented_SeriesInstanceUID\" = '1.3.6.1.4.1.14519.5.2.1.3671.4754.105976129314091491952445656147'\n),\n\"COUNTS\" AS (\n    SELECT\n        \"Modality\",\n        COUNT(*) AS \"SOP_Instance_Count\"\n    FROM \"IDC\".\"IDC_V17\".\"DICOM_ALL\"\n    WHERE \"SeriesInstanceUID\" IN (\n        SELECT \"SeriesInstanceUID\" FROM \"TARGET_SERIES\"\n    )\n    GROUP BY \"Modality\"\n)\nSELECT \"Modality\", \"SOP_Instance_Count\"\nFROM \"COUNTS\"\nQUALIFY ROW_NUMBER() OVER (ORDER BY \"SOP_Instance_Count\" DESC, \"Modality\") = 1"
    },
    {
        "instance_id":"sf_bq390",
        "instruction":"In the \"qin_prostate_repeatability\" collection, please provide the distinct StudyInstanceUIDs for studies that include T2-weighted axial MR imaging and also contain anatomical structure segmentations labeled as \"Peripheral zone.\"",
        "db_id":"IDC",
        "external_knowledge":null,
        "sql":"SELECT DISTINCT\n  T1.\"StudyInstanceUID\"\nFROM \"IDC\".\"IDC_V17\".\"DICOM_ALL\" AS T1\nINNER JOIN \"IDC\".\"IDC_V17\".\"SEGMENTATIONS\" AS T2\n  ON T1.\"StudyInstanceUID\" = T2.\"StudyInstanceUID\"\nWHERE\n  T1.\"collection_id\" = 'qin_prostate_repeatability'\n  AND T1.\"SeriesDescription\" = 'T2 Weighted Axial'\n  AND T2.\"SegmentedPropertyType\":CodeMeaning::STRING = 'Peripheral zone of the prostate'"
    },
    {
        "instance_id":"sf_bq421",
        "instruction":"Can you list all unique pairs of embedding medium and staining substance code meanings, along with the number of occurrences for each pair, based on distinct embedding medium and staining substance codes from the 'SM' modality in the DICOM dataset's un-nested specimen preparation sequences, ensuring that the codes are from the SCT coding scheme?",
        "db_id":"IDC",
        "external_knowledge":null,
        "sql":"WITH\n  SpecimenPreparationSequence_unnested AS (\n    SELECT\n      d.\"SOPInstanceUID\",\n      concept_name_code_sequence.value:\"CodeMeaning\"::STRING AS \"cnc_cm\",\n      concept_name_code_sequence.value:\"CodingSchemeDesignator\"::STRING AS \"cnc_csd\",\n      concept_name_code_sequence.value:\"CodeValue\"::STRING AS \"cnc_val\",\n      concept_code_sequence.value:\"CodeMeaning\"::STRING AS \"ccs_cm\",\n      concept_code_sequence.value:\"CodingSchemeDesignator\"::STRING AS \"ccs_csd\",\n      concept_code_sequence.value:\"CodeValue\"::STRING AS \"ccs_val\"\n    FROM\n      \"IDC\".\"IDC_V17\".\"DICOM_ALL\" AS d,\n      LATERAL FLATTEN(input => d.\"SpecimenDescriptionSequence\") AS spec_desc,\n      LATERAL FLATTEN(input => spec_desc.value:\"SpecimenPreparationSequence\") AS prep_seq,\n      LATERAL FLATTEN(input => prep_seq.value:\"SpecimenPreparationStepContentItemSequence\") AS prep_step,\n      LATERAL FLATTEN(input => prep_step.value:\"ConceptNameCodeSequence\") AS concept_name_code_sequence,\n      LATERAL FLATTEN(input => prep_step.value:\"ConceptCodeSequence\") AS concept_code_sequence\n  ),\n  slide_embedding AS (\n    SELECT\n      \"SOPInstanceUID\",\n      ARRAY_AGG(DISTINCT(CONCAT(\"ccs_cm\", ':', \"ccs_csd\", ':', \"ccs_val\"))) AS \"embeddingMedium_code_str\"\n    FROM\n      SpecimenPreparationSequence_unnested\n    WHERE\n      \"cnc_csd\" = 'SCT' AND \"cnc_val\" = '430863003' -- CodeMeaning is 'Embedding medium'\n    GROUP BY\n      \"SOPInstanceUID\"\n  ),\n  slide_staining AS (\n    SELECT\n      \"SOPInstanceUID\",\n      ARRAY_AGG(DISTINCT(CONCAT(\"ccs_cm\", ':', \"ccs_csd\", ':', \"ccs_val\"))) AS \"staining_usingSubstance_code_str\"\n    FROM\n      SpecimenPreparationSequence_unnested\n    WHERE\n      \"cnc_csd\" = 'SCT' AND \"cnc_val\" = '424361007' -- CodeMeaning is 'Using substance'\n    GROUP BY\n      \"SOPInstanceUID\"\n  ),\n  embedding_data AS (\n    SELECT\n      d.\"SOPInstanceUID\",\n      d.\"instance_size\",\n      e.\"embeddingMedium_code_str\",\n      s.\"staining_usingSubstance_code_str\"\n    FROM\n      \"IDC\".\"IDC_V17\".\"DICOM_ALL\" AS d\n    LEFT JOIN\n      slide_embedding AS e ON d.\"SOPInstanceUID\" = e.\"SOPInstanceUID\"\n    LEFT JOIN\n      slide_staining AS s ON d.\"SOPInstanceUID\" = s.\"SOPInstanceUID\"\n    WHERE\n      d.\"Modality\" = 'SM'\n  )\nSELECT\n  SPLIT_PART(embeddingMedium_CodeMeaning_flat.VALUE::STRING, ':', 1) AS \"embeddingMedium_CodeMeaning\",\n  SPLIT_PART(staining_usingSubstance_CodeMeaning_flat.VALUE::STRING, ':', 1) AS \"staining_usingSubstance_CodeMeaning\",\n  COUNT(*) AS \"count_\"\nFROM\n  embedding_data\n  , LATERAL FLATTEN(input => embedding_data.\"embeddingMedium_code_str\") AS embeddingMedium_CodeMeaning_flat\n  , LATERAL FLATTEN(input => embedding_data.\"staining_usingSubstance_code_str\") AS staining_usingSubstance_CodeMeaning_flat\nGROUP BY\n  SPLIT_PART(embeddingMedium_CodeMeaning_flat.VALUE::STRING, ':', 1),\n  SPLIT_PART(staining_usingSubstance_CodeMeaning_flat.VALUE::STRING, ':', 1);\n"
    },
    {
        "instance_id":"sf_bq422",
        "instruction":"Using the 'nlst' collection's CT images, calculate and compare two separate metrics: 1) The average series size in MiB for the top 3 patients with the highest slice interval difference tolerance (defined as the difference between the maximum and minimum unique slice intervals across all their series), and 2) The average series size in MiB for the top 3 patients with the highest exposure difference (defined as the difference between the maximum and minimum unique exposure values across all their series). For each patient, calculate the series size by summing the instance sizes of all images in that series and converting to MiB. Return the results as two separate groups labeled \"Top 3 by Slice Interval\" and \"Top 3 by Max Exposure\" with their respective average series sizes.",
        "db_id":"IDC",
        "external_knowledge":null,
        "sql":"WITH f AS (\n  SELECT\n    \"PatientID\",\n    \"SeriesInstanceUID\",\n    \"instance_size\",\n    \"SpacingBetweenSlices\",\n    \"SliceThickness\",\n    \"Exposure\",\n    \"ExposureInmAs\",\n    \"Modality\",\n    \"collection_name\"\n  FROM \"IDC\".\"IDC_V17\".\"DICOM_ALL\"\n  WHERE UPPER(\"collection_name\") = 'NLST'\n    AND \"Modality\" = 'CT'\n),\nseries_sizes AS (\n  SELECT\n    \"PatientID\",\n    \"SeriesInstanceUID\",\n    SUM(COALESCE(\"instance_size\", 0))::FLOAT \/ (1024 * 1024) AS series_size_mib\n  FROM f\n  GROUP BY \"PatientID\", \"SeriesInstanceUID\"\n),\npatient_slice_intervals AS (\n  SELECT DISTINCT\n    \"PatientID\",\n    COALESCE(TRY_TO_DECIMAL(\"SpacingBetweenSlices\"), TRY_TO_DECIMAL(\"SliceThickness\")) AS slice_interval\n  FROM f\n  WHERE COALESCE(TRY_TO_DECIMAL(\"SpacingBetweenSlices\"), TRY_TO_DECIMAL(\"SliceThickness\")) IS NOT NULL\n),\npatient_slice_diff AS (\n  SELECT\n    \"PatientID\",\n    MAX(slice_interval) - MIN(slice_interval) AS slice_diff\n  FROM patient_slice_intervals\n  GROUP BY \"PatientID\"\n),\ntop3_by_slice AS (\n  SELECT \"PatientID\"\n  FROM patient_slice_diff\n  ORDER BY slice_diff DESC NULLS LAST\n  LIMIT 3\n),\npatient_exposure_values AS (\n  SELECT DISTINCT\n    \"PatientID\",\n    COALESCE(CAST(\"ExposureInmAs\" AS FLOAT), CAST(TRY_TO_DECIMAL(\"Exposure\") AS FLOAT)) AS exposure_val\n  FROM f\n  WHERE COALESCE(CAST(\"ExposureInmAs\" AS FLOAT), CAST(TRY_TO_DECIMAL(\"Exposure\") AS FLOAT)) IS NOT NULL\n),\npatient_exposure_diff AS (\n  SELECT\n    \"PatientID\",\n    MAX(exposure_val) - MIN(exposure_val) AS exp_diff\n  FROM patient_exposure_values\n  GROUP BY \"PatientID\"\n),\ntop3_by_exposure AS (\n  SELECT \"PatientID\"\n  FROM patient_exposure_diff\n  ORDER BY exp_diff DESC NULLS LAST\n  LIMIT 3\n)\nSELECT 'Top 3 by Slice Interval' AS \"Label\",\n       AVG(s.series_size_mib) AS \"Average Series Size MiB\"\nFROM series_sizes s\nJOIN top3_by_slice t USING (\"PatientID\")\nUNION ALL\nSELECT 'Top 3 by Max Exposure' AS \"Label\",\n       AVG(s.series_size_mib) AS \"Average Series Size MiB\"\nFROM series_sizes s\nJOIN top3_by_exposure t USING (\"PatientID\");"
    },
    {
        "instance_id":"sf_bq219",
        "instruction":"In the Iowa Liquor Sales dataset, starting from January 1, 2022 through the last fully completed month, which two liquor categories, each contributing an average of at least 1% to the monthly sales volume over at least 24 months of available data, have the lowest Pearson correlation coefficient when comparing their monthly percentages of total liquor sales across those months, and what are their names?",
        "db_id":"IOWA_LIQUOR_SALES",
        "external_knowledge":null,
        "sql":"WITH MonthlyCategorySales AS (\n    SELECT\n        DATE_TRUNC('month', \"date\") AS sales_month,\n        \"category_name\",\n        SUM(\"volume_sold_liters\") AS category_volume\n    FROM \"IOWA_LIQUOR_SALES\".\"IOWA_LIQUOR_SALES\".\"SALES\"\n    WHERE \"date\" >= '2022-01-01' AND \"date\" < '2024-09-01' AND \"category_name\" IS NOT NULL\n    GROUP BY 1, 2\n),\nTotalMonthlySales AS (\n    SELECT\n        sales_month,\n        SUM(category_volume) AS total_volume\n    FROM MonthlyCategorySales\n    GROUP BY 1\n),\nMonthlyCategoryPercentage AS (\n    SELECT\n        mcs.sales_month,\n        mcs.\"category_name\",\n        (mcs.category_volume \/ tms.total_volume) AS percentage_of_volume\n    FROM MonthlyCategorySales mcs\n    JOIN TotalMonthlySales tms ON mcs.sales_month = tms.sales_month\n),\nEligibleCategories AS (\n    SELECT\n        \"category_name\"\n    FROM MonthlyCategoryPercentage\n    GROUP BY \"category_name\"\n    HAVING COUNT(DISTINCT sales_month) >= 24 AND AVG(percentage_of_volume) >= 0.01\n)\nSELECT\n    t1.\"category_name\" AS category1,\n    t2.\"category_name\" AS category2\nFROM MonthlyCategoryPercentage AS t1\nJOIN MonthlyCategoryPercentage AS t2 ON t1.sales_month = t2.sales_month AND t1.\"category_name\" < t2.\"category_name\"\nWHERE t1.\"category_name\" IN (SELECT \"category_name\" FROM EligibleCategories)\n  AND t2.\"category_name\" IN (SELECT \"category_name\" FROM EligibleCategories)\nGROUP BY 1, 2\nORDER BY CORR(t1.percentage_of_volume, t2.percentage_of_volume)\nLIMIT 1;"
    },
    {
        "instance_id":"sf_bq043",
        "instruction":"What are the RNA expression levels of the genes MDM2, TP53, CDKN1A, and CCNE1, along with associated clinical information, in bladder cancer patients with CDKN2A mutations in the 'TCGA-BLCA' project?  Use clinical data from the Genomic Data Commons Release 39, data about somatic mutations derived from the hg19 human genome reference in Feb 2017.",
        "db_id":"TCGA",
        "external_knowledge":null,
        "sql":"WITH \"cdkn2a_cases\" AS (\n  SELECT DISTINCT \"case_barcode\"\n  FROM \"TCGA\".\"TCGA_VERSIONED\".\"SOMATIC_MUTATION_HG19_DCC_2017_02\"\n  WHERE \"project_short_name\" = 'TCGA-BLCA'\n    AND \"Hugo_Symbol\" = 'CDKN2A'\n    AND \"Mutation_Status\" = 'Somatic'\n  UNION\n  SELECT DISTINCT \"case_barcode\"\n  FROM \"TCGA\".\"TCGA_VERSIONED\".\"SOMATIC_MUTATION_HG19_MC3_2017_02\"\n  WHERE \"project_short_name\" = 'TCGA-BLCA'\n    AND \"Hugo_Symbol\" = 'CDKN2A'\n),\n\"expr_hg19\" AS (\n  SELECT\n    e.\"case_barcode\",\n    MAX(IFF(e.\"HGNC_gene_symbol\" = 'MDM2', e.\"normalized_count\", NULL))   AS \"MDM2_normalized_count\",\n    MAX(IFF(e.\"HGNC_gene_symbol\" = 'TP53', e.\"normalized_count\", NULL))   AS \"TP53_normalized_count\",\n    MAX(IFF(e.\"HGNC_gene_symbol\" = 'CDKN1A', e.\"normalized_count\", NULL)) AS \"CDKN1A_normalized_count\",\n    MAX(IFF(e.\"HGNC_gene_symbol\" = 'CCNE1', e.\"normalized_count\", NULL))  AS \"CCNE1_normalized_count\"\n  FROM \"TCGA\".\"TCGA_VERSIONED\".\"RNASEQ_HG19_GDC_2017_02\" e\n  WHERE e.\"project_short_name\" = 'TCGA-BLCA'\n    AND e.\"HGNC_gene_symbol\" IN ('MDM2','TP53','CDKN1A','CCNE1')\n    AND e.\"sample_barcode\" LIKE '%-01%'  -- Primary Tumor samples\n  GROUP BY e.\"case_barcode\"\n)\nSELECT\n  c.\"submitter_id\" AS \"case_barcode\",\n  e.\"MDM2_normalized_count\",\n  e.\"TP53_normalized_count\",\n  e.\"CDKN1A_normalized_count\",\n  e.\"CCNE1_normalized_count\",\n  c.\"proj__project_id\" AS \"project_id\",\n  c.\"primary_site\",\n  c.\"disease_type\",\n  c.\"demo__gender\" AS \"gender\",\n  c.\"demo__race\" AS \"race\",\n  c.\"demo__ethnicity\" AS \"ethnicity\",\n  c.\"demo__vital_status\" AS \"vital_status\",\n  c.\"diag__age_at_diagnosis\" AS \"age_at_diagnosis_days\",\n  c.\"diag__year_of_diagnosis\" AS \"year_of_diagnosis\",\n  c.\"diag__ajcc_pathologic_stage\" AS \"ajcc_pathologic_stage\",\n  c.\"diag__ajcc_clinical_stage\" AS \"ajcc_clinical_stage\"\nFROM \"TCGA\".\"TCGA_VERSIONED\".\"CLINICAL_GDC_R39\" c\nJOIN \"cdkn2a_cases\" m\n  ON c.\"submitter_id\" = m.\"case_barcode\"\nLEFT JOIN \"expr_hg19\" e\n  ON e.\"case_barcode\" = c.\"submitter_id\"\nWHERE c.\"proj__project_id\" = 'TCGA-BLCA'\nORDER BY c.\"submitter_id\";"
    },
    {
        "instance_id":"sf_bq176",
        "instruction":"Identify the case barcodes from the TCGA-LAML study with the highest weighted average copy number in cytoband 15q11 on chromosome 15, using segment data and cytoband overlaps from TCGA's genomic and Mitelman databases.",
        "db_id":"TCGA_MITELMAN",
        "external_knowledge":null,
        "sql":"\/*\nGoal: Identify TCGA-LAML case barcodes with the highest weighted average copy number \n      inside cytoband 15q11 (chr15:19,000,000-25,500,000 on hg38).\n      \u2011 Use cytoband coordinates from CYTOBANDS_HG38.\n      \u2011 Use segment-level copy number from COPY_NUMBER_SEGMENT_ALLELIC_HG38_GDC_R23 \n        (this table stores integer \"copy_number\" values and chromosome names like 'chr15').\n      \u2011 Weighted average =  \u03a3(copy_number * overlap_length) \/ \u03a3(overlap_length),\n        where overlap_length is the number of bases of a segment that fall inside 15q11.\n      \u2011 Return the barcode(s) whose weighted average equals the global maximum.\n*\/\n\nWITH band AS (\n    \/* Exact genomic span of cytoband 15q11 on chr15 (hg38) *\/\n    SELECT \n        MIN(\"hg38_start\") AS \"region_start\",\n        MAX(\"hg38_stop\")  AS \"region_end\"\n    FROM \"TCGA_MITELMAN\".\"PROD\".\"CYTOBANDS_HG38\"\n    WHERE \"cytoband_name\" ILIKE '15q11%'      -- include 15q11, 15q11.1, 15q11.2, ...\n      AND \"chromosome\" = 'chr15'\n), laml_segments AS (\n    \/* All chr15 copy-number segments for TCGA-LAML cases (hg38, allelic table) *\/\n    SELECT \n        s.\"case_barcode\",\n        s.\"start_pos\",\n        s.\"end_pos\",\n        s.\"copy_number\"\n    FROM \"TCGA_MITELMAN\".\"TCGA_VERSIONED\".\"COPY_NUMBER_SEGMENT_ALLELIC_HG38_GDC_R23\" s\n    WHERE s.\"project_short_name\" = 'TCGA-LAML'\n      AND s.\"chromosome\" = 'chr15'\n), overlaps AS (\n    \/* Keep only segment portions that actually overlap 15q11 *\/\n    SELECT \n        l.\"case_barcode\",\n        GREATEST(l.\"start_pos\", b.\"region_start\") AS \"ov_start\",\n        LEAST(l.\"end_pos\",   b.\"region_end\")   AS \"ov_end\",\n        l.\"copy_number\"\n    FROM laml_segments l\n    CROSS JOIN band b\n    WHERE l.\"end_pos\"   >= b.\"region_start\"   -- segment starts before region end\n      AND l.\"start_pos\" <= b.\"region_end\"     -- segment ends after region start\n), weighted AS (\n    \/* Compute weighted sums per case *\/\n    SELECT \n        \"case_barcode\",\n        SUM( (\"ov_end\" - \"ov_start\" + 1) * \"copy_number\" ) AS \"weighted_sum\",\n        SUM(  \"ov_end\" - \"ov_start\" + 1 )                  AS \"total_len\"\n    FROM overlaps\n    GROUP BY \"case_barcode\"\n), per_case AS (\n    \/* Final weighted average copy number per case *\/\n    SELECT \n        \"case_barcode\",\n        \"weighted_sum\" \/ \"total_len\" AS \"weighted_avg_copy_number\"\n    FROM weighted\n)\n\/* Return the barcode(s) with the highest weighted average *\/\nSELECT \"case_barcode\"\nFROM per_case\nWHERE \"weighted_avg_copy_number\" = (\n    SELECT MAX(\"weighted_avg_copy_number\") FROM per_case\n)\nORDER BY \"case_barcode\";"
    },
    {
        "instance_id":"sf_bq150",
        "instruction":"Assess whether different genetic variants affect the log10-transformed TP53 expression levels in TCGA-BRCA samples using sequencing and mutation data. Provide the total number of samples, the number of mutation types, the mean square between groups, the mean square within groups, and the F-statistic.",
        "db_id":"TCGA_HG19_DATA_V0",
        "external_knowledge":"TCGA_F_Score.md",
        "sql":"with expr as (\n    select\n        \"sample_barcode\",\n        ln(\"normalized_count\") \/ ln(10) as log_expr\n    from \"TCGA_HG19_DATA_V0\".\"TCGA_HG19_DATA_V0\".\"RNASEQ_GENE_EXPRESSION_UNC_RSEM\"\n    where \"project_short_name\" = 'TCGA-BRCA'\n      and \"HGNC_gene_symbol\" = 'TP53'\n      and \"normalized_count\" > 0\n      and substr(\"sample_barcode\", 14, 2) = '01'\n),\nmut as (\n    select\n        \"sample_barcode_tumor\" as sample_barcode,\n        case\n            when count(distinct \"Variant_Classification\") = 1 then max(\"Variant_Classification\")\n            else 'Multiple'\n        end as mutation_type\n    from \"TCGA_HG19_DATA_V0\".\"TCGA_HG19_DATA_V0\".\"SOMATIC_MUTATION_MC3\"\n    where \"project_short_name\" = 'TCGA-BRCA'\n      and \"Hugo_Symbol\" = 'TP53'\n    group by \"sample_barcode_tumor\"\n),\ncombined as (\n    select\n        e.\"sample_barcode\",\n        coalesce(m.mutation_type, 'No Mutation') as mutation_type,\n        e.log_expr\n    from expr e\n    left join mut m\n        on e.\"sample_barcode\" = m.sample_barcode\n),\ngroup_stats as (\n    select\n        mutation_type,\n        count(*) as n_j,\n        avg(log_expr) as mean_j\n    from combined\n    group by mutation_type\n),\noverall as (\n    select\n        count(*) as N,\n        avg(log_expr) as grand_mean\n    from combined\n),\nss_between as (\n    select\n        sum(gs.n_j * power(gs.mean_j - o.grand_mean, 2)) as ssb\n    from group_stats gs\n    cross join overall o\n),\nss_within as (\n    select\n        sum(power(c.log_expr - gs.mean_j, 2)) as ssw\n    from combined c\n    join group_stats gs\n        on c.mutation_type = gs.mutation_type\n),\ngroup_count as (\n    select count(*) as k\n    from group_stats\n)\nselect\n    o.N as total_samples,\n    gc.k as mutation_types,\n    ssb.ssb \/ nullif(gc.k - 1, 0) as mean_square_between,\n    ssw.ssw \/ nullif(o.N - gc.k, 0) as mean_square_within,\n    (ssb.ssb \/ nullif(gc.k - 1, 0)) \/ (ssw.ssw \/ nullif(o.N - gc.k, 0)) as f_statistic\nfrom overall o\ncross join group_count gc\ncross join ss_between ssb\ncross join ss_within ssw;"
    },
    {
        "instance_id":"sf_bq155",
        "instruction":"In the TCGA-BRCA cohort of patients who are 80 years old or younger at diagnosis and have a pathological stage of Stage I, Stage II, or Stage IIA, calculate the t-statistic derived from the Pearson correlation between the log10-transformed average RNA-Seq expression levels (using HTSeq__Counts + 1) of the gene SNORA31 and the average microRNA-Seq expression levels of all unique microRNAs, only considering pairs with more than 25 samples and where the absolute Pearson correlation coefficient is between 0.3 and 1.0",
        "db_id":"TCGA_HG38_DATA_V0",
        "external_knowledge":null,
        "sql":"WITH FilteredCases AS (\n    SELECT\n        \"case_barcode\"\n    FROM \"TCGA_HG38_DATA_V0\".\"TCGA_BIOCLIN_V0\".\"CLINICAL\"\n    WHERE\n        \"project_short_name\" = 'TCGA-BRCA'\n        AND \"age_at_diagnosis\" <= 80\n        AND \"pathologic_stage\" IN ('Stage I', 'Stage II', 'Stage IIA')\n),\nSNORA31_Expression AS (\n    SELECT\n        T1.\"case_barcode\",\n        LOG(10, AVG(T1.\"HTSeq__Counts\") + 1) AS snora31_log_expr\n    FROM \"TCGA_HG38_DATA_V0\".\"TCGA_HG38_DATA_V0\".\"RNASEQ_GENE_EXPRESSION\" AS T1\n    INNER JOIN FilteredCases AS T2\n        ON T1.\"case_barcode\" = T2.\"case_barcode\"\n    WHERE\n        T1.\"gene_name\" = 'SNORA31'\n    GROUP BY\n        T1.\"case_barcode\"\n),\nmiRNA_Expression AS (\n    SELECT\n        T1.\"case_barcode\",\n        T1.\"mirna_id\",\n        AVG(T1.\"reads_per_million_miRNA_mapped\") AS mirna_avg_expr\n    FROM \"TCGA_HG38_DATA_V0\".\"TCGA_HG38_DATA_V0\".\"MIRNASEQ_EXPRESSION\" AS T1\n    INNER JOIN FilteredCases AS T2\n        ON T1.\"case_barcode\" = T2.\"case_barcode\"\n    GROUP BY\n        T1.\"case_barcode\",\n        T1.\"mirna_id\"\n),\nCorrelationData AS (\n    SELECT\n        T2.\"mirna_id\",\n        CORR(T1.snora31_log_expr, T2.mirna_avg_expr) AS pearson_corr,\n        COUNT(*) AS sample_count\n    FROM SNORA31_Expression AS T1\n    INNER JOIN miRNA_Expression AS T2\n        ON T1.\"case_barcode\" = T2.\"case_barcode\"\n    GROUP BY\n        T2.\"mirna_id\"\n)\nSELECT\n    \"mirna_id\",\n    pearson_corr * SQRT((sample_count - 2) \/ (1 - pearson_corr * pearson_corr)) AS t_statistic\nFROM CorrelationData\nWHERE\n    sample_count > 25\n    AND ABS(pearson_corr) BETWEEN 0.3 AND 1.0"
    },
    {
        "instance_id":"sf_bq153",
        "instruction":"Calculate, for each histology type specified in the 'icd_o_3_histology' field (excluding those enclosed in square brackets), the average of the per-patient average log10(normalized_count + 1) expression levels of the IGF2 gene among LGG patients with valid IGF2 expression data. Match gene expression and clinical data using the ParticipantBarcode field.",
        "db_id":"PANCANCER_ATLAS_1",
        "external_knowledge":null,
        "sql":"WITH PatientAvg AS (\n    SELECT\n        T1.\"bcr_patient_barcode\",\n        T1.\"icd_o_3_histology\",\n        AVG(LOG(10, T2.\"normalized_count\" + 1)) AS AvgLogExpression\n    FROM \"PANCANCER_ATLAS_1\".\"PANCANCER_ATLAS_FILTERED\".\"CLINICAL_PANCAN_PATIENT_WITH_FOLLOWUP_FILTERED\" AS T1\n    JOIN \"PANCANCER_ATLAS_1\".\"PANCANCER_ATLAS_FILTERED\".\"EBPP_ADJUSTPANCAN_ILLUMINAHISEQ_RNASEQV2_GENEXP_FILTERED\" AS T2\n        ON T1.\"bcr_patient_barcode\" = T2.\"ParticipantBarcode\"\n    WHERE\n        T1.\"acronym\" = 'LGG'\n        AND T2.\"Symbol\" = 'IGF2'\n        AND T1.\"icd_o_3_histology\" IS NOT NULL\n        AND NOT (T1.\"icd_o_3_histology\" LIKE '[%' AND T1.\"icd_o_3_histology\" LIKE '%]')\n    GROUP BY\n        T1.\"bcr_patient_barcode\",\n        T1.\"icd_o_3_histology\"\n)\nSELECT\n    \"icd_o_3_histology\",\n    AVG(AvgLogExpression)\nFROM PatientAvg\nGROUP BY\n    \"icd_o_3_histology\";"
    },
    {
        "instance_id":"sf_bq158",
        "instruction":"Which top five histological types of breast cancer (BRCA) in the PanCancer Atlas exhibit the highest percentage of CDH1 gene mutations?",
        "db_id":"PANCANCER_ATLAS_1",
        "external_knowledge":null,
        "sql":"WITH clinical_brca AS (\n  SELECT DISTINCT\n    c.\"bcr_patient_barcode\" AS \"ParticipantBarcode\",\n    c.\"histological_type\"\n  FROM \"PANCANCER_ATLAS_1\".\"PANCANCER_ATLAS_FILTERED\".\"CLINICAL_PANCAN_PATIENT_WITH_FOLLOWUP_FILTERED\" c\n  WHERE c.\"acronym\" = 'BRCA'\n    AND c.\"histological_type\" IS NOT NULL\n),\nbrca_patients AS (\n  SELECT DISTINCT\n    m.\"ParticipantBarcode\"\n  FROM \"PANCANCER_ATLAS_1\".\"PANCANCER_ATLAS_FILTERED\".\"MC3_MAF_V5_ONE_PER_TUMOR_SAMPLE\" m\n  WHERE m.\"Study\" = 'BRCA'\n),\ndenom_hist AS (\n  SELECT\n    p.\"ParticipantBarcode\",\n    cb.\"histological_type\"\n  FROM brca_patients p\n  JOIN clinical_brca cb\n    ON p.\"ParticipantBarcode\" = cb.\"ParticipantBarcode\"\n),\ncdh1_mutants AS (\n  SELECT DISTINCT\n    m.\"ParticipantBarcode\"\n  FROM \"PANCANCER_ATLAS_1\".\"PANCANCER_ATLAS_FILTERED\".\"MC3_MAF_V5_ONE_PER_TUMOR_SAMPLE\" m\n  WHERE m.\"Study\" = 'BRCA'\n    AND m.\"Hugo_Symbol\" = 'CDH1'\n)\nSELECT\n  dh.\"histological_type\" AS \"histological_type\",\n  COUNT(DISTINCT CASE WHEN cm.\"ParticipantBarcode\" IS NOT NULL THEN dh.\"ParticipantBarcode\" END) AS \"mutated_cases\",\n  COUNT(DISTINCT dh.\"ParticipantBarcode\") AS \"total_cases\",\n  100.0 * COUNT(DISTINCT CASE WHEN cm.\"ParticipantBarcode\" IS NOT NULL THEN dh.\"ParticipantBarcode\" END)\n    \/ NULLIF(COUNT(DISTINCT dh.\"ParticipantBarcode\"), 0) AS \"cdh1_mutation_percentage\"\nFROM denom_hist dh\nLEFT JOIN cdh1_mutants cm\n  ON dh.\"ParticipantBarcode\" = cm.\"ParticipantBarcode\"\nGROUP BY dh.\"histological_type\"\nORDER BY \"cdh1_mutation_percentage\" DESC\nLIMIT 5;"
    },
    {
        "instance_id":"sf_bq159",
        "instruction":"Calculate the chi-square value to assess the association between histological types and the presence of CDH1 gene mutations in BRCA patients using data from the PanCancer Atlas. Focus on patients with known histological types and consider only reliable mutation entries.  Exclude any histological types or mutation statuses with marginal totals less than or equal to 10. Match clinical and mutation data using ParticipantBarcode",
        "db_id":"PANCANCER_ATLAS_1",
        "external_knowledge":null,
        "sql":"WITH Cdh1MutatedPatients AS (\n    SELECT DISTINCT\n        \"ParticipantBarcode\"\n    FROM \"PANCANCER_ATLAS_1\".\"PANCANCER_ATLAS_FILTERED\".\"MC3_MAF_V5_ONE_PER_TUMOR_SAMPLE\"\n    WHERE\n        \"Hugo_Symbol\" = 'CDH1' AND \"FILTER\" = 'PASS'\n), PatientData AS (\n    SELECT\n        T1.\"histological_type\",\n        CASE\n            WHEN T2.\"ParticipantBarcode\" IS NOT NULL THEN 'mutated'\n            ELSE 'not_mutated'\n        END AS mutation_status\n    FROM \"PANCANCER_ATLAS_1\".\"PANCANCER_ATLAS_FILTERED\".\"CLINICAL_PANCAN_PATIENT_WITH_FOLLOWUP_FILTERED\" AS T1\n    LEFT JOIN Cdh1MutatedPatients AS T2\n        ON T1.\"bcr_patient_barcode\" = T2.\"ParticipantBarcode\"\n    WHERE\n        T1.\"acronym\" = 'BRCA'\n        AND T1.\"histological_type\" IS NOT NULL\n), InitialContingency AS (\n    SELECT\n        \"histological_type\",\n        mutation_status,\n        COUNT(*) AS observed\n    FROM PatientData\n    GROUP BY\n        \"histological_type\",\n        mutation_status\n), FilteredContingency AS (\n    SELECT\n        \"histological_type\",\n        mutation_status,\n        observed\n    FROM InitialContingency\n    WHERE\n        \"histological_type\" IN (\n            SELECT \"histological_type\"\n            FROM InitialContingency\n            GROUP BY \"histological_type\"\n            HAVING SUM(observed) > 10\n        )\n        AND mutation_status IN (\n            SELECT mutation_status\n            FROM InitialContingency\n            GROUP BY mutation_status\n            HAVING SUM(observed) > 10\n        )\n), ChiSquareInput AS (\n    SELECT\n        \"histological_type\",\n        mutation_status,\n        observed,\n        SUM(observed) OVER (PARTITION BY \"histological_type\") AS row_total,\n        SUM(observed) OVER (PARTITION BY mutation_status) AS col_total,\n        SUM(observed) OVER () AS grand_total\n    FROM FilteredContingency\n)\nSELECT\n    SUM(POWER(observed - (row_total * col_total \/ grand_total), 2) \/ (row_total * col_total \/ grand_total))\nFROM ChiSquareInput;"
    },
    {
        "instance_id":"sf_bq166",
        "instruction":"Using segment-level copy number data from the copy_number_segment_allelic_hg38_gdc_r23 dataset restricted to 'TCGA-KIRC' samples, merge these segments with the cytogenetic band definitions in 'CytoBands_hg38' to identify each sample\u2019s maximum copy number per cytoband. Classify these maximum copy numbers into amplifications (>3), gains (=3), homozygous deletions (=0), heterozygous deletions (=1), or normal (=2), then calculate the frequency of each subtype out of the total number of distinct cases, and finally present these frequencies as percentages sorted by chromosome and cytoband.",
        "db_id":"TCGA_MITELMAN",
        "external_knowledge":"Comprehensive_Guide_to_Copy_Number_Variations_in_Cancer_Genomics.md",
        "sql":"WITH \"kirc_total\" AS (\n  SELECT COUNT(DISTINCT \"case_barcode\") AS \"total_cases\"\n  FROM \"TCGA_MITELMAN\".\"TCGA_VERSIONED\".\"COPY_NUMBER_SEGMENT_ALLELIC_HG38_GDC_R23\"\n  WHERE \"project_short_name\" = 'TCGA-KIRC'\n),\n\"seg_band\" AS (\n  SELECT\n    s.\"case_barcode\",\n    s.\"sample_barcode\",\n    s.\"chromosome\",\n    b.\"cytoband_name\",\n    s.\"copy_number\"\n  FROM \"TCGA_MITELMAN\".\"TCGA_VERSIONED\".\"COPY_NUMBER_SEGMENT_ALLELIC_HG38_GDC_R23\" s\n  JOIN \"TCGA_MITELMAN\".\"PROD\".\"CYTOBANDS_HG38\" b\n    ON s.\"chromosome\" = b.\"chromosome\"\n   AND LEAST(s.\"end_pos\", b.\"hg38_stop\") - GREATEST(s.\"start_pos\", b.\"hg38_start\") > 0\n  WHERE s.\"project_short_name\" = 'TCGA-KIRC'\n),\n\"sample_band_max\" AS (\n  SELECT\n    \"case_barcode\",\n    \"sample_barcode\",\n    \"chromosome\",\n    \"cytoband_name\",\n    MAX(\"copy_number\") AS \"sample_max_copy\"\n  FROM \"seg_band\"\n  GROUP BY \"case_barcode\", \"sample_barcode\", \"chromosome\", \"cytoband_name\"\n),\n\"case_band_max\" AS (\n  SELECT\n    \"case_barcode\",\n    \"chromosome\",\n    \"cytoband_name\",\n    MAX(\"sample_max_copy\") AS \"case_max_copy\"\n  FROM \"sample_band_max\"\n  GROUP BY \"case_barcode\", \"chromosome\", \"cytoband_name\"\n),\n\"case_band_category\" AS (\n  SELECT\n    \"case_barcode\",\n    \"chromosome\",\n    \"cytoband_name\",\n    CASE\n      WHEN \"case_max_copy\" > 3 THEN 'Amplification'\n      WHEN \"case_max_copy\" = 3 THEN 'Gain'\n      WHEN \"case_max_copy\" = 2 THEN 'Normal'\n      WHEN \"case_max_copy\" = 1 THEN 'Heterozygous Deletion'\n      WHEN \"case_max_copy\" = 0 THEN 'Homozygous Deletion'\n      ELSE 'Unknown'\n    END AS \"category\"\n  FROM \"case_band_max\"\n),\n\"band_counts\" AS (\n  SELECT\n    \"chromosome\",\n    \"cytoband_name\",\n    SUM(CASE WHEN \"category\" = 'Amplification' THEN 1 ELSE 0 END) AS \"n_amplification\",\n    SUM(CASE WHEN \"category\" = 'Gain' THEN 1 ELSE 0 END) AS \"n_gain\",\n    SUM(CASE WHEN \"category\" = 'Homozygous Deletion' THEN 1 ELSE 0 END) AS \"n_homdel\",\n    SUM(CASE WHEN \"category\" = 'Heterozygous Deletion' THEN 1 ELSE 0 END) AS \"n_hetdel\",\n    SUM(CASE WHEN \"category\" = 'Normal' THEN 1 ELSE 0 END) AS \"n_normal\"\n  FROM \"case_band_category\"\n  GROUP BY \"chromosome\", \"cytoband_name\"\n)\nSELECT\n  bc.\"chromosome\",\n  bc.\"cytoband_name\",\n  100.0 * bc.\"n_amplification\" \/ kt.\"total_cases\" AS \"amplification_pct\",\n  100.0 * bc.\"n_gain\" \/ kt.\"total_cases\" AS \"gain_pct\",\n  100.0 * bc.\"n_homdel\" \/ kt.\"total_cases\" AS \"homozygous_deletion_pct\",\n  100.0 * bc.\"n_hetdel\" \/ kt.\"total_cases\" AS \"heterozygous_deletion_pct\",\n  100.0 * bc.\"n_normal\" \/ kt.\"total_cases\" AS \"normal_pct\"\nFROM \"band_counts\" bc\nCROSS JOIN \"kirc_total\" kt\nORDER BY\n  CASE\n    WHEN REGEXP_LIKE(REGEXP_REPLACE(bc.\"chromosome\", '^chr', ''), '^[0-9]+$')\n      THEN TO_NUMBER(REGEXP_REPLACE(bc.\"chromosome\", '^chr', ''))\n    WHEN REGEXP_REPLACE(bc.\"chromosome\", '^chr', '') = 'X' THEN 23\n    WHEN REGEXP_REPLACE(bc.\"chromosome\", '^chr', '') = 'Y' THEN 24\n    ELSE 25\n  END,\n  bc.\"cytoband_name\";"
    },
    {
        "instance_id":"sf_bq412",
        "instruction":"Please retrieve the page URLs, first shown time, last shown time, removal reason, violation category, and the lower and upper bounds of times shown for the five most recently removed ads in the Croatia region (region code 'HR'), where the times shown availability date is null, the times shown lower bound exceeds 10,000, the times shown upper bound is below 25,000, and the ads used at least one non-unused audience selection approach among demographics, geographic location, contextual signals, customer lists, or topics of interest, ordering the resulting ads by their last shown time in descending order.",
        "db_id":"GOOGLE_ADS",
        "external_knowledge":null,
        "sql":"WITH parsed_regions AS (SELECT \"creative_page_url\", value:\"first_shown\"::STRING AS first_shown_str, value:\"last_shown\"::STRING AS last_shown_str, value:\"times_shown_lower_bound\"::NUMBER AS times_shown_lower, value:\"times_shown_upper_bound\"::NUMBER AS times_shown_upper, value:\"region_code\"::STRING AS region_code, value:\"times_shown_availability_date\" AS availability_date FROM GOOGLE_ADS.GOOGLE_ADS_TRANSPARENCY_CENTER.REMOVED_CREATIVE_STATS, LATERAL FLATTEN(INPUT => PARSE_JSON(\"region_stats\"))), filtered_regions AS (SELECT \"creative_page_url\", TO_TIMESTAMP(first_shown_str) AS \"first_shown\", TO_TIMESTAMP(last_shown_str) AS \"last_shown\", times_shown_lower, times_shown_upper FROM parsed_regions WHERE region_code = 'HR' AND availability_date IS NULL AND times_shown_lower > 10000 AND times_shown_upper < 25000) SELECT T1.\"creative_page_url\", T2.\"first_shown\", T2.\"last_shown\", PARSE_JSON(T1.\"disapproval\")[0]:\"removal_reason\"::STRING AS \"removal_reason\", PARSE_JSON(T1.\"disapproval\")[0]:\"violation_category\"::STRING AS \"violation_category\", T2.times_shown_lower, T2.times_shown_upper FROM GOOGLE_ADS.GOOGLE_ADS_TRANSPARENCY_CENTER.REMOVED_CREATIVE_STATS T1 INNER JOIN filtered_regions T2 ON T1.\"creative_page_url\" = T2.\"creative_page_url\" WHERE (PARSE_JSON(T1.\"audience_selection_approach_info\"):\"demographic_info\"::STRING != 'CRITERIA_UNUSED' OR PARSE_JSON(T1.\"audience_selection_approach_info\"):\"geo_location\"::STRING != 'CRITERIA_UNUSED' OR PARSE_JSON(T1.\"audience_selection_approach_info\"):\"contextual_signals\"::STRING != 'CRITERIA_UNUSED' OR PARSE_JSON(T1.\"audience_selection_approach_info\"):\"customer_lists\"::STRING != 'CRITERIA_UNUSED' OR PARSE_JSON(T1.\"audience_selection_approach_info\"):\"topics_of_interest\"::STRING != 'CRITERIA_UNUSED') ORDER BY T2.\"last_shown\" DESC LIMIT 5;"
    },
    {
        "instance_id":"sf_bq070",
        "instruction":"Could you provide a clean, structured dataset from dicom_all table that only includes SM images marked as VOLUME from the TCGA-LUAD and TCGA-LUSC collections, excluding any slides with compression type \u201cother,\u201d where the specimen preparation step explicitly has \u201cEmbedding medium\u201d set to \u201cTissue freezing medium,\u201d and ensuring that the tissue type is only \u201cnormal\u201d or \u201ctumor\u201d and the cancer subtype is reported accordingly?",
        "db_id":"IDC",
        "external_knowledge":"dicom_dataset_selection.md",
        "sql":"WITH base AS (\n  SELECT\n    t.\"SOPInstanceUID\",\n    t.\"SeriesInstanceUID\",\n    t.\"StudyInstanceUID\",\n    t.\"PatientID\",\n    t.\"collection_id\",\n    t.\"collection_name\",\n    t.\"Rows\",\n    t.\"Columns\",\n    t.\"PixelSpacing\",\n    t.\"LossyImageCompressionMethod\",\n    t.\"VolumetricProperties\",\n    t.\"Modality\",\n    t.\"gcs_url\",\n    sd.value:\"SpecimenIdentifier\"::STRING AS SPECIMEN_IDENTIFIER,\n    pams.value:\"CodeValue\"::STRING AS TISSUE_CODE,\n    pams.value:\"CodeMeaning\"::STRING AS TISSUE_MEANING\n  FROM \"IDC\".\"IDC_V17\".\"DICOM_ALL\" t\n  JOIN LATERAL FLATTEN(INPUT => t.\"SpecimenDescriptionSequence\") sd\n  JOIN LATERAL FLATTEN(INPUT => sd.value:\"PrimaryAnatomicStructureSequence\") pas\n  JOIN LATERAL FLATTEN(INPUT => pas.value:\"PrimaryAnatomicStructureModifierSequence\") pams\n  JOIN LATERAL FLATTEN(INPUT => sd.value:\"SpecimenPreparationSequence\") sps\n  JOIN LATERAL FLATTEN(INPUT => sps.value:\"SpecimenPreparationStepContentItemSequence\") sp\n  JOIN LATERAL FLATTEN(INPUT => sp.value:\"ConceptNameCodeSequence\") cn\n  LEFT JOIN LATERAL FLATTEN(INPUT => sp.value:\"ConceptCodeSequence\", OUTER => TRUE) cc\n  WHERE t.\"Modality\" = 'SM'\n    AND t.\"VolumetricProperties\" = 'VOLUME'\n    AND (\n      t.\"collection_name\" IN ('TCGA-LUAD', 'TCGA-LUSC') OR\n      t.\"collection_id\" IN ('tcga_luad', 'tcga_lusc') OR\n      t.\"tcia_api_collection_id\" IN ('TCGA-LUAD', 'TCGA-LUSC') OR\n      t.\"idc_webapp_collection_id\" IN ('TCGA-LUAD', 'TCGA-LUSC')\n    )\n    AND cn.value:\"CodeMeaning\"::STRING = 'Embedding medium'\n    AND (\n      UPPER(COALESCE(sp.value:\"TextValue\"::STRING, '')) = 'TISSUE FREEZING MEDIUM'\n      OR UPPER(COALESCE(cc.value:\"CodeMeaning\"::STRING, '')) = 'TISSUE FREEZING MEDIUM'\n    )\n    AND (\n      pams.value:\"CodeValue\"::STRING IN ('17621005', '86049000')\n      OR UPPER(COALESCE(pams.value:\"CodeMeaning\"::STRING, '')) IN ('NORMAL', 'NEOPLASM', 'TUMOR')\n    )\n),\nlabeled AS (\n  SELECT DISTINCT\n    b.\"SOPInstanceUID\",\n    b.\"SeriesInstanceUID\",\n    b.\"StudyInstanceUID\",\n    b.\"PatientID\",\n    b.\"collection_id\",\n    b.\"collection_name\",\n    b.\"Rows\",\n    b.\"Columns\",\n    b.\"PixelSpacing\",\n    b.\"LossyImageCompressionMethod\",\n    b.\"gcs_url\",\n    b.SPECIMEN_IDENTIFIER,\n    CASE\n      WHEN UPPER(COALESCE(b.TISSUE_MEANING, '')) LIKE 'NORMAL%' THEN 'normal'\n      WHEN UPPER(COALESCE(b.TISSUE_MEANING, '')) LIKE '%NEOPLASM%' OR UPPER(COALESCE(b.TISSUE_MEANING, '')) LIKE '%TUMOR%' THEN 'tumor'\n      WHEN b.TISSUE_CODE = '17621005' THEN 'normal'\n      WHEN b.TISSUE_CODE = '86049000' THEN 'tumor'\n      ELSE NULL\n    END AS TISSUE_TYPE,\n    CASE\n      WHEN ARRAY_CONTAINS(TO_VARIANT('ISO_10918_1'), b.\"LossyImageCompressionMethod\") THEN 'jpeg'\n      WHEN ARRAY_CONTAINS(TO_VARIANT('ISO_15444_1'), b.\"LossyImageCompressionMethod\") THEN 'jpeg2000'\n      ELSE 'other'\n    END AS COMPRESSION_TYPE,\n    CASE\n      WHEN b.\"collection_name\" = 'TCGA-LUAD' OR b.\"collection_id\" = 'tcga_luad' THEN 'luad'\n      WHEN b.\"collection_name\" = 'TCGA-LUSC' OR b.\"collection_id\" = 'tcga_lusc' THEN 'lscc'\n      ELSE NULL\n    END AS CANCER_SUBTYPE\n  FROM base b\n)\nSELECT\n  l.\"SeriesInstanceUID\" AS \"DIGITAL_SLIDE_ID\",\n  l.\"StudyInstanceUID\" AS \"CASE_ID\",\n  l.SPECIMEN_IDENTIFIER AS \"PHYSICAL_SLIDE_ID\",\n  l.\"PatientID\" AS \"PATIENT_ID\",\n  l.\"collection_id\" AS \"COLLECTION_ID\",\n  l.\"SOPInstanceUID\" AS \"INSTANCE_ID\",\n  l.\"gcs_url\" AS \"GCS_URL\",\n  l.\"Columns\" AS \"WIDTH\",\n  l.\"Rows\" AS \"HEIGHT\",\n  ROUND(TRY_TO_DOUBLE((l.\"PixelSpacing\"[0])::STRING), 4) AS \"PIXEL_SPACING\",\n  l.COMPRESSION_TYPE AS \"COMPRESSION_TYPE\",\n  l.TISSUE_TYPE AS \"TISSUE_TYPE\",\n  l.CANCER_SUBTYPE AS \"CANCER_SUBTYPE\"\nFROM labeled l\nWHERE l.COMPRESSION_TYPE IN ('jpeg', 'jpeg2000')\n  AND l.TISSUE_TYPE IN ('normal', 'tumor')\nORDER BY l.\"SOPInstanceUID\" ASC;"
    },
    {
        "instance_id":"sf_bq320",
        "instruction":"In the dicom_pivot table, how many unique StudyInstanceUID values exactly match the SegmentedPropertyTypeCodeSequence of \"15825003\" (case-insensitive) and also have a collection_id of either \"Community\" or \"nsclc_radiomics\"?",
        "db_id":"IDC",
        "external_knowledge":null,
        "sql":"SELECT COUNT(DISTINCT \"StudyInstanceUID\") AS \"unique_study_count\"\nFROM \"IDC\".\"IDC_V17\".\"DICOM_PIVOT\"\nWHERE \"SegmentedPropertyTypeCodeSequence\" IS NOT NULL\n  AND LOWER(TRIM(\"SegmentedPropertyTypeCodeSequence\")) = '15825003'\n  AND LOWER(TRIM(\"collection_id\")) IN ('community', 'nsclc_radiomics');"
    },
    {
        "instance_id":"sf_bq321",
        "instruction":"How many unique StudyInstanceUIDs are there from the DWI, T2 Weighted Axial, Apparent Diffusion Coefficient series, and T2 Weighted Axial Segmentations in the 'qin_prostate_repeatability' collection?",
        "db_id":"IDC",
        "external_knowledge":null,
        "sql":"SELECT COUNT(DISTINCT \"StudyInstanceUID\") AS \"unique_study_count\"\nFROM \"IDC\".\"IDC_V17\".\"DICOM_PIVOT\"\nWHERE \"collection_id\" = 'qin_prostate_repeatability'\n  AND \"SeriesDescription\" IN (\n    'DWI',\n    'T2 Weighted Axial',\n    'Apparent Diffusion Coefficient',\n    'T2 Weighted Axial Segmentations'\n  );"
    },
    {
        "instance_id":"sf_bq455",
        "instruction":"Identify the top five CT scan series by size (in MiB), including their SeriesInstanceUID, series number, patient ID, and series size. These series must be from the CT modality and not part of the 'nlst' collection. Exclude any series where the ImageType is classified as 'LOCALIZER' or where the TransferSyntaxUID is either '1.2.840.10008.1.2.4.70' or '1.2.840.10008.1.2.4.51' (i.e., JPEG compressed). The selected series must have consistent slice intervals, exposure levels, image orientation (with only one unique ImageOrientationPatient value), pixel spacing, image positions (both z-axis and xy positions), and pixel dimensions (rows and columns). Ensure that the number of images matches the number of unique z-axis positions, indicating no duplicate slices. Additionally, the z-axis component of the cross product of the x and y direction cosines from ImageOrientationPatient must have an absolute value between 0.99 and 1.01, ensuring alignment with the expected imaging plane. Finally, order the results by series size in descending order and limit the output to the top five series satisfying these conditions.",
        "db_id":"IDC",
        "external_knowledge":null,
        "sql":"SELECT\n    sm.\"SeriesInstanceUID\",\n    sm.\"SeriesNumber\",\n    sm.\"PatientID\",\n    sm.\"series_size_mib\"\nFROM (\n    SELECT\n        wd.\"SeriesInstanceUID\",\n        MIN(wd.\"SeriesNumber\") AS \"SeriesNumber\",\n        MIN(wd.\"PatientID\") AS \"PatientID\",\n        SUM(wd.\"instance_size\") \/ 1048576.0 AS \"series_size_mib\",\n        COUNT(*) AS \"image_count\",\n        COUNT(DISTINCT wd.\"pos_z_r\") AS \"unique_z_count\",\n        COUNT(DISTINCT wd.\"pos_xy_key\") AS \"unique_xy_count\",\n        COUNT(DISTINCT wd.\"ps_row_r\") AS \"ps_row_count\",\n        COUNT(DISTINCT wd.\"ps_col_r\") AS \"ps_col_count\",\n        COUNT(DISTINCT wd.\"Rows\") AS \"rows_count\",\n        COUNT(DISTINCT wd.\"Columns\") AS \"cols_count\",\n        COUNT(DISTINCT wd.\"orientation_raw\") AS \"orientation_count\",\n        COUNT(DISTINCT wd.\"exposure_inmas_r\") AS \"exposure_inmas_count\",\n        COUNT(DISTINCT wd.\"exposure_r\") AS \"exposure_count\",\n        COUNT(DISTINCT wd.\"tube_current_r\") AS \"tube_current_count\",\n        COUNT(DISTINCT CASE WHEN wd.\"z_diff\" IS NOT NULL THEN wd.\"z_diff\" END) AS \"distinct_z_diff_count\",\n        MIN(wd.\"z_diff\") AS \"min_z_diff\",\n        MAX(wd.\"z_diff\") AS \"max_z_diff\",\n        MIN(wd.\"cross_z\") AS \"min_cross_z\",\n        MAX(wd.\"cross_z\") AS \"max_cross_z\"\n    FROM (\n        SELECT\n            f.\"SeriesInstanceUID\",\n            f.\"SeriesNumber\",\n            f.\"PatientID\",\n            f.\"instance_size\",\n            CONCAT(TO_VARCHAR(f.\"pos_x_r\"), '|', TO_VARCHAR(f.\"pos_y_r\")) AS \"pos_xy_key\",\n            f.\"pos_z\",\n            f.\"pos_z_r\",\n            f.\"ps_row_r\",\n            f.\"ps_col_r\",\n            f.\"Rows\",\n            f.\"Columns\",\n            f.\"orientation_raw\",\n            f.\"exposure_inmas_r\",\n            f.\"exposure_r\",\n            f.\"tube_current_r\",\n            f.\"cross_z\",\n            ROUND(ABS(LEAD(f.\"pos_z\") OVER (PARTITION BY f.\"SeriesInstanceUID\" ORDER BY f.\"pos_z\") - f.\"pos_z\"), 5) AS \"z_diff\"\n        FROM (\n            SELECT\n                da.\"SeriesInstanceUID\",\n                da.\"SeriesNumber\",\n                da.\"PatientID\",\n                da.\"instance_size\",\n                TRY_TO_DOUBLE(da.\"ImagePositionPatient\"[2]::STRING) AS \"pos_z\",\n                ROUND(TRY_TO_DOUBLE(da.\"ImagePositionPatient\"[0]::STRING), 5) AS \"pos_x_r\",\n                ROUND(TRY_TO_DOUBLE(da.\"ImagePositionPatient\"[1]::STRING), 5) AS \"pos_y_r\",\n                ROUND(TRY_TO_DOUBLE(da.\"ImagePositionPatient\"[2]::STRING), 5) AS \"pos_z_r\",\n                ROUND(TRY_TO_DOUBLE(da.\"PixelSpacing\"[0]::STRING), 5) AS \"ps_row_r\",\n                ROUND(TRY_TO_DOUBLE(da.\"PixelSpacing\"[1]::STRING), 5) AS \"ps_col_r\",\n                da.\"Rows\",\n                da.\"Columns\",\n                TO_VARCHAR(da.\"ImageOrientationPatient\") AS \"orientation_raw\",\n                ROUND(TRY_TO_DOUBLE(da.\"ExposureInmAs\"::STRING), 4) AS \"exposure_inmas_r\",\n                ROUND(TRY_TO_DOUBLE(da.\"Exposure\"::STRING), 4) AS \"exposure_r\",\n                ROUND(TRY_TO_DOUBLE(da.\"XRayTubeCurrentInmA\"::STRING), 4) AS \"tube_current_r\",\n                TRY_TO_DOUBLE(da.\"ImageOrientationPatient\"[0]::STRING) * TRY_TO_DOUBLE(da.\"ImageOrientationPatient\"[4]::STRING)\n                  - TRY_TO_DOUBLE(da.\"ImageOrientationPatient\"[1]::STRING) * TRY_TO_DOUBLE(da.\"ImageOrientationPatient\"[3]::STRING) AS \"cross_z\"\n            FROM \"IDC\".\"IDC_V17\".\"DICOM_ALL\" da\n            WHERE NVL(UPPER(da.\"collection_name\"), '') != 'NLST'\n              AND da.\"Modality\" = 'CT'\n              AND da.\"TransferSyntaxUID\" NOT IN ('1.2.840.10008.1.2.4.70', '1.2.840.10008.1.2.4.51')\n              AND NOT (UPPER(TO_VARCHAR(da.\"ImageType\")) LIKE '%LOCALIZER%')\n              AND da.\"SeriesInstanceUID\" IS NOT NULL\n              AND da.\"SeriesNumber\" IS NOT NULL\n              AND da.\"PatientID\" IS NOT NULL\n              AND da.\"ImagePositionPatient\" IS NOT NULL\n              AND da.\"ImageOrientationPatient\" IS NOT NULL\n              AND da.\"PixelSpacing\" IS NOT NULL\n              AND TRY_TO_DOUBLE(da.\"ImagePositionPatient\"[0]::STRING) IS NOT NULL\n              AND TRY_TO_DOUBLE(da.\"ImagePositionPatient\"[1]::STRING) IS NOT NULL\n              AND TRY_TO_DOUBLE(da.\"ImagePositionPatient\"[2]::STRING) IS NOT NULL\n              AND TRY_TO_DOUBLE(da.\"PixelSpacing\"[0]::STRING) IS NOT NULL\n              AND TRY_TO_DOUBLE(da.\"PixelSpacing\"[1]::STRING) IS NOT NULL\n              AND TRY_TO_DOUBLE(da.\"ImageOrientationPatient\"[0]::STRING) IS NOT NULL\n              AND TRY_TO_DOUBLE(da.\"ImageOrientationPatient\"[1]::STRING) IS NOT NULL\n              AND TRY_TO_DOUBLE(da.\"ImageOrientationPatient\"[3]::STRING) IS NOT NULL\n              AND TRY_TO_DOUBLE(da.\"ImageOrientationPatient\"[4]::STRING) IS NOT NULL\n        ) f\n    ) wd\n    GROUP BY wd.\"SeriesInstanceUID\"\n) sm\nWHERE sm.\"image_count\" = sm.\"unique_z_count\"\n  AND sm.\"unique_xy_count\" = 1\n  AND sm.\"ps_row_count\" = 1\n  AND sm.\"ps_col_count\" = 1\n  AND sm.\"rows_count\" = 1\n  AND sm.\"cols_count\" = 1\n  AND sm.\"orientation_count\" = 1\n  AND sm.\"distinct_z_diff_count\" <= 1\n  AND (sm.\"distinct_z_diff_count\" = 0 OR sm.\"min_z_diff\" = sm.\"max_z_diff\")\n  AND sm.\"exposure_inmas_count\" <= 1\n  AND sm.\"exposure_count\" <= 1\n  AND sm.\"tube_current_count\" <= 1\n  AND ABS(sm.\"min_cross_z\") BETWEEN 0.99 AND 1.01\n  AND ABS(sm.\"max_cross_z\") BETWEEN 0.99 AND 1.01\nORDER BY sm.\"series_size_mib\" DESC\nLIMIT 5;"
    },
    {
        "instance_id":"sf_bq167",
        "instruction":"Identify the pair of Kaggle users involved in ForumMessageVotes such that one user has given the other the greatest distinct number of upvotes, then also display how many upvotes that recipient returned. Present the usernames of both users, the total distinct upvotes one received from the other, and the upvotes they gave back, sorting by the highest received count and then by the highest given count, and show only the top result.",
        "db_id":"META_KAGGLE",
        "external_knowledge":null,
        "sql":"WITH \"DIRECTED\" AS (\n  SELECT\n    \"v\".\"FromUserId\" AS \"FROM_USER_ID\",\n    \"v\".\"ToUserId\" AS \"TO_USER_ID\",\n    COUNT(DISTINCT \"v\".\"ForumMessageId\") AS \"RECEIVED_CNT\"\n  FROM \"META_KAGGLE\".\"META_KAGGLE\".\"FORUMMESSAGEVOTES\" AS \"v\"\n  WHERE \"v\".\"FromUserId\" IS NOT NULL\n    AND \"v\".\"ToUserId\" IS NOT NULL\n    AND \"v\".\"FromUserId\" != \"v\".\"ToUserId\"\n  GROUP BY \"v\".\"FromUserId\", \"v\".\"ToUserId\"\n)\nSELECT\n  COALESCE(\"u_from\".\"UserName\", \"u_from\".\"DisplayName\") AS \"GiverUserName\",\n  COALESCE(\"u_to\".\"UserName\", \"u_to\".\"DisplayName\") AS \"ReceiverUserName\",\n  \"d\".\"RECEIVED_CNT\" AS \"ReceivedUpvotes\",\n  COALESCE(\"d2\".\"RECEIVED_CNT\", 0) AS \"ReturnedUpvotes\"\nFROM \"DIRECTED\" AS \"d\"\nLEFT JOIN \"DIRECTED\" AS \"d2\"\n  ON \"d2\".\"FROM_USER_ID\" = \"d\".\"TO_USER_ID\"\n AND \"d2\".\"TO_USER_ID\" = \"d\".\"FROM_USER_ID\"\nLEFT JOIN \"META_KAGGLE\".\"META_KAGGLE\".\"USERS\" AS \"u_from\"\n  ON \"u_from\".\"Id\" = \"d\".\"FROM_USER_ID\"\nLEFT JOIN \"META_KAGGLE\".\"META_KAGGLE\".\"USERS\" AS \"u_to\"\n  ON \"u_to\".\"Id\" = \"d\".\"TO_USER_ID\"\nORDER BY \"d\".\"RECEIVED_CNT\" DESC,\n         COALESCE(\"d2\".\"RECEIVED_CNT\", 0) DESC\nFETCH FIRST 1 ROWS ONLY;"
    },
    {
        "instance_id":"sf_bq072",
        "instruction":"Please provide, for each age from 12 through 18 (inclusive), the total number of deaths and the number of deaths among individuals identified as Black (based on race descriptions containing the word \u2018black\u2019), specifically for deaths associated with ICD-10 codes whose descriptions include the word \u2018vehicle\u2019 and for deaths associated with ICD-10 codes whose descriptions include the word \u2018firearm.\u2019 Use the EntityAxisConditions table to determine which ICD-10 codes were involved in each death, rather than joining ICD-10 code information directly on the death records.",
        "db_id":"DEATH",
        "external_knowledge":null,
        "sql":"WITH \"ages\" AS (\n  SELECT 12 AS \"Age\" UNION ALL\n  SELECT 13 UNION ALL\n  SELECT 14 UNION ALL\n  SELECT 15 UNION ALL\n  SELECT 16 UNION ALL\n  SELECT 17 UNION ALL\n  SELECT 18\n),\n\"vehicle_deaths\" AS (\n  SELECT DISTINCT\n    \"dr\".\"Id\" AS \"dr_id\",\n    \"dr\".\"Age\" AS \"Age\",\n    CASE WHEN \"r\".\"Description\" ILIKE '%black%' THEN 1 ELSE 0 END AS \"is_black\"\n  FROM \"DEATH\".\"DEATH\".\"ENTITYAXISCONDITIONS\" AS \"e\"\n  JOIN \"DEATH\".\"DEATH\".\"ICD10CODE\" AS \"icd\"\n    ON \"icd\".\"Code\" = \"e\".\"Icd10Code\"\n  JOIN \"DEATH\".\"DEATH\".\"DEATHRECORDS\" AS \"dr\"\n    ON \"dr\".\"Id\" = \"e\".\"DeathRecordId\"\n  LEFT JOIN \"DEATH\".\"DEATH\".\"RACE\" AS \"r\"\n    ON \"r\".\"Code\" = \"dr\".\"Race\"\n  WHERE \"dr\".\"AgeType\" = 1\n    AND \"dr\".\"Age\" BETWEEN 12 AND 18\n    AND \"icd\".\"Description\" ILIKE '%vehicle%'\n),\n\"vehicle_agg\" AS (\n  SELECT\n    \"Age\",\n    COUNT(DISTINCT \"dr_id\") AS \"vehicle_total_deaths\",\n    COUNT(DISTINCT CASE WHEN \"is_black\" = 1 THEN \"dr_id\" END) AS \"vehicle_black_deaths\"\n  FROM \"vehicle_deaths\"\n  GROUP BY \"Age\"\n),\n\"firearm_deaths\" AS (\n  SELECT DISTINCT\n    \"dr\".\"Id\" AS \"dr_id\",\n    \"dr\".\"Age\" AS \"Age\",\n    CASE WHEN \"r\".\"Description\" ILIKE '%black%' THEN 1 ELSE 0 END AS \"is_black\"\n  FROM \"DEATH\".\"DEATH\".\"ENTITYAXISCONDITIONS\" AS \"e\"\n  JOIN \"DEATH\".\"DEATH\".\"ICD10CODE\" AS \"icd\"\n    ON \"icd\".\"Code\" = \"e\".\"Icd10Code\"\n  JOIN \"DEATH\".\"DEATH\".\"DEATHRECORDS\" AS \"dr\"\n    ON \"dr\".\"Id\" = \"e\".\"DeathRecordId\"\n  LEFT JOIN \"DEATH\".\"DEATH\".\"RACE\" AS \"r\"\n    ON \"r\".\"Code\" = \"dr\".\"Race\"\n  WHERE \"dr\".\"AgeType\" = 1\n    AND \"dr\".\"Age\" BETWEEN 12 AND 18\n    AND \"icd\".\"Description\" ILIKE '%firearm%'\n),\n\"firearm_agg\" AS (\n  SELECT\n    \"Age\",\n    COUNT(DISTINCT \"dr_id\") AS \"firearm_total_deaths\",\n    COUNT(DISTINCT CASE WHEN \"is_black\" = 1 THEN \"dr_id\" END) AS \"firearm_black_deaths\"\n  FROM \"firearm_deaths\"\n  GROUP BY \"Age\"\n)\nSELECT\n  \"a\".\"Age\",\n  COALESCE(\"v\".\"vehicle_total_deaths\", 0) AS \"vehicle_total_deaths\",\n  COALESCE(\"v\".\"vehicle_black_deaths\", 0) AS \"vehicle_black_deaths\",\n  COALESCE(\"f\".\"firearm_total_deaths\", 0) AS \"firearm_total_deaths\",\n  COALESCE(\"f\".\"firearm_black_deaths\", 0) AS \"firearm_black_deaths\"\nFROM \"ages\" AS \"a\"\nLEFT JOIN \"vehicle_agg\" AS \"v\"\n  ON \"v\".\"Age\" = \"a\".\"Age\"\nLEFT JOIN \"firearm_agg\" AS \"f\"\n  ON \"f\".\"Age\" = \"a\".\"Age\"\nORDER BY \"a\".\"Age\""
    },
    {
        "instance_id":"sf_ga001",
        "instruction":"I want to know the preferences of customers who purchased the Google Navy Speckled Tee in December 2020. What other product was purchased with the highest total quantity alongside this item?",
        "db_id":"GA4",
        "external_knowledge":null,
        "sql":"WITH \"events_dec\" AS (\n  SELECT \"EVENT_NAME\",\"EVENT_DATE\",\"ECOMMERCE\",\"ITEMS\" FROM \"GA4\".\"GA4_OBFUSCATED_SAMPLE_ECOMMERCE\".\"EVENTS_20201201\"\n  UNION ALL SELECT \"EVENT_NAME\",\"EVENT_DATE\",\"ECOMMERCE\",\"ITEMS\" FROM \"GA4\".\"GA4_OBFUSCATED_SAMPLE_ECOMMERCE\".\"EVENTS_20201202\"\n  UNION ALL SELECT \"EVENT_NAME\",\"EVENT_DATE\",\"ECOMMERCE\",\"ITEMS\" FROM \"GA4\".\"GA4_OBFUSCATED_SAMPLE_ECOMMERCE\".\"EVENTS_20201203\"\n  UNION ALL SELECT \"EVENT_NAME\",\"EVENT_DATE\",\"ECOMMERCE\",\"ITEMS\" FROM \"GA4\".\"GA4_OBFUSCATED_SAMPLE_ECOMMERCE\".\"EVENTS_20201204\"\n  UNION ALL SELECT \"EVENT_NAME\",\"EVENT_DATE\",\"ECOMMERCE\",\"ITEMS\" FROM \"GA4\".\"GA4_OBFUSCATED_SAMPLE_ECOMMERCE\".\"EVENTS_20201205\"\n  UNION ALL SELECT \"EVENT_NAME\",\"EVENT_DATE\",\"ECOMMERCE\",\"ITEMS\" FROM \"GA4\".\"GA4_OBFUSCATED_SAMPLE_ECOMMERCE\".\"EVENTS_20201206\"\n  UNION ALL SELECT \"EVENT_NAME\",\"EVENT_DATE\",\"ECOMMERCE\",\"ITEMS\" FROM \"GA4\".\"GA4_OBFUSCATED_SAMPLE_ECOMMERCE\".\"EVENTS_20201207\"\n  UNION ALL SELECT \"EVENT_NAME\",\"EVENT_DATE\",\"ECOMMERCE\",\"ITEMS\" FROM \"GA4\".\"GA4_OBFUSCATED_SAMPLE_ECOMMERCE\".\"EVENTS_20201208\"\n  UNION ALL SELECT \"EVENT_NAME\",\"EVENT_DATE\",\"ECOMMERCE\",\"ITEMS\" FROM \"GA4\".\"GA4_OBFUSCATED_SAMPLE_ECOMMERCE\".\"EVENTS_20201209\"\n  UNION ALL SELECT \"EVENT_NAME\",\"EVENT_DATE\",\"ECOMMERCE\",\"ITEMS\" FROM \"GA4\".\"GA4_OBFUSCATED_SAMPLE_ECOMMERCE\".\"EVENTS_20201210\"\n  UNION ALL SELECT \"EVENT_NAME\",\"EVENT_DATE\",\"ECOMMERCE\",\"ITEMS\" FROM \"GA4\".\"GA4_OBFUSCATED_SAMPLE_ECOMMERCE\".\"EVENTS_20201211\"\n  UNION ALL SELECT \"EVENT_NAME\",\"EVENT_DATE\",\"ECOMMERCE\",\"ITEMS\" FROM \"GA4\".\"GA4_OBFUSCATED_SAMPLE_ECOMMERCE\".\"EVENTS_20201212\"\n  UNION ALL SELECT \"EVENT_NAME\",\"EVENT_DATE\",\"ECOMMERCE\",\"ITEMS\" FROM \"GA4\".\"GA4_OBFUSCATED_SAMPLE_ECOMMERCE\".\"EVENTS_20201213\"\n  UNION ALL SELECT \"EVENT_NAME\",\"EVENT_DATE\",\"ECOMMERCE\",\"ITEMS\" FROM \"GA4\".\"GA4_OBFUSCATED_SAMPLE_ECOMMERCE\".\"EVENTS_20201214\"\n  UNION ALL SELECT \"EVENT_NAME\",\"EVENT_DATE\",\"ECOMMERCE\",\"ITEMS\" FROM \"GA4\".\"GA4_OBFUSCATED_SAMPLE_ECOMMERCE\".\"EVENTS_20201215\"\n  UNION ALL SELECT \"EVENT_NAME\",\"EVENT_DATE\",\"ECOMMERCE\",\"ITEMS\" FROM \"GA4\".\"GA4_OBFUSCATED_SAMPLE_ECOMMERCE\".\"EVENTS_20201216\"\n  UNION ALL SELECT \"EVENT_NAME\",\"EVENT_DATE\",\"ECOMMERCE\",\"ITEMS\" FROM \"GA4\".\"GA4_OBFUSCATED_SAMPLE_ECOMMERCE\".\"EVENTS_20201217\"\n  UNION ALL SELECT \"EVENT_NAME\",\"EVENT_DATE\",\"ECOMMERCE\",\"ITEMS\" FROM \"GA4\".\"GA4_OBFUSCATED_SAMPLE_ECOMMERCE\".\"EVENTS_20201218\"\n  UNION ALL SELECT \"EVENT_NAME\",\"EVENT_DATE\",\"ECOMMERCE\",\"ITEMS\" FROM \"GA4\".\"GA4_OBFUSCATED_SAMPLE_ECOMMERCE\".\"EVENTS_20201219\"\n  UNION ALL SELECT \"EVENT_NAME\",\"EVENT_DATE\",\"ECOMMERCE\",\"ITEMS\" FROM \"GA4\".\"GA4_OBFUSCATED_SAMPLE_ECOMMERCE\".\"EVENTS_20201220\"\n  UNION ALL SELECT \"EVENT_NAME\",\"EVENT_DATE\",\"ECOMMERCE\",\"ITEMS\" FROM \"GA4\".\"GA4_OBFUSCATED_SAMPLE_ECOMMERCE\".\"EVENTS_20201221\"\n  UNION ALL SELECT \"EVENT_NAME\",\"EVENT_DATE\",\"ECOMMERCE\",\"ITEMS\" FROM \"GA4\".\"GA4_OBFUSCATED_SAMPLE_ECOMMERCE\".\"EVENTS_20201222\"\n  UNION ALL SELECT \"EVENT_NAME\",\"EVENT_DATE\",\"ECOMMERCE\",\"ITEMS\" FROM \"GA4\".\"GA4_OBFUSCATED_SAMPLE_ECOMMERCE\".\"EVENTS_20201223\"\n  UNION ALL SELECT \"EVENT_NAME\",\"EVENT_DATE\",\"ECOMMERCE\",\"ITEMS\" FROM \"GA4\".\"GA4_OBFUSCATED_SAMPLE_ECOMMERCE\".\"EVENTS_20201224\"\n  UNION ALL SELECT \"EVENT_NAME\",\"EVENT_DATE\",\"ECOMMERCE\",\"ITEMS\" FROM \"GA4\".\"GA4_OBFUSCATED_SAMPLE_ECOMMERCE\".\"EVENTS_20201225\"\n  UNION ALL SELECT \"EVENT_NAME\",\"EVENT_DATE\",\"ECOMMERCE\",\"ITEMS\" FROM \"GA4\".\"GA4_OBFUSCATED_SAMPLE_ECOMMERCE\".\"EVENTS_20201226\"\n  UNION ALL SELECT \"EVENT_NAME\",\"EVENT_DATE\",\"ECOMMERCE\",\"ITEMS\" FROM \"GA4\".\"GA4_OBFUSCATED_SAMPLE_ECOMMERCE\".\"EVENTS_20201227\"\n  UNION ALL SELECT \"EVENT_NAME\",\"EVENT_DATE\",\"ECOMMERCE\",\"ITEMS\" FROM \"GA4\".\"GA4_OBFUSCATED_SAMPLE_ECOMMERCE\".\"EVENTS_20201228\"\n  UNION ALL SELECT \"EVENT_NAME\",\"EVENT_DATE\",\"ECOMMERCE\",\"ITEMS\" FROM \"GA4\".\"GA4_OBFUSCATED_SAMPLE_ECOMMERCE\".\"EVENTS_20201229\"\n  UNION ALL SELECT \"EVENT_NAME\",\"EVENT_DATE\",\"ECOMMERCE\",\"ITEMS\" FROM \"GA4\".\"GA4_OBFUSCATED_SAMPLE_ECOMMERCE\".\"EVENTS_20201230\"\n  UNION ALL SELECT \"EVENT_NAME\",\"EVENT_DATE\",\"ECOMMERCE\",\"ITEMS\" FROM \"GA4\".\"GA4_OBFUSCATED_SAMPLE_ECOMMERCE\".\"EVENTS_20201231\"\n),\n\"purchase_items\" AS (\n  SELECT\n    (\"ECOMMERCE\":\"transaction_id\")::string AS \"TRANSACTION_ID\",\n    f.value:\"item_id\"::string AS \"ITEM_ID\",\n    f.value:\"item_name\"::string AS \"ITEM_NAME\",\n    COALESCE(f.value:\"quantity\"::number, 1) AS \"QUANTITY\"\n  FROM \"events_dec\" e,\n  LATERAL FLATTEN(input => e.\"ITEMS\") f\n  WHERE e.\"EVENT_NAME\" = 'purchase'\n    AND (\"ECOMMERCE\":\"transaction_id\") IS NOT NULL\n    AND (\"ECOMMERCE\":\"transaction_id\")::string != '(not set)'\n),\n\"tx_with_target\" AS (\n  SELECT DISTINCT \"TRANSACTION_ID\"\n  FROM \"purchase_items\"\n  WHERE UPPER(\"ITEM_NAME\") = 'GOOGLE NAVY SPECKLED TEE'\n)\nSELECT\n  p.\"ITEM_NAME\" AS \"OTHER_PRODUCT_NAME\",\n  SUM(p.\"QUANTITY\") AS \"TOTAL_QUANTITY_ALONGSIDE\"\nFROM \"purchase_items\" p\nJOIN \"tx_with_target\" t\n  ON p.\"TRANSACTION_ID\" = t.\"TRANSACTION_ID\"\nWHERE UPPER(p.\"ITEM_NAME\") != 'GOOGLE NAVY SPECKLED TEE'\nGROUP BY p.\"ITEM_NAME\"\nORDER BY SUM(p.\"QUANTITY\") DESC, p.\"ITEM_NAME\" ASC\nLIMIT 1;"
    },
    {
        "instance_id":"sf_local003",
        "instruction":"According to the RFM definition document, calculate the average sales per order for each customer within distinct RFM segments, considering only 'delivered' orders. Use the customer unique identifier. Clearly define how to calculate Recency based on the latest purchase timestamp and specify the criteria for classifying RFM segments. The average sales should be computed as the total spend divided by the total number of orders. Please analyze and report the differences in average sales across the RFM segments",
        "db_id":"E_COMMERCE",
        "external_knowledge":"RFM.md",
        "sql":"WITH CustomerOrderStats AS (\n    SELECT\n        C.\"customer_unique_id\",\n        O.\"order_id\",\n        TO_TIMESTAMP(O.\"order_purchase_timestamp\") AS \"order_purchase_timestamp\",\n        P.\"payment_value\"\n    FROM \"E_COMMERCE\".\"E_COMMERCE\".\"CUSTOMERS\" AS C\n    JOIN \"E_COMMERCE\".\"E_COMMERCE\".\"ORDERS\" AS O\n        ON C.\"customer_id\" = O.\"customer_id\"\n    JOIN \"E_COMMERCE\".\"E_COMMERCE\".\"ORDER_PAYMENTS\" AS P\n        ON O.\"order_id\" = P.\"order_id\"\n    WHERE O.\"order_status\" = 'delivered'\n),\nMaxDate AS (\n    SELECT MAX(\"order_purchase_timestamp\") AS \"max_purchase_date\" FROM CustomerOrderStats\n),\nRFM_Base AS (\n    SELECT\n        \"customer_unique_id\",\n        DATEDIFF(day, MAX(\"order_purchase_timestamp\"), (SELECT \"max_purchase_date\" FROM MaxDate)) AS \"Recency\",\n        COUNT(DISTINCT \"order_id\") AS \"Frequency\",\n        SUM(\"payment_value\") AS \"Monetary\",\n        COUNT(DISTINCT \"order_id\") AS \"TotalOrders\",\n        SUM(\"payment_value\") AS \"TotalSpend\"\n    FROM CustomerOrderStats\n    GROUP BY \"customer_unique_id\"\n),\nRFM_Scores AS (\n    SELECT\n        *,\n        NTILE(5) OVER (ORDER BY \"Recency\" ASC) AS \"R\",\n        NTILE(5) OVER (ORDER BY \"Frequency\" DESC) AS \"F\",\n        NTILE(5) OVER (ORDER BY \"Monetary\" DESC) AS \"M\"\n    FROM RFM_Base\n),\nRFM_Segments AS (\n    SELECT\n        *,\n        CASE\n            WHEN \"R\" = 1 AND (\"F\" + \"M\") BETWEEN 1 AND 4 THEN 'Champions'\n            WHEN (\"R\" = 4 OR \"R\" = 5) AND (\"F\" + \"M\") BETWEEN 1 AND 2 THEN 'Can''t Lose Them'\n            WHEN (\"R\" = 4 OR \"R\" = 5) AND (\"F\" + \"M\") BETWEEN 3 AND 6 THEN 'Hibernating'\n            WHEN (\"R\" = 4 OR \"R\" = 5) AND (\"F\" + \"M\") BETWEEN 7 AND 10 THEN 'Lost'\n            WHEN (\"R\" = 2 OR \"R\" = 3) AND (\"F\" + \"M\") BETWEEN 1 AND 4 THEN 'Loyal Customers'\n            WHEN \"R\" = 3 AND (\"F\" + \"M\") BETWEEN 5 AND 6 THEN 'Needs Attention'\n            WHEN \"R\" = 1 AND (\"F\" + \"M\") BETWEEN 7 AND 8 THEN 'Recent Users'\n            WHEN (\"R\" = 1 AND (\"F\" + \"M\") BETWEEN 5 AND 6) OR (\"R\" = 2 AND (\"F\" + \"M\") BETWEEN 5 AND 8) THEN 'Potential Loyalists'\n            WHEN \"R\" = 1 AND (\"F\" + \"M\") BETWEEN 9 AND 10 THEN 'Price Sensitive'\n            WHEN \"R\" = 2 AND (\"F\" + \"M\") BETWEEN 9 AND 10 THEN 'Promising'\n            WHEN \"R\" = 3 AND (\"F\" + \"M\") BETWEEN 7 AND 10 THEN 'About to Sleep'\n            ELSE 'Others'\n        END AS \"RFM_Segment\"\n    FROM RFM_Scores\n)\nSELECT\n    \"RFM_Segment\",\n    SUM(\"TotalSpend\") \/ SUM(\"TotalOrders\") AS \"AverageSalesPerOrder\"\nFROM RFM_Segments\nGROUP BY \"RFM_Segment\"\nORDER BY \"AverageSalesPerOrder\" DESC"
    },
    {
        "instance_id":"sf_local004",
        "instruction":"Could you tell me the number of orders, average payment per order and customer lifespan in weeks of the 3 custumers with the highest average payment per order, where the lifespan is calculated by subtracting the earliest purchase date from the latest purchase date in days, dividing by seven, and if the result is less than seven days, setting it to 1.0?",
        "db_id":"E_COMMERCE",
        "external_knowledge":null,
        "sql":"WITH OrderTotalPayments AS (\n    SELECT\n        \"order_id\",\n        SUM(\"payment_value\") AS \"total_payment\"\n    FROM \"E_COMMERCE\".\"E_COMMERCE\".\"ORDER_PAYMENTS\"\n    GROUP BY \"order_id\"\n)\nSELECT\n    C.\"customer_unique_id\",\n    COUNT(O.\"order_id\") AS \"number_of_orders\",\n    AVG(OTP.\"total_payment\") AS \"average_payment_per_order\",\n    CASE\n        WHEN DATEDIFF('day', MIN(TO_TIMESTAMP(O.\"order_purchase_timestamp\")), MAX(TO_TIMESTAMP(O.\"order_purchase_timestamp\"))) < 7\n        THEN 1.0\n        ELSE DATEDIFF('day', MIN(TO_TIMESTAMP(O.\"order_purchase_timestamp\")), MAX(TO_TIMESTAMP(O.\"order_purchase_timestamp\"))) \/ 7.0\n    END AS \"customer_lifespan_in_weeks\"\nFROM \"E_COMMERCE\".\"E_COMMERCE\".\"CUSTOMERS\" AS C\nJOIN \"E_COMMERCE\".\"E_COMMERCE\".\"ORDERS\" AS O ON C.\"customer_id\" = O.\"customer_id\"\nJOIN OrderTotalPayments AS OTP ON O.\"order_id\" = OTP.\"order_id\"\nGROUP BY C.\"customer_unique_id\"\nORDER BY \"average_payment_per_order\" DESC\nLIMIT 3;"
    },
    {
        "instance_id":"sf_local009",
        "instruction":"What is the distance of the longest route where Abakan is either the departure or destination city (in kilometers)?",
        "db_id":"AIRLINES",
        "external_knowledge":"haversine_formula.md",
        "sql":"WITH flights_abakan AS (\n  SELECT\n    f.\"flight_id\",\n    dep.\"airport_code\" AS \"dep_code\",\n    arr.\"airport_code\" AS \"arr_code\",\n    TRY_PARSE_JSON(dep.\"city\"):\"en\"::string AS \"dep_city\",\n    TRY_PARSE_JSON(arr.\"city\"):\"en\"::string AS \"arr_city\",\n    dep.\"coordinates\" AS \"dep_coord\",\n    arr.\"coordinates\" AS \"arr_coord\"\n  FROM \"AIRLINES\".\"AIRLINES\".\"FLIGHTS\" AS f\n  JOIN \"AIRLINES\".\"AIRLINES\".\"AIRPORTS_DATA\" AS dep\n    ON dep.\"airport_code\" = f.\"departure_airport\"\n  JOIN \"AIRLINES\".\"AIRLINES\".\"AIRPORTS_DATA\" AS arr\n    ON arr.\"airport_code\" = f.\"arrival_airport\"\n  WHERE TRY_PARSE_JSON(dep.\"city\"):\"en\"::string = 'Abakan'\n     OR TRY_PARSE_JSON(arr.\"city\"):\"en\"::string = 'Abakan'\n),\ncoords AS (\n  SELECT\n    \"flight_id\",\n    CAST(SPLIT_PART(REPLACE(REPLACE(\"dep_coord\",'(',''),')',''), ',', 1) AS DOUBLE) AS \"dep_lon\",\n    CAST(SPLIT_PART(REPLACE(REPLACE(\"dep_coord\",'(',''),')',''), ',', 2) AS DOUBLE) AS \"dep_lat\",\n    CAST(SPLIT_PART(REPLACE(REPLACE(\"arr_coord\",'(',''),')',''), ',', 1) AS DOUBLE) AS \"arr_lon\",\n    CAST(SPLIT_PART(REPLACE(REPLACE(\"arr_coord\",'(',''),')',''), ',', 2) AS DOUBLE) AS \"arr_lat\"\n  FROM flights_abakan\n),\ndist AS (\n  SELECT\n    \"flight_id\",\n    6371 * 2 * ASIN(\n      SQRT(\n        POWER(SIN((RADIANS(\"arr_lat\") - RADIANS(\"dep_lat\")) \/ 2), 2)\n        + COS(RADIANS(\"dep_lat\")) * COS(RADIANS(\"arr_lat\")) * POWER(SIN((RADIANS(\"arr_lon\") - RADIANS(\"dep_lon\")) \/ 2), 2)\n      )\n    ) AS \"distance_km\"\n  FROM coords\n)\nSELECT MAX(\"distance_km\") AS \"max_distance_km\"\nFROM dist;"
    },
    {
        "instance_id":"sf_local010",
        "instruction":"Distribute all the unique city pairs into the distance ranges 0, 1000, 2000, 3000, 4000, 5000, and 6000+, based on their average distance of all routes between them. Then how many pairs are there in the distance range with the fewest unique city paires?",
        "db_id":"AIRLINES",
        "external_knowledge":"haversine_formula.md",
        "sql":"\nWITH FLIGHT_INFO AS (\n    SELECT    \n        FLIGHTS.\"flight_id\",\n        PARSE_JSON(DEPARTURE.\"city\"):\"en\" AS \"from_city\",\n        CAST(SUBSTR(DEPARTURE.\"coordinates\", 2, POSITION(',' IN DEPARTURE.\"coordinates\") - 2) AS DOUBLE) AS \"from_longitude\",\n        CAST(SUBSTR(DEPARTURE.\"coordinates\", POSITION(',' IN DEPARTURE.\"coordinates\") + 1, LENGTH(DEPARTURE.\"coordinates\") - POSITION(',' IN DEPARTURE.\"coordinates\") - 2) AS DOUBLE) AS \"from_latitude\",\n        PARSE_JSON(ARRIVAL.\"city\"):\"en\" AS \"to_city\",\n        CAST(SUBSTR(ARRIVAL.\"coordinates\", 2, POSITION(',' IN ARRIVAL.\"coordinates\") - 2) AS DOUBLE) AS \"to_longitude\",\n        CAST(SUBSTR(ARRIVAL.\"coordinates\", POSITION(',' IN ARRIVAL.\"coordinates\") + 1, LENGTH(ARRIVAL.\"coordinates\") - POSITION(',' IN ARRIVAL.\"coordinates\") - 2) AS DOUBLE) AS \"to_latitude\"\n    FROM\n        AIRLINES.AIRLINES.FLIGHTS \n    LEFT JOIN AIRLINES.AIRLINES.AIRPORTS_DATA AS DEPARTURE ON FLIGHTS.\"departure_airport\" = DEPARTURE.\"airport_code\"\n    LEFT JOIN AIRLINES.AIRLINES.AIRPORTS_DATA AS ARRIVAL ON FLIGHTS.\"arrival_airport\" = ARRIVAL.\"airport_code\"\n)\n-- Create a histogram distribution of average_distance_km\nSELECT \"group_count\" FROM\n(\n    SELECT\n        FLOOR(\"average_distance_km\" \/ 1000) * 1000 AS \"distance_range\",\n        COUNT(*) AS \"group_count\"\n    FROM (\n        -- Calculate the average distance for each unique combination of from_city and to_city\n        SELECT\n            \"from_city\",\n            \"to_city\",\n            AVG(\"distance_km\") AS \"average_distance_km\"\n        FROM (\n            -- Subquery to calculate the distances as before\n            SELECT\n                \"from_city\",\n                \"to_city\",\n                -- Calculate the distance using the Haversine formula\n                2 * 6371 * ASIN(SQRT(\n                    POWER(SIN(RADIANS((\"to_latitude\" - \"from_latitude\") \/ 2)), 2) +\n                    COS(RADIANS(\"from_latitude\")) * COS(RADIANS(\"to_latitude\")) *\n                    POWER(SIN(RADIANS((\"to_longitude\" - \"from_longitude\") \/ 2)), 2)\n                )) AS \"distance_km\"\n            FROM FLIGHT_INFO\n        ) AS subquery\n        GROUP BY \"from_city\", \"to_city\"\n    ) AS distances\n    GROUP BY \"distance_range\"\n    ORDER BY \"group_count\"\n    LIMIT 1\n)"
    },
    {
        "instance_id":"sf_local015",
        "instruction":"Please calculate the fatality rate for motorcycle collisions, separated by helmet usage. Specifically, calculate two percentages: 1) the percentage of motorcyclist fatalities in collisions where parties (drivers or passengers) were wearing helmets, and 2) the percentage of motorcyclist fatalities in collisions where parties were not wearing helmets. For each group, compute this by dividing the total number of motorcyclist fatalities by the total number of collisions involving that group. Use the parties table to determine helmet usage (from party_safety_equipment fields).",
        "db_id":"CALIFORNIA_TRAFFIC_COLLISION",
        "external_knowledge":null,
        "sql":"WITH \"moto_collisions\" AS (\n  SELECT\n    \"case_id\",\n    COALESCE(\"motorcyclist_killed_count\", 0) AS \"motorcyclist_killed_count\"\n  FROM \"CALIFORNIA_TRAFFIC_COLLISION\".\"CALIFORNIA_TRAFFIC_COLLISION\".\"COLLISIONS\"\n  WHERE COALESCE(\"motorcycle_collision\", 0) = 1\n),\n\"moto_party_helmet\" AS (\n  SELECT\n    p.\"case_id\",\n    MAX(\n      CASE\n        WHEN (\n          (LOWER(COALESCE(p.\"party_safety_equipment_1\", '')) LIKE '%helmet%' AND LOWER(COALESCE(p.\"party_safety_equipment_1\", '')) NOT LIKE '%not%' AND LOWER(COALESCE(p.\"party_safety_equipment_1\", '')) NOT LIKE '%no helmet%')\n          OR\n          (LOWER(COALESCE(p.\"party_safety_equipment_2\", '')) LIKE '%helmet%' AND LOWER(COALESCE(p.\"party_safety_equipment_2\", '')) NOT LIKE '%not%' AND LOWER(COALESCE(p.\"party_safety_equipment_2\", '')) NOT LIKE '%no helmet%')\n        ) THEN 1 ELSE 0\n      END\n    ) AS \"worn_any\",\n    MAX(\n      CASE\n        WHEN (\n          (LOWER(COALESCE(p.\"party_safety_equipment_1\", '')) LIKE '%helmet%' AND (LOWER(COALESCE(p.\"party_safety_equipment_1\", '')) LIKE '%not%' OR LOWER(COALESCE(p.\"party_safety_equipment_1\", '')) LIKE '%no helmet%'))\n          OR\n          (LOWER(COALESCE(p.\"party_safety_equipment_2\", '')) LIKE '%helmet%' AND (LOWER(COALESCE(p.\"party_safety_equipment_2\", '')) LIKE '%not%' OR LOWER(COALESCE(p.\"party_safety_equipment_2\", '')) LIKE '%no helmet%'))\n        ) THEN 1 ELSE 0\n      END\n    ) AS \"not_worn_any\"\n  FROM \"CALIFORNIA_TRAFFIC_COLLISION\".\"CALIFORNIA_TRAFFIC_COLLISION\".\"PARTIES\" p\n  INNER JOIN \"moto_collisions\" mc\n    ON mc.\"case_id\" = p.\"case_id\"\n  WHERE LOWER(COALESCE(p.\"statewide_vehicle_type\", '')) LIKE '%motorcycle%'\n  GROUP BY p.\"case_id\"\n),\n\"classified\" AS (\n  SELECT\n    mc.\"case_id\",\n    mc.\"motorcyclist_killed_count\",\n    CASE\n      WHEN mph.\"worn_any\" = 1 AND COALESCE(mph.\"not_worn_any\", 0) = 0 THEN 'helmet_worn'\n      WHEN mph.\"not_worn_any\" = 1 AND COALESCE(mph.\"worn_any\", 0) = 0 THEN 'no_helmet'\n      ELSE NULL\n    END AS \"helmet_group\"\n  FROM \"moto_collisions\" mc\n  LEFT JOIN \"moto_party_helmet\" mph\n    ON mc.\"case_id\" = mph.\"case_id\"\n)\nSELECT\n  'helmet_worn' AS \"helmet_usage\",\n  COALESCE(\n    100.0 * SUM(CASE WHEN \"helmet_group\" = 'helmet_worn' THEN \"motorcyclist_killed_count\" ELSE 0 END)\n    \/ NULLIF(SUM(CASE WHEN \"helmet_group\" = 'helmet_worn' THEN 1 ELSE 0 END), 0),\n    0\n  ) AS \"fatality_percentage\"\nFROM \"classified\"\nUNION ALL\nSELECT\n  'no_helmet' AS \"helmet_usage\",\n  COALESCE(\n    100.0 * SUM(CASE WHEN \"helmet_group\" = 'no_helmet' THEN \"motorcyclist_killed_count\" ELSE 0 END)\n    \/ NULLIF(SUM(CASE WHEN \"helmet_group\" = 'no_helmet' THEN 1 ELSE 0 END), 0),\n    0\n  ) AS \"fatality_percentage\"\nFROM \"classified\";"
    },
    {
        "instance_id":"sf_local019",
        "instruction":"For the NXT title that had the shortest match (excluding titles with \"title change\"), what were the names of the two wrestlers involved?",
        "db_id":"WWE",
        "external_knowledge":null,
        "sql":"SELECT w1.\"name\" as wrestler1, w2.\"name\" as wrestler2\nFROM WWE.WWE.MATCHES m\nJOIN WWE.WWE.WRESTLERS w1 ON m.\"winner_id\" = w1.\"id\"::VARCHAR\nJOIN WWE.WWE.WRESTLERS w2 ON m.\"loser_id\" = w2.\"id\"::VARCHAR\nWHERE m.\"title_id\" IN (SELECT \"id\"::VARCHAR FROM WWE.WWE.BELTS WHERE \"name\" LIKE '%NXT%') \nAND m.\"title_change\" = 0 \nAND m.\"duration\" IS NOT NULL \nAND m.\"duration\" != '' \nORDER BY m.\"duration\" ASC \nLIMIT 1"
    },
    {
        "instance_id":"sf_local026",
        "instruction":"Please help me identify the top 3 bowlers who, in the overs where the maximum runs were conceded in each match, gave up the highest number of runs in a single over across all matches. For each of these bowlers, provide the match in which they conceded these maximum runs. Only consider overs that had the most runs conceded within their respective matches, and among these, determine which bowlers conceded the most runs in a single over overall.",
        "db_id":"IPL",
        "external_knowledge":null,
        "sql":"-- Status: Finished in turn 6.\n\nWITH over_runs AS (\n  SELECT \n    \"bbb\".\"match_id\",\n    \"bbb\".\"over_id\",\n    \"bbb\".\"bowler\",\n    COALESCE(SUM(\"bs\".\"runs_scored\"), 0) + COALESCE(SUM(\"er\".\"extra_runs\"), 0) AS \"total_runs_conceded\"\n  FROM \"IPL\".\"IPL\".\"BALL_BY_BALL\" \"bbb\"\n  LEFT JOIN \"IPL\".\"IPL\".\"BATSMAN_SCORED\" \"bs\" \n    ON \"bbb\".\"match_id\" = \"bs\".\"match_id\" \n    AND \"bbb\".\"innings_no\" = \"bs\".\"innings_no\"\n    AND \"bbb\".\"over_id\" = \"bs\".\"over_id\"\n    AND \"bbb\".\"ball_id\" = \"bs\".\"ball_id\"\n  LEFT JOIN \"IPL\".\"IPL\".\"EXTRA_RUNS\" \"er\"\n    ON \"bbb\".\"match_id\" = \"er\".\"match_id\"\n    AND \"bbb\".\"innings_no\" = \"er\".\"innings_no\" \n    AND \"bbb\".\"over_id\" = \"er\".\"over_id\"\n    AND \"bbb\".\"ball_id\" = \"er\".\"ball_id\"\n  GROUP BY \"bbb\".\"match_id\", \"bbb\".\"over_id\", \"bbb\".\"bowler\"\n),\nmax_runs_per_match AS (\n  SELECT \n    \"match_id\",\n    MAX(\"total_runs_conceded\") AS \"max_runs_conceded_in_single_over\"\n  FROM over_runs\n  GROUP BY \"match_id\"\n),\nworst_bowlers_per_match AS (\n  SELECT \n    \"or\".\"match_id\",\n    \"or\".\"bowler\",\n    \"or\".\"over_id\",\n    \"or\".\"total_runs_conceded\"\n  FROM over_runs \"or\"\n  INNER JOIN max_runs_per_match \"mrpm\"\n    ON \"or\".\"match_id\" = \"mrpm\".\"match_id\"\n    AND \"or\".\"total_runs_conceded\" = \"mrpm\".\"max_runs_conceded_in_single_over\"\n),\ntop_3_worst_bowlers AS (\n  SELECT \n    \"match_id\",\n    \"bowler\",\n    \"over_id\",\n    \"total_runs_conceded\",\n    ROW_NUMBER() OVER (ORDER BY \"total_runs_conceded\" DESC) AS \"rank\"\n  FROM worst_bowlers_per_match\n)\nSELECT \n  \"p\".\"player_name\",\n  \"t3wb\".\"total_runs_conceded\",\n  \"t3wb\".\"match_id\"\nFROM top_3_worst_bowlers \"t3wb\"\nINNER JOIN \"IPL\".\"IPL\".\"PLAYER\" \"p\"\n  ON \"t3wb\".\"bowler\" = \"p\".\"player_id\"\nWHERE \"t3wb\".\"rank\" <= 3"
    },
    {
        "instance_id":"sf_local022",
        "instruction":"Retrieve the names of players who scored no less than 100 runs in a match while playing for the team that lost that match.",
        "db_id":"IPL",
        "external_knowledge":null,
        "sql":"with \"PLAYER_RUNS\" as (\n  select\n    \"B\".\"match_id\",\n    \"B\".\"striker\" as \"player_id\",\n    sum(\"BS\".\"runs_scored\") as \"runs\"\n  from \"IPL\".\"IPL\".\"BALL_BY_BALL\" \"B\"\n  join \"IPL\".\"IPL\".\"BATSMAN_SCORED\" \"BS\"\n    on \"B\".\"match_id\" = \"BS\".\"match_id\"\n   and \"B\".\"over_id\" = \"BS\".\"over_id\"\n   and \"B\".\"ball_id\" = \"BS\".\"ball_id\"\n   and \"B\".\"innings_no\" = \"BS\".\"innings_no\"\n  group by \"B\".\"match_id\", \"B\".\"striker\"\n)\nselect distinct \"P\".\"player_name\"\nfrom \"PLAYER_RUNS\" \"PR\"\njoin \"IPL\".\"IPL\".\"PLAYER_MATCH\" \"PM\"\n  on \"PM\".\"match_id\" = \"PR\".\"match_id\"\n and \"PM\".\"player_id\" = \"PR\".\"player_id\"\njoin \"IPL\".\"IPL\".\"MATCH\" \"M\"\n  on \"M\".\"match_id\" = \"PR\".\"match_id\"\njoin \"IPL\".\"IPL\".\"PLAYER\" \"P\"\n  on \"P\".\"player_id\" = \"PR\".\"player_id\"\nwhere \"PR\".\"runs\" >= 100\n  and \"M\".\"match_winner\" is not null\n  and \"PM\".\"team_id\" in (\"M\".\"team_1\", \"M\".\"team_2\")\n  and \"PM\".\"team_id\" != \"M\".\"match_winner\""
    },
    {
        "instance_id":"sf_local028",
        "instruction":"Could you generate a report that shows the number of delivered orders for each month in the years 2016, 2017, and 2018? Each column represents a year, and each row represents a month",
        "db_id":"BRAZILIAN_E_COMMERCE",
        "external_knowledge":null,
        "sql":"SELECT \n    CASE delivery_month \n        WHEN 1 THEN 'January'\n        WHEN 2 THEN 'February' \n        WHEN 3 THEN 'March'\n        WHEN 4 THEN 'April'\n        WHEN 5 THEN 'May'\n        WHEN 6 THEN 'June'\n        WHEN 7 THEN 'July'\n        WHEN 8 THEN 'August'\n        WHEN 9 THEN 'September'\n        WHEN 10 THEN 'October'\n        WHEN 11 THEN 'November'\n        WHEN 12 THEN 'December'\n    END as \"Month\",\n    SUM(CASE WHEN delivery_year = 2016 THEN delivered_orders ELSE 0 END) as \"2016\",\n    SUM(CASE WHEN delivery_year = 2017 THEN delivered_orders ELSE 0 END) as \"2017\",\n    SUM(CASE WHEN delivery_year = 2018 THEN delivered_orders ELSE 0 END) as \"2018\"\nFROM (\n    SELECT \n        EXTRACT(YEAR FROM TO_TIMESTAMP(\"order_delivered_customer_date\", 'YYYY-MM-DD HH24:MI:SS')) as delivery_year,\n        EXTRACT(MONTH FROM TO_TIMESTAMP(\"order_delivered_customer_date\", 'YYYY-MM-DD HH24:MI:SS')) as delivery_month,\n        COUNT(*) as delivered_orders\n    FROM BRAZILIAN_E_COMMERCE.BRAZILIAN_E_COMMERCE.OLIST_ORDERS \n    WHERE \"order_status\" = 'delivered' \n        AND \"order_delivered_customer_date\" IS NOT NULL \n        AND \"order_delivered_customer_date\" != ''\n        AND EXTRACT(YEAR FROM TO_TIMESTAMP(\"order_delivered_customer_date\", 'YYYY-MM-DD HH24:MI:SS')) IN (2016, 2017, 2018)\n    GROUP BY delivery_year, delivery_month\n) monthly_data\nGROUP BY delivery_month\nORDER BY delivery_month"
    },
    {
        "instance_id":"sf_local030",
        "instruction":"Among all cities with delivered orders, find the five cities whose summed payments are the lowest, then calculate the average of their total payments and the average of their total delivered order counts.",
        "db_id":"BRAZILIAN_E_COMMERCE",
        "external_knowledge":null,
        "sql":"WITH CityPayments AS (\n  SELECT\n    C.\"customer_city\",\n    SUM(P.\"payment_value\") AS total_payment,\n    COUNT(DISTINCT O.\"order_id\") AS delivered_order_count\n  FROM\n    \"BRAZILIAN_E_COMMERCE\".\"BRAZILIAN_E_COMMERCE\".\"OLIST_ORDERS\" AS O\n    JOIN \"BRAZILIAN_E_COMMERCE\".\"BRAZILIAN_E_COMMERCE\".\"OLIST_CUSTOMERS\" AS C\n      ON O.\"customer_id\" = C.\"customer_id\"\n    JOIN \"BRAZILIAN_E_COMMERCE\".\"BRAZILIAN_E_COMMERCE\".\"OLIST_ORDER_PAYMENTS\" AS P\n      ON O.\"order_id\" = P.\"order_id\"\n  WHERE\n    O.\"order_status\" = 'delivered'\n  GROUP BY\n    C.\"customer_city\"\n), Top5Cities AS (\n  SELECT\n    total_payment,\n    delivered_order_count\n  FROM CityPayments\n  ORDER BY\n    total_payment ASC\n  LIMIT 5\n)\nSELECT\n  AVG(total_payment),\n  AVG(delivered_order_count)\nFROM Top5Cities;"
    },
    {
        "instance_id":"sf_local038",
        "instruction":"Could you help me determine which actor starred most frequently in English-language children's category films that were rated either G or PG, had a running time of 120 minutes or less, and were released between 2000 and 2010? Please provide the actor's full name.",
        "db_id":"PAGILA",
        "external_knowledge":null,
        "sql":"SELECT CONCAT(\"a\".\"first_name\", ' ', \"a\".\"last_name\") AS \"actor_full_name\"\nFROM \"PAGILA\".\"PAGILA\".\"FILM\" AS \"f\"\nJOIN \"PAGILA\".\"PAGILA\".\"LANGUAGE\" AS \"l\" ON \"f\".\"language_id\" = \"l\".\"language_id\"\nJOIN \"PAGILA\".\"PAGILA\".\"FILM_CATEGORY\" AS \"fc\" ON \"fc\".\"film_id\" = \"f\".\"film_id\"\nJOIN \"PAGILA\".\"PAGILA\".\"CATEGORY\" AS \"c\" ON \"c\".\"category_id\" = \"fc\".\"category_id\"\nJOIN \"PAGILA\".\"PAGILA\".\"FILM_ACTOR\" AS \"fa\" ON \"fa\".\"film_id\" = \"f\".\"film_id\"\nJOIN \"PAGILA\".\"PAGILA\".\"ACTOR\" AS \"a\" ON \"a\".\"actor_id\" = \"fa\".\"actor_id\"\nWHERE UPPER(\"c\".\"name\") = 'CHILDREN'\n  AND UPPER(\"l\".\"name\") = 'ENGLISH'\n  AND \"f\".\"rating\" IN ('G', 'PG')\n  AND \"f\".\"length\" <= 120\n  AND TRY_TO_NUMBER(\"f\".\"release_year\") BETWEEN 2000 AND 2010\nGROUP BY 1\nORDER BY COUNT(DISTINCT \"f\".\"film_id\") DESC, \"actor_full_name\"\nLIMIT 1;"
    },
    {
        "instance_id":"sf_local039",
        "instruction":"Please help me find the film category with the highest total rental hours in cities where the city's name either starts with \"A\" or contains a hyphen. ",
        "db_id":"PAGILA",
        "external_knowledge":null,
        "sql":"SELECT\n  c.\"name\" AS category_name,\n  SUM(DATEDIFF('second', TRY_TO_TIMESTAMP_NTZ(r.\"rental_date\"), TRY_TO_TIMESTAMP_NTZ(r.\"return_date\")))\/3600.0 AS total_rental_hours\nFROM \"PAGILA\".\"PAGILA\".\"RENTAL\" r\nJOIN \"PAGILA\".\"PAGILA\".\"INVENTORY\" i ON r.\"inventory_id\" = i.\"inventory_id\"\nJOIN \"PAGILA\".\"PAGILA\".\"FILM_CATEGORY\" fc ON i.\"film_id\" = fc.\"film_id\"\nJOIN \"PAGILA\".\"PAGILA\".\"CATEGORY\" c ON fc.\"category_id\" = c.\"category_id\"\nJOIN \"PAGILA\".\"PAGILA\".\"CUSTOMER\" cu ON r.\"customer_id\" = cu.\"customer_id\"\nJOIN \"PAGILA\".\"PAGILA\".\"ADDRESS\" a ON cu.\"address_id\" = a.\"address_id\"\nJOIN \"PAGILA\".\"PAGILA\".\"CITY\" ci ON a.\"city_id\" = ci.\"city_id\"\nWHERE TRY_TO_TIMESTAMP_NTZ(r.\"rental_date\") IS NOT NULL\n  AND TRY_TO_TIMESTAMP_NTZ(r.\"return_date\") IS NOT NULL\n  AND (ci.\"city\" ILIKE 'A%' OR ci.\"city\" ILIKE '%-%')\nGROUP BY c.\"category_id\", c.\"name\"\nORDER BY total_rental_hours DESC\nLIMIT 1;"
    },
    {
        "instance_id":"sf_local056",
        "instruction":"Which customer has the highest average monthly change in payment amounts? Provide the customer's full name.",
        "db_id":"SQLITE_SAKILA",
        "external_knowledge":null,
        "sql":"WITH MonthlyPayments AS (\n  SELECT\n    \"customer_id\",\n    TO_VARCHAR(TO_TIMESTAMP(\"payment_date\"), 'YYYY-MM') AS \"payment_month\",\n    SUM(\"amount\") AS \"monthly_total\"\n  FROM \"SQLITE_SAKILA\".\"SQLITE_SAKILA\".\"PAYMENT\"\n  GROUP BY \"customer_id\", \"payment_month\"\n), MonthlyChanges AS (\n  SELECT\n    \"customer_id\",\n    ABS(\"monthly_total\" - LAG(\"monthly_total\", 1) OVER (PARTITION BY \"customer_id\" ORDER BY \"payment_month\")) AS \"change\"\n  FROM MonthlyPayments\n), AvgMonthlyChange AS (\n  SELECT\n    \"customer_id\",\n    AVG(\"change\") AS \"avg_change\"\n  FROM MonthlyChanges\n  WHERE \"change\" IS NOT NULL\n  GROUP BY \"customer_id\"\n)\nSELECT\n  T1.\"first_name\" || ' ' || T1.\"last_name\"\nFROM \"SQLITE_SAKILA\".\"SQLITE_SAKILA\".\"CUSTOMER\" AS T1\nJOIN AvgMonthlyChange AS T2\n  ON T1.\"customer_id\" = T2.\"customer_id\"\nORDER BY T2.\"avg_change\" DESC\nLIMIT 1;"
    },
    {
        "instance_id":"sf_local064",
        "instruction":"For each customer and each month of 2020, first calculate the month-end balance by adding all deposit amounts and subtracting all withdrawal amounts that occurred during that specific month. Then determine which month in 2020 has the highest count of customers with a positive month-end balance and which month has the lowest count. For each of these two months, compute the average month-end balance across all customers and provide the difference between these two averages",
        "db_id":"BANK_SALES_TRADING",
        "external_knowledge":null,
        "sql":"WITH \"months\" AS (\n  SELECT 1 AS \"month_num\" UNION ALL SELECT 2 UNION ALL SELECT 3 UNION ALL SELECT 4 UNION ALL\n  SELECT 5 UNION ALL SELECT 6 UNION ALL SELECT 7 UNION ALL SELECT 8 UNION ALL\n  SELECT 9 UNION ALL SELECT 10 UNION ALL SELECT 11 UNION ALL SELECT 12\n),\n\"customers\" AS (\n  SELECT DISTINCT \"customer_id\"\n  FROM \"BANK_SALES_TRADING\".\"BANK_SALES_TRADING\".\"CUSTOMER_TRANSACTIONS\"\n),\n\"txn_agg\" AS (\n  SELECT\n    \"customer_id\",\n    EXTRACT(month FROM CAST(\"txn_date\" AS DATE)) AS \"month_num\",\n    SUM(\n      CASE\n        WHEN LOWER(\"txn_type\") LIKE '%deposit%' THEN \"txn_amount\"\n        WHEN LOWER(\"txn_type\") LIKE '%withdraw%' THEN - \"txn_amount\"\n        ELSE 0\n      END\n    ) AS \"month_sum\"\n  FROM \"BANK_SALES_TRADING\".\"BANK_SALES_TRADING\".\"CUSTOMER_TRANSACTIONS\"\n  WHERE CAST(\"txn_date\" AS DATE) >= DATE_FROM_PARTS(2020,1,1)\n    AND CAST(\"txn_date\" AS DATE) < DATE_FROM_PARTS(2021,1,1)\n  GROUP BY \"customer_id\", EXTRACT(month FROM CAST(\"txn_date\" AS DATE))\n),\n\"customer_months\" AS (\n  SELECT\n    c.\"customer_id\",\n    m.\"month_num\",\n    COALESCE(t.\"month_sum\", 0) AS \"balance\"\n  FROM \"customers\" c\n  CROSS JOIN \"months\" m\n  LEFT JOIN \"txn_agg\" t\n    ON t.\"customer_id\" = c.\"customer_id\"\n   AND t.\"month_num\" = m.\"month_num\"\n),\n\"month_stats\" AS (\n  SELECT\n    \"month_num\",\n    SUM(CASE WHEN \"balance\" > 0 THEN 1 ELSE 0 END) AS \"positive_count\",\n    AVG(\"balance\") AS \"avg_balance\"\n  FROM \"customer_months\"\n  GROUP BY \"month_num\"\n)\nSELECT\n  h.\"month_num\" AS \"highest_month\",\n  h.\"positive_count\" AS \"highest_positive_count\",\n  h.\"avg_balance\" AS \"highest_avg_balance\",\n  l.\"month_num\" AS \"lowest_month\",\n  l.\"positive_count\" AS \"lowest_positive_count\",\n  l.\"avg_balance\" AS \"lowest_avg_balance\",\n  (h.\"avg_balance\" - l.\"avg_balance\") AS \"average_difference\"\nFROM\n  (SELECT * FROM \"month_stats\" ORDER BY \"positive_count\" DESC, \"month_num\" ASC LIMIT 1) h,\n  (SELECT * FROM \"month_stats\" ORDER BY \"positive_count\" ASC, \"month_num\" ASC LIMIT 1) l;"
    },
    {
        "instance_id":"sf_local299",
        "instruction":"For a bank database with customer transactions, calculate each customer's daily running balance (where deposits add to the balance and other transaction types subtract). For each customer and each day, compute the 30-day rolling average balance (only after having 30 days of data, and treating negative averages as zero). Then group these daily averages by month and find each customer's maximum 30-day average balance within each month. Sum these maximum values across all customers for each month. Consider the first month of each customer's transaction history as the baseline period and exclude it from the final results, presenting monthly totals of these summed maximum 30-day average balances.",
        "db_id":"BANK_SALES_TRADING",
        "external_knowledge":null,
        "sql":"WITH bounds AS (\n  SELECT\n    MIN(TO_DATE(\"txn_date\",'YYYY-MM-DD')) AS \"global_min_date\",\n    MAX(TO_DATE(\"txn_date\",'YYYY-MM-DD')) AS \"global_max_date\"\n  FROM \"BANK_SALES_TRADING\".\"BANK_SALES_TRADING\".\"CUSTOMER_TRANSACTIONS\"\n),\ncalendar AS (\n  SELECT DATEADD('day', seq4(), b.\"global_min_date\") AS \"day\"\n  FROM bounds b,\n       TABLE(GENERATOR(ROWCOUNT => 10000))\n  WHERE seq4() <= DATEDIFF('day', b.\"global_min_date\", b.\"global_max_date\")\n),\ncust_bounds AS (\n  SELECT\n    \"customer_id\",\n    MIN(TO_DATE(\"txn_date\", 'YYYY-MM-DD')) AS \"min_date\",\n    MAX(TO_DATE(\"txn_date\", 'YYYY-MM-DD')) AS \"max_date\",\n    DATE_TRUNC('MONTH', MIN(TO_DATE(\"txn_date\", 'YYYY-MM-DD'))) AS \"first_month\"\n  FROM \"BANK_SALES_TRADING\".\"BANK_SALES_TRADING\".\"CUSTOMER_TRANSACTIONS\"\n  GROUP BY \"customer_id\"\n),\ncustomer_days AS (\n  SELECT\n    cb.\"customer_id\",\n    c.\"day\"\n  FROM cust_bounds cb\n  JOIN calendar c\n    ON c.\"day\" BETWEEN cb.\"min_date\" AND cb.\"max_date\"\n),\ndaily_txn_agg AS (\n  SELECT\n    \"customer_id\",\n    TO_DATE(\"txn_date\", 'YYYY-MM-DD') AS \"day\",\n    SUM(CASE WHEN \"txn_type\" = 'deposit' THEN \"txn_amount\" ELSE - \"txn_amount\" END) AS \"daily_net\"\n  FROM \"BANK_SALES_TRADING\".\"BANK_SALES_TRADING\".\"CUSTOMER_TRANSACTIONS\"\n  GROUP BY 1, 2\n),\ndaily_balances AS (\n  SELECT\n    cd.\"customer_id\",\n    cd.\"day\",\n    COALESCE(dta.\"daily_net\", 0) AS \"daily_net\",\n    SUM(COALESCE(dta.\"daily_net\", 0)) \n      OVER (PARTITION BY cd.\"customer_id\" ORDER BY cd.\"day\" \n            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS \"running_balance\",\n    ROW_NUMBER() OVER (PARTITION BY cd.\"customer_id\" ORDER BY cd.\"day\") AS \"row_num\"\n  FROM customer_days cd\n  LEFT JOIN daily_txn_agg dta\n    ON cd.\"customer_id\" = dta.\"customer_id\"\n   AND cd.\"day\" = dta.\"day\"\n),\nrolling_30d AS (\n  SELECT\n    \"customer_id\",\n    \"day\",\n    \"running_balance\",\n    \"row_num\",\n    AVG(\"running_balance\") \n      OVER (PARTITION BY \"customer_id\" ORDER BY \"day\"\n            ROWS BETWEEN 29 PRECEDING AND CURRENT ROW) AS \"avg_30d\"\n  FROM daily_balances\n),\nclamped_30d AS (\n  SELECT\n    r.\"customer_id\",\n    r.\"day\",\n    GREATEST(r.\"avg_30d\", 0) AS \"clamped_30d_avg\",\n    DATE_TRUNC('MONTH', r.\"day\") AS \"month_start\"\n  FROM rolling_30d r\n  WHERE r.\"row_num\" >= 30\n    AND r.\"avg_30d\" IS NOT NULL\n),\nmonthly_customer_max AS (\n  SELECT\n    \"customer_id\",\n    \"month_start\",\n    MAX(\"clamped_30d_avg\") AS \"monthly_max_30d_avg\"\n  FROM clamped_30d\n  GROUP BY 1, 2\n),\nmonthly_customer_filtered AS (\n  SELECT\n    mcm.\"customer_id\",\n    mcm.\"month_start\",\n    mcm.\"monthly_max_30d_avg\"\n  FROM monthly_customer_max mcm\n  JOIN cust_bounds cb\n    ON mcm.\"customer_id\" = cb.\"customer_id\"\n  WHERE mcm.\"month_start\" <> cb.\"first_month\"\n)\nSELECT\n  TO_CHAR(\"month_start\", 'YYYY-MM') AS \"month\",\n  COALESCE(SUM(\"monthly_max_30d_avg\"), 0) AS \"monthly_total_max_30d_avg\"\nFROM monthly_customer_filtered\nGROUP BY \"month_start\"\nORDER BY \"month_start\" ASC;"
    },
    {
        "instance_id":"sf_local300",
        "instruction":"For each customer, calculate their daily balances for every day between their earliest and latest transaction dates, including days without transactions by carrying forward the previous day's balance. Treat any negative daily balances as zero. Then, for each month, determine the highest daily balance each customer had during that month. Finally, for each month, sum these maximum daily balances across all customers to obtain a monthly total.",
        "db_id":"BANK_SALES_TRADING",
        "external_knowledge":null,
        "sql":"WITH customer_date_range AS (\n    SELECT \n        \"customer_id\",\n        MIN(TO_DATE(\"txn_date\")) AS start_date,\n        MAX(TO_DATE(\"txn_date\")) AS end_date\n    FROM \"BANK_SALES_TRADING\".\"BANK_SALES_TRADING\".\"CUSTOMER_TRANSACTIONS\"\n    GROUP BY \"customer_id\"\n),\nall_days AS (\n    SELECT \n        cdr.\"customer_id\",\n        DATEADD(day, seq.seq, cdr.start_date) AS date_day\n    FROM customer_date_range cdr\n    CROSS JOIN (\n        SELECT ROW_NUMBER() OVER (ORDER BY NULL) - 1 AS seq\n        FROM TABLE(GENERATOR(ROWCOUNT => 1000))\n    ) seq\n    WHERE DATEADD(day, seq.seq, cdr.start_date) <= cdr.end_date\n),\ndaily_transactions AS (\n    SELECT \n        \"customer_id\",\n        TO_DATE(\"txn_date\") AS txn_date,\n        SUM(CASE \n            WHEN \"txn_type\" = 'deposit' THEN \"txn_amount\"\n            WHEN \"txn_type\" = 'purchase' THEN -\"txn_amount\"\n            WHEN \"txn_type\" = 'withdrawal' THEN -\"txn_amount\"\n            ELSE 0 \n        END) AS daily_net_change\n    FROM \"BANK_SALES_TRADING\".\"BANK_SALES_TRADING\".\"CUSTOMER_TRANSACTIONS\"\n    GROUP BY \"customer_id\", TO_DATE(\"txn_date\")\n),\ndaily_balances AS (\n    SELECT \n        ad.\"customer_id\",\n        ad.date_day,\n        COALESCE(dt.daily_net_change, 0) AS daily_change,\n        SUM(COALESCE(dt.daily_net_change, 0)) OVER (\n            PARTITION BY ad.\"customer_id\" \n            ORDER BY ad.date_day \n            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n        ) AS running_balance\n    FROM all_days ad\n    LEFT JOIN daily_transactions dt \n        ON ad.\"customer_id\" = dt.\"customer_id\" \n        AND ad.date_day = dt.txn_date\n),\nnon_negative_balances AS (\n    SELECT \n        \"customer_id\",\n        date_day,\n        GREATEST(running_balance, 0) AS daily_balance,\n        DATE_TRUNC('month', date_day) AS month_start\n    FROM daily_balances\n),\nmonthly_max_balances AS (\n    SELECT \n        \"customer_id\",\n        month_start,\n        MAX(daily_balance) AS max_daily_balance\n    FROM non_negative_balances\n    GROUP BY \"customer_id\", month_start\n)\nSELECT \n    month_start,\n    SUM(max_daily_balance) AS monthly_total_max_balance\nFROM monthly_max_balances\nGROUP BY month_start\nORDER BY month_start;"
    },
    {
        "instance_id":"sf_local075",
        "instruction":"Can you provide a breakdown of how many times each product was viewed, how many times they were added to the shopping cart, and how many times they were left in the cart without being purchased? Also, give me the count of actual purchases for each product. Ensure that products with a page id in (1, 2, 12, 13) are filtered out.",
        "db_id":"BANK_SALES_TRADING",
        "external_knowledge":null,
        "sql":"WITH \"product_events\" AS (\n  SELECT\n    p.\"product_id\",\n    p.\"page_name\",\n    p.\"product_category\",\n    e.\"visit_id\",\n    e.\"event_type\",\n    e.\"sequence_number\"\n  FROM \"BANK_SALES_TRADING\".\"BANK_SALES_TRADING\".\"SHOPPING_CART_EVENTS\" e\n  JOIN \"BANK_SALES_TRADING\".\"BANK_SALES_TRADING\".\"SHOPPING_CART_PAGE_HIERARCHY\" p\n    ON e.\"page_id\" = p.\"page_id\"\n  WHERE p.\"product_id\" IS NOT NULL\n    AND p.\"page_id\" NOT IN (1, 2, 12, 13)\n),\n\"views_agg\" AS (\n  SELECT\n    \"product_id\",\n    \"page_name\",\n    \"product_category\",\n    COUNT(*) AS \"total_views\"\n  FROM \"product_events\"\n  WHERE \"event_type\" = 1\n  GROUP BY \"product_id\", \"page_name\", \"product_category\"\n),\n\"adds\" AS (\n  SELECT\n    \"product_id\",\n    \"page_name\",\n    \"product_category\",\n    \"visit_id\",\n    \"sequence_number\" AS \"add_seq\"\n  FROM \"product_events\"\n  WHERE \"event_type\" = 2\n),\n\"purchases\" AS (\n  SELECT\n    \"visit_id\",\n    \"sequence_number\" AS \"purchase_seq\"\n  FROM \"BANK_SALES_TRADING\".\"BANK_SALES_TRADING\".\"SHOPPING_CART_EVENTS\"\n  WHERE \"event_type\" = 3\n),\n\"add_outcomes\" AS (\n  SELECT\n    a.\"product_id\",\n    a.\"page_name\",\n    a.\"product_category\",\n    a.\"visit_id\",\n    a.\"add_seq\",\n    CASE WHEN EXISTS (\n      SELECT 1 FROM \"purchases\" p\n      WHERE p.\"visit_id\" = a.\"visit_id\"\n        AND p.\"purchase_seq\" > a.\"add_seq\"\n    ) THEN 1 ELSE 0 END AS \"purchased_flag\"\n  FROM \"adds\" a\n),\n\"add_agg\" AS (\n  SELECT\n    \"product_id\",\n    \"page_name\",\n    \"product_category\",\n    COUNT(*) AS \"total_adds_to_cart\",\n    SUM(\"purchased_flag\") AS \"total_purchases\",\n    COUNT(*) - SUM(\"purchased_flag\") AS \"left_in_cart_without_purchase\"\n  FROM \"add_outcomes\"\n  GROUP BY \"product_id\", \"page_name\", \"product_category\"\n)\nSELECT\n  v.\"product_id\",\n  v.\"page_name\",\n  v.\"product_category\",\n  v.\"total_views\",\n  COALESCE(a.\"total_adds_to_cart\", 0) AS \"total_adds_to_cart\",\n  COALESCE(a.\"left_in_cart_without_purchase\", 0) AS \"left_in_cart_without_purchase\",\n  COALESCE(a.\"total_purchases\", 0) AS \"total_purchases\"\nFROM \"views_agg\" v\nLEFT JOIN \"add_agg\" a\n  ON v.\"product_id\" = a.\"product_id\"\n AND v.\"page_name\" = a.\"page_name\"\n AND v.\"product_category\" = a.\"product_category\"\nORDER BY v.\"product_id\";"
    },
    {
        "instance_id":"sf_local157",
        "instruction":"Using the \"bitcoin_prices\" table, please calculate the daily percentage change in trading volume for each ticker from August 1 to August 10, 2021, ensuring that any volume ending in \"K\" or \"M\" is accurately converted to thousands or millions, any \"-\" volume is treated as zero, only non-zero volumes are used to determine the previous day's volume, and the results are ordered by ticker and date.",
        "db_id":"BANK_SALES_TRADING",
        "external_knowledge":null,
        "sql":"WITH \"parsed_volume\" AS (\n  SELECT\n    \"ticker\",\n    TO_DATE(\"market_date\", 'DD-MM-YYYY') AS \"market_date\",\n    CASE\n      WHEN \"volume\" = '-' THEN 0\n      WHEN RIGHT(\"volume\", 1) = 'K' THEN CAST(REPLACE(\"volume\", 'K', '') AS FLOAT) * 1000\n      WHEN RIGHT(\"volume\", 1) = 'M' THEN CAST(REPLACE(\"volume\", 'M', '') AS FLOAT) * 1000000\n      ELSE CAST(\"volume\" AS FLOAT)\n    END AS \"volume_cleaned\"\n  FROM \"BANK_SALES_TRADING\".\"BANK_SALES_TRADING\".\"BITCOIN_PRICES\"\n  WHERE TO_DATE(\"market_date\", 'DD-MM-YYYY') BETWEEN '2021-08-01' AND '2021-08-10'\n),\n\"lagged_volume\" AS (\n  SELECT\n    \"ticker\",\n    \"market_date\",\n    \"volume_cleaned\",\n    LAG(IFF(\"volume_cleaned\" = 0, NULL, \"volume_cleaned\"), 1) IGNORE NULLS OVER (PARTITION BY \"ticker\" ORDER BY \"market_date\") AS \"previous_day_volume\"\n  FROM \"parsed_volume\"\n)\nSELECT\n  \"ticker\",\n  \"market_date\",\n  (\"volume_cleaned\" - \"previous_day_volume\") \/ \"previous_day_volume\" * 100 AS \"volume_percentage_change\"\nFROM \"lagged_volume\"\nORDER BY \"ticker\", \"market_date\";"
    },
    {
        "instance_id":"sf_local194",
        "instruction":"Please provide a list of the top three revenue-generating films for each actor, along with the average revenue per actor in those films, calculated by dividing the total film revenue equally among the actors for each film.",
        "db_id":"SQLITE_SAKILA",
        "external_knowledge":null,
        "sql":"-- Assumptions \/ Reasoning:\n-- 1. \"Revenue\" is interpreted as the total rental payment amount for a film (as in the probes that joined PAYMENT).\n-- 2. \"Top three revenue-generating films\" for an actor are determined by the film\u2019s total revenue (not the actor\u2019s share).\n-- 3. The actor\u2019s share in a film is the film\u2019s total revenue divided equally by the number of actors in that film.\n-- 4. For each actor we output up to three rows (one per film) plus, on every row, the average of the actor-share across those top films.\n-- 5. Actors with fewer than three films will return whatever number (<3) they have.\n\nWITH \"film_revenue\" AS (\n    -- total revenue per film\n    SELECT\n        f.\"film_id\",\n        f.\"title\"                       AS \"film_title\",\n        SUM(p.\"amount\")                 AS \"total_revenue\"\n    FROM \"SQLITE_SAKILA\".\"SQLITE_SAKILA\".\"FILM\"       f\n    JOIN \"SQLITE_SAKILA\".\"SQLITE_SAKILA\".\"INVENTORY\"  i  ON f.\"film_id\" = i.\"film_id\"\n    JOIN \"SQLITE_SAKILA\".\"SQLITE_SAKILA\".\"RENTAL\"     r  ON i.\"inventory_id\" = r.\"inventory_id\"\n    JOIN \"SQLITE_SAKILA\".\"SQLITE_SAKILA\".\"PAYMENT\"    p  ON r.\"rental_id\"   = p.\"rental_id\"\n    GROUP BY f.\"film_id\", f.\"title\"\n),\n\"film_actor_cnt\" AS (\n    -- number of actors in each film\n    SELECT\n        fa.\"film_id\",\n        COUNT(DISTINCT fa.\"actor_id\")   AS \"actor_count\"\n    FROM \"SQLITE_SAKILA\".\"SQLITE_SAKILA\".\"FILM_ACTOR\" fa\n    GROUP BY fa.\"film_id\"\n),\n\"film_stats\" AS (\n    -- combine revenue and actor count, compute per-actor share\n    SELECT\n        fr.\"film_id\",\n        fr.\"film_title\",\n        fr.\"total_revenue\",\n        fac.\"actor_count\",\n        fr.\"total_revenue\" \/ fac.\"actor_count\" AS \"revenue_per_actor\"\n    FROM \"film_revenue\"   fr\n    JOIN \"film_actor_cnt\" fac ON fr.\"film_id\" = fac.\"film_id\"\n),\n\"actor_film\" AS (\n    -- link each actor to films with their share, rank films by total revenue within each actor\n    SELECT\n        a.\"actor_id\",\n        a.\"first_name\",\n        a.\"last_name\",\n        fs.\"film_id\",\n        fs.\"film_title\",\n        fs.\"total_revenue\",\n        fs.\"revenue_per_actor\",\n        ROW_NUMBER() OVER (\n            PARTITION BY a.\"actor_id\" \n            ORDER BY fs.\"total_revenue\" DESC, fs.\"film_id\" ASC) AS \"film_rank\"\n    FROM \"SQLITE_SAKILA\".\"SQLITE_SAKILA\".\"ACTOR\"      a\n    JOIN \"SQLITE_SAKILA\".\"SQLITE_SAKILA\".\"FILM_ACTOR\" fa ON a.\"actor_id\" = fa.\"actor_id\"\n    JOIN \"film_stats\"                           fs ON fa.\"film_id\" = fs.\"film_id\"\n),\n\"top3\" AS (\n    -- keep only the top 3 films per actor\n    SELECT *\n    FROM \"actor_film\"\n    WHERE \"film_rank\" <= 3\n)\nSELECT\n    \"actor_id\",\n    \"first_name\",\n    \"last_name\",\n    \"film_title\",\n    \"total_revenue\",\n    \"revenue_per_actor\"                     AS \"actor_share_in_film\",\n    AVG(\"revenue_per_actor\") OVER (\n        PARTITION BY \"actor_id\"\n    )                                        AS \"avg_revenue_per_actor_top3\"\nFROM \"top3\"\nORDER BY \"actor_id\", \"film_rank\";"
    },
    {
        "instance_id":"sf_local195",
        "instruction":"Please find out how widespread the appeal of our top five actors is. What percentage of our customers have rented films featuring these actors?",
        "db_id":"SQLITE_SAKILA",
        "external_knowledge":null,
        "sql":"WITH TopActors AS (\n    SELECT\n        \"actor_id\"\n    FROM \"SQLITE_SAKILA\".\"SQLITE_SAKILA\".\"FILM_ACTOR\"\n    GROUP BY\n        \"actor_id\"\n    ORDER BY\n        COUNT(\"film_id\") DESC\n    LIMIT 5\n),\nCustomersWithTopActorFilms AS (\n    SELECT DISTINCT\n        R.\"customer_id\"\n    FROM \"SQLITE_SAKILA\".\"SQLITE_SAKILA\".\"RENTAL\" AS R\n    JOIN \"SQLITE_SAKILA\".\"SQLITE_SAKILA\".\"INVENTORY\" AS I\n        ON R.\"inventory_id\" = I.\"inventory_id\"\n    JOIN \"SQLITE_SAKILA\".\"SQLITE_SAKILA\".\"FILM_ACTOR\" AS FA\n        ON I.\"film_id\" = FA.\"film_id\"\n    WHERE\n        FA.\"actor_id\" IN (SELECT \"actor_id\" FROM TopActors)\n)\nSELECT\n    (\n        SELECT\n            COUNT(\"customer_id\")\n        FROM CustomersWithTopActorFilms\n    ) * 100.0 \/ (\n        SELECT\n            COUNT(\"customer_id\")\n        FROM \"SQLITE_SAKILA\".\"SQLITE_SAKILA\".\"CUSTOMER\"\n    ) AS PercentageOfCustomers;"
    },
    {
        "instance_id":"sf_local199",
        "instruction":"Can you identify the year and month with the highest rental orders created by the store's staff for each store? Please list the store ID, the year, the month, and the total rentals for those dates.",
        "db_id":"SQLITE_SAKILA",
        "external_knowledge":null,
        "sql":"WITH rentals_with_store AS (\n  SELECT\n    s.\"store_id\" AS \"store_id\",\n    TRY_TO_TIMESTAMP(r.\"rental_date\") AS \"rental_ts\"\n  FROM \"SQLITE_SAKILA\".\"SQLITE_SAKILA\".\"RENTAL\" r\n  JOIN \"SQLITE_SAKILA\".\"SQLITE_SAKILA\".\"STAFF\" s\n    ON r.\"staff_id\" = s.\"staff_id\"\n  WHERE TRY_TO_TIMESTAMP(r.\"rental_date\") IS NOT NULL\n),\nmonthly_counts AS (\n  SELECT\n    \"store_id\",\n    EXTRACT(YEAR FROM \"rental_ts\") AS \"year\",\n    EXTRACT(MONTH FROM \"rental_ts\") AS \"month\",\n    COUNT(*) AS \"total_rentals\"\n  FROM rentals_with_store\n  GROUP BY \"store_id\", \"year\", \"month\"\n)\nSELECT\n  \"store_id\",\n  \"year\",\n  \"month\",\n  \"total_rentals\"\nFROM monthly_counts\nQUALIFY DENSE_RANK() OVER (\n  PARTITION BY \"store_id\"\n  ORDER BY \"total_rentals\" DESC\n) = 1\nORDER BY \"store_id\", \"year\", \"month\""
    },
    {
        "instance_id":"sf_local209",
        "instruction":"In the dataset of orders joined with store information, which store has the highest total number of orders, and among that store\u2019s orders, what is the ratio of orders that appear in the deliveries table with a 'DELIVERED' status to the total orders for that store?",
        "db_id":"DELIVERY_CENTER",
        "external_knowledge":null,
        "sql":"WITH \"top_store\" AS (\n    SELECT\n        o.\"store_id\"\n    FROM \"DELIVERY_CENTER\".\"DELIVERY_CENTER\".\"ORDERS\" o\n    GROUP BY o.\"store_id\"\n    ORDER BY COUNT(*) DESC, o.\"store_id\"\n    FETCH FIRST 1 ROW ONLY\n)\nSELECT\n    ts.\"store_id\",\n    s.\"store_name\",\n    COUNT(*) AS \"total_orders\",\n    SUM(\n        CASE\n            WHEN EXISTS (\n                SELECT 1\n                FROM \"DELIVERY_CENTER\".\"DELIVERY_CENTER\".\"DELIVERIES\" d\n                WHERE d.\"delivery_order_id\" = o.\"delivery_order_id\"\n                  AND d.\"delivery_status\" = 'DELIVERED'\n            ) THEN 1\n            ELSE 0\n        END\n    ) AS \"delivered_orders\",\n    SUM(\n        CASE\n            WHEN EXISTS (\n                SELECT 1\n                FROM \"DELIVERY_CENTER\".\"DELIVERY_CENTER\".\"DELIVERIES\" d\n                WHERE d.\"delivery_order_id\" = o.\"delivery_order_id\"\n                  AND d.\"delivery_status\" = 'DELIVERED'\n            ) THEN 1\n            ELSE 0\n        END\n    )::FLOAT \/ COUNT(*) AS \"delivered_ratio\"\nFROM \"DELIVERY_CENTER\".\"DELIVERY_CENTER\".\"ORDERS\" o\nJOIN \"top_store\" ts\n    ON o.\"store_id\" = ts.\"store_id\"\nJOIN \"DELIVERY_CENTER\".\"DELIVERY_CENTER\".\"STORES\" s\n    ON s.\"store_id\" = ts.\"store_id\"\nGROUP BY\n    ts.\"store_id\",\n    s.\"store_name\";"
    },
    {
        "instance_id":"sf_local210",
        "instruction":"Can you identify the hubs that saw more than a 20% increase in finished orders from February to March?",
        "db_id":"DELIVERY_CENTER",
        "external_knowledge":null,
        "sql":"WITH completed_orders AS (\n  SELECT\n    o.\"order_id\",\n    o.\"store_id\",\n    o.\"order_created_month\",\n    o.\"order_created_year\"\n  FROM DELIVERY_CENTER.DELIVERY_CENTER.ORDERS o\n  WHERE o.\"order_created_year\" = 2021\n    AND o.\"order_created_month\" IN (2,3)\n    AND UPPER(TRIM(o.\"order_status\")) = 'FINISHED'\n)\nSELECT\n  h.\"hub_id\" AS \"hub_id\",\n  h.\"hub_name\" AS \"hub_name\",\n  CAST(SUM(CASE WHEN c.\"order_created_month\" = 2 THEN 1 ELSE 0 END) AS INTEGER) AS \"orders_february\",\n  CAST(SUM(CASE WHEN c.\"order_created_month\" = 3 THEN 1 ELSE 0 END) AS INTEGER) AS \"orders_march\",\n  (\n    CAST(SUM(CASE WHEN c.\"order_created_month\" = 3 THEN 1 ELSE 0 END) AS FLOAT)\n    - CAST(SUM(CASE WHEN c.\"order_created_month\" = 2 THEN 1 ELSE 0 END) AS FLOAT)\n  ) \/ NULLIF(CAST(SUM(CASE WHEN c.\"order_created_month\" = 2 THEN 1 ELSE 0 END) AS FLOAT), 0) AS \"pct_increase\"\nFROM completed_orders c\nJOIN DELIVERY_CENTER.DELIVERY_CENTER.STORES s\n  ON c.\"store_id\" = s.\"store_id\"\nJOIN DELIVERY_CENTER.DELIVERY_CENTER.HUBS h\n  ON s.\"hub_id\" = h.\"hub_id\"\nGROUP BY h.\"hub_id\", h.\"hub_name\"\nHAVING\n  SUM(CASE WHEN c.\"order_created_month\" = 2 THEN 1 ELSE 0 END) > 0\n  AND (\n    (\n      CAST(SUM(CASE WHEN c.\"order_created_month\" = 3 THEN 1 ELSE 0 END) AS FLOAT)\n      - CAST(SUM(CASE WHEN c.\"order_created_month\" = 2 THEN 1 ELSE 0 END) AS FLOAT)\n    ) \/ NULLIF(CAST(SUM(CASE WHEN c.\"order_created_month\" = 2 THEN 1 ELSE 0 END) AS FLOAT), 0)\n  ) > 0.2\nORDER BY \"pct_increase\" DESC, \"hub_id\" ASC;"
    },
    {
        "instance_id":"sf_local218",
        "instruction":"Can you calculate the median from the highest season goals of each team?",
        "db_id":"EU_SOCCER",
        "external_knowledge":null,
        "sql":"WITH team_goals_by_season AS (\n  -- Get goals scored by each team in each season (as home team)\n  SELECT \n    \"home_team_api_id\" AS \"team_api_id\",\n    \"season\",\n    SUM(\"home_team_goal\") AS \"goals\"\n  FROM \"EU_SOCCER\".\"EU_SOCCER\".\"MATCH\"\n  GROUP BY \"home_team_api_id\", \"season\"\n  \n  UNION ALL\n  \n  -- Get goals scored by each team in each season (as away team)\n  SELECT \n    \"away_team_api_id\" AS \"team_api_id\",\n    \"season\",\n    SUM(\"away_team_goal\") AS \"goals\"\n  FROM \"EU_SOCCER\".\"EU_SOCCER\".\"MATCH\"\n  GROUP BY \"away_team_api_id\", \"season\"\n),\ntotal_goals_per_team_season AS (\n  -- Combine home and away goals for each team in each season\n  SELECT \n    \"team_api_id\",\n    \"season\",\n    SUM(\"goals\") AS \"total_goals\"\n  FROM team_goals_by_season\n  GROUP BY \"team_api_id\", \"season\"\n),\nmax_goals_per_team AS (\n  -- Find the maximum goals for each team across all seasons\n  SELECT \n    \"team_api_id\",\n    MAX(\"total_goals\") AS \"max_season_goals\"\n  FROM total_goals_per_team_season\n  GROUP BY \"team_api_id\"\n)\n-- Calculate the median of the maximum season goals\nSELECT \n  MEDIAN(\"max_season_goals\") AS \"median_of_highest_season_goals\"\nFROM max_goals_per_team;"
    },
    {
        "instance_id":"sf_local244",
        "instruction":"Calculate the duration of each track, classify them as short, medium, or long, output the minimum and maximum time for each kind (in minutes) and the total revenue for each category, group by the category.",
        "db_id":"MUSIC",
        "external_knowledge":"music_length_type.md",
        "sql":"WITH stats AS (\n    SELECT\n        MIN(\"Milliseconds\") \/ 60000.0 AS min_length,\n        AVG(\"Milliseconds\") \/ 60000.0 AS avg_length,\n        MAX(\"Milliseconds\") \/ 60000.0 AS max_length\n    FROM \"MUSIC\".\"MUSIC\".\"TRACK\"\n),\ntrack_lengths AS (\n    SELECT\n        \"TrackId\",\n        \"Milliseconds\" \/ 60000.0 AS length_minutes\n    FROM \"MUSIC\".\"MUSIC\".\"TRACK\"\n),\nrevenue AS (\n    SELECT\n        \"TrackId\",\n        SUM(\"UnitPrice\" * \"Quantity\") AS track_revenue\n    FROM \"MUSIC\".\"MUSIC\".\"INVOICELINE\"\n    GROUP BY \"TrackId\"\n),\ntrack_data AS (\n    SELECT\n        tl.\"TrackId\",\n        tl.length_minutes,\n        COALESCE(rv.track_revenue, 0) AS track_revenue,\n        st.min_length,\n        st.avg_length,\n        st.max_length,\n        (st.min_length + st.avg_length) \/ 2 AS lower_midpoint,\n        (st.avg_length + st.max_length) \/ 2 AS upper_midpoint\n    FROM track_lengths tl\n    CROSS JOIN stats st\n    LEFT JOIN revenue rv ON tl.\"TrackId\" = rv.\"TrackId\"\n)\nSELECT\n    CASE\n        WHEN length_minutes < lower_midpoint THEN 'short'\n        WHEN length_minutes < upper_midpoint THEN 'medium'\n        ELSE 'long'\n    END AS category,\n    MIN(length_minutes) AS min_minutes,\n    MAX(length_minutes) AS max_minutes,\n    SUM(track_revenue) AS total_revenue\nFROM track_data\nGROUP BY category\nORDER BY category"
    },
    {
        "instance_id":"sf_local263",
        "instruction":"Identify the L1_model associated with each model (specified by name and version) that occurs most frequently for each status ('strong' or 'soft'), along with the number of times it occurs. A model has a 'strong' status if, for any of its steps, the maximum test score among non-'Stack' models is less than the 'Stack' model's test score. It has a 'soft' status if the maximum test score among non-'Stack' models equals the 'Stack' model's test score. Count how many times each L1_model is associated with a 'strong' or 'soft' status across all models, and determine which L1_model has the highest occurrence for each status.",
        "db_id":"STACKING",
        "external_knowledge":null,
        "sql":"WITH step_comp AS (\n  SELECT\n    \"name\",\n    \"version\",\n    \"step\",\n    MAX(CASE WHEN \"model\" = 'Stack' THEN \"test_score\" END) AS \"stack_score\",\n    MAX(CASE WHEN \"model\" != 'Stack' THEN \"test_score\" END) AS \"max_non_stack\"\n  FROM \"STACKING\".\"STACKING\".\"MODEL_SCORE\"\n  GROUP BY \"name\", \"version\", \"step\"\n),\nmodel_status AS (\n  SELECT\n    \"name\",\n    \"version\",\n    CASE\n      WHEN MAX(CASE WHEN \"stack_score\" > \"max_non_stack\" THEN 1 ELSE 0 END) = 1 THEN 'strong'\n      WHEN MAX(CASE WHEN \"stack_score\" = \"max_non_stack\" THEN 1 ELSE 0 END) = 1 THEN 'soft'\n      ELSE NULL\n    END AS \"status\"\n  FROM step_comp\n  WHERE \"stack_score\" IS NOT NULL AND \"max_non_stack\" IS NOT NULL\n  GROUP BY \"name\", \"version\"\n),\nl1_per_model AS (\n  SELECT\n    \"name\",\n    \"version\",\n    \"L1_model\"\n  FROM (\n    SELECT\n      \"name\",\n      \"version\",\n      \"L1_model\",\n      COUNT(DISTINCT \"step\") AS \"cnt\",\n      ROW_NUMBER() OVER (PARTITION BY \"name\", \"version\" ORDER BY COUNT(DISTINCT \"step\") DESC, \"L1_model\" ASC) AS \"rn\"\n    FROM \"STACKING\".\"STACKING\".\"MODEL\"\n    WHERE \"L1_model\" IS NOT NULL\n    GROUP BY \"name\", \"version\", \"L1_model\"\n  )\n  WHERE \"rn\" = 1\n),\nstatus_l1_counts AS (\n  SELECT\n    ms.\"status\",\n    lpm.\"L1_model\",\n    COUNT(*) AS \"occurrence_count\"\n  FROM model_status ms\n  JOIN l1_per_model lpm\n    ON ms.\"name\" = lpm.\"name\"\n   AND ms.\"version\" = lpm.\"version\"\n  WHERE ms.\"status\" IN ('strong', 'soft')\n  GROUP BY ms.\"status\", lpm.\"L1_model\"\n),\ntop_l1_per_status AS (\n  SELECT\n    \"status\",\n    \"L1_model\",\n    \"occurrence_count\",\n    RANK() OVER (PARTITION BY \"status\" ORDER BY \"occurrence_count\" DESC) AS \"rnk\"\n  FROM status_l1_counts\n)\nSELECT\n  \"status\",\n  \"L1_model\",\n  \"occurrence_count\"\nFROM top_l1_per_status\nWHERE \"rnk\" = 1\nORDER BY \"status\", \"L1_model\""
    },
    {
        "instance_id":"sf_local269",
        "instruction":"What is the average total quantity across all final packaging combinations, considering only the leaf-level items within each combination after fully expanding any nested packaging relationships?",
        "db_id":"ORACLE_SQL",
        "external_knowledge":null,
        "sql":"WITH RECURSIVE exploded (root_id, child_id, qty) AS (\n    SELECT\n        pr.\"packaging_id\",\n        pr.\"contains_id\",\n        pr.\"qty\"\n    FROM \"ORACLE_SQL\".\"ORACLE_SQL\".\"PACKAGING_RELATIONS\" AS pr\n    WHERE pr.\"packaging_id\" NOT IN (\n        SELECT \"contains_id\"\n        FROM \"ORACLE_SQL\".\"ORACLE_SQL\".\"PACKAGING_RELATIONS\"\n        WHERE \"contains_id\" IS NOT NULL\n    )\n    UNION ALL\n    SELECT\n        exploded.root_id,\n        pr2.\"contains_id\",\n        exploded.qty * pr2.\"qty\"\n    FROM exploded\n    JOIN \"ORACLE_SQL\".\"ORACLE_SQL\".\"PACKAGING_RELATIONS\" AS pr2\n        ON pr2.\"packaging_id\" = exploded.child_id\n),\nleaf_nodes AS (\n    SELECT DISTINCT pr.\"contains_id\" AS leaf_id\n    FROM \"ORACLE_SQL\".\"ORACLE_SQL\".\"PACKAGING_RELATIONS\" AS pr\n    WHERE pr.\"contains_id\" NOT IN (\n        SELECT \"packaging_id\"\n        FROM \"ORACLE_SQL\".\"ORACLE_SQL\".\"PACKAGING_RELATIONS\"\n        WHERE \"packaging_id\" IS NOT NULL\n    )\n),\nroot_totals AS (\n    SELECT\n        exploded.root_id,\n        SUM(exploded.qty) AS total_leaf_qty\n    FROM exploded\n    JOIN leaf_nodes\n        ON leaf_nodes.leaf_id = exploded.child_id\n    GROUP BY exploded.root_id\n)\nSELECT AVG(total_leaf_qty) AS average_total_quantity\nFROM root_totals;"
    },
    {
        "instance_id":"sf_local283",
        "instruction":"Analyze the soccer match dataset to determine the champion team for each season across all countries and leagues, awarding 3 points for every win, 1 point for every tie, and 0 points for every loss. For each season, return the champion\u2019s team name, the league, the country, and the total points accumulated.",
        "db_id":"EU_SOCCER",
        "external_knowledge":null,
        "sql":"WITH per_team_match AS (\n  SELECT\n    m.\"season\",\n    m.\"league_id\",\n    m.\"home_team_api_id\" AS \"team_api_id\",\n    m.\"home_team_goal\" AS \"gf\",\n    m.\"away_team_goal\" AS \"ga\",\n    CASE\n      WHEN m.\"home_team_goal\" > m.\"away_team_goal\" THEN 3\n      WHEN m.\"home_team_goal\" = m.\"away_team_goal\" THEN 1\n      ELSE 0\n    END AS \"points\"\n  FROM \"EU_SOCCER\".\"EU_SOCCER\".\"MATCH\" AS m\n  WHERE m.\"home_team_goal\" IS NOT NULL AND m.\"away_team_goal\" IS NOT NULL\n  UNION ALL\n  SELECT\n    m.\"season\",\n    m.\"league_id\",\n    m.\"away_team_api_id\" AS \"team_api_id\",\n    m.\"away_team_goal\" AS \"gf\",\n    m.\"home_team_goal\" AS \"ga\",\n    CASE\n      WHEN m.\"away_team_goal\" > m.\"home_team_goal\" THEN 3\n      WHEN m.\"away_team_goal\" = m.\"home_team_goal\" THEN 1\n      ELSE 0\n    END AS \"points\"\n  FROM \"EU_SOCCER\".\"EU_SOCCER\".\"MATCH\" AS m\n  WHERE m.\"home_team_goal\" IS NOT NULL AND m.\"away_team_goal\" IS NOT NULL\n),\nteam_season_points AS (\n  SELECT\n    p.\"season\",\n    p.\"league_id\",\n    p.\"team_api_id\",\n    SUM(p.\"points\") AS \"total_points\",\n    SUM(p.\"gf\") AS \"goals_for\",\n    SUM(p.\"ga\") AS \"goals_against\"\n  FROM per_team_match p\n  GROUP BY p.\"season\", p.\"league_id\", p.\"team_api_id\"\n),\nranked_champions AS (\n  SELECT\n    tsp.\"season\",\n    tsp.\"league_id\",\n    tsp.\"team_api_id\",\n    tsp.\"total_points\",\n    tsp.\"goals_for\",\n    tsp.\"goals_against\",\n    ROW_NUMBER() OVER (\n      PARTITION BY tsp.\"season\", tsp.\"league_id\"\n      ORDER BY tsp.\"total_points\" DESC,\n               (tsp.\"goals_for\" - tsp.\"goals_against\") DESC,\n               tsp.\"goals_for\" DESC,\n               t.\"team_long_name\" ASC\n    ) AS \"rn\"\n  FROM team_season_points tsp\n  JOIN \"EU_SOCCER\".\"EU_SOCCER\".\"TEAM\" t\n    ON tsp.\"team_api_id\" = t.\"team_api_id\"\n)\nSELECT\n  rc.\"season\",\n  t.\"team_long_name\" AS \"champion_team\",\n  l.\"name\" AS \"league\",\n  c.\"name\" AS \"country\",\n  rc.\"total_points\"\nFROM ranked_champions rc\nJOIN \"EU_SOCCER\".\"EU_SOCCER\".\"TEAM\" t\n  ON rc.\"team_api_id\" = t.\"team_api_id\"\nJOIN \"EU_SOCCER\".\"EU_SOCCER\".\"LEAGUE\" l\n  ON rc.\"league_id\" = l.\"id\"\nJOIN \"EU_SOCCER\".\"EU_SOCCER\".\"COUNTRY\" c\n  ON l.\"country_id\" = c.\"id\"\nWHERE rc.\"rn\" = 1\nORDER BY rc.\"season\", c.\"name\", l.\"name\", \"champion_team\";"
    },
    {
        "instance_id":"sf_local285",
        "instruction":"For veg whsle data, can you analyze our financial performance over the years 2020 to 2023? I need insights into the average wholesale price, maximum wholesale price, minimum wholesale price, wholesale price difference, total wholesale price, total selling price, average loss rate, total loss, and profit for each category within each year. Round all calculated values to two decimal places.",
        "db_id":"BANK_SALES_TRADING",
        "external_knowledge":null,
        "sql":"SELECT \n    EXTRACT(YEAR FROM TO_DATE(w.\"whsle_date\")) as year,\n    c.\"category_name\",\n    ROUND(AVG(w.\"whsle_px_rmb-kg\"), 2) as avg_wholesale_price,\n    ROUND(MAX(w.\"whsle_px_rmb-kg\"), 2) as max_wholesale_price,\n    ROUND(MIN(w.\"whsle_px_rmb-kg\"), 2) as min_wholesale_price,\n    ROUND(MAX(w.\"whsle_px_rmb-kg\") - MIN(w.\"whsle_px_rmb-kg\"), 2) as wholesale_price_difference,\n    ROUND(SUM(w.\"whsle_px_rmb-kg\" * t.\"qty_sold(kg)\"), 2) as total_wholesale_price,\n    ROUND(SUM(t.\"unit_selling_px_rmb\/kg\" * t.\"qty_sold(kg)\"), 2) as total_selling_price,\n    ROUND(AVG(l.\"loss_rate_%\"), 2) as avg_loss_rate,\n    ROUND(SUM(w.\"whsle_px_rmb-kg\" * t.\"qty_sold(kg)\" * l.\"loss_rate_%\" \/ 100), 2) as total_loss,\n    ROUND(SUM(t.\"unit_selling_px_rmb\/kg\" * t.\"qty_sold(kg)\") - \n          SUM(w.\"whsle_px_rmb-kg\" * t.\"qty_sold(kg)\") - \n          SUM(w.\"whsle_px_rmb-kg\" * t.\"qty_sold(kg)\" * l.\"loss_rate_%\" \/ 100), 2) as profit\nFROM \"BANK_SALES_TRADING\".\"BANK_SALES_TRADING\".\"VEG_WHSLE_DF\" w\nJOIN \"BANK_SALES_TRADING\".\"BANK_SALES_TRADING\".\"VEG_TXN_DF\" t ON w.\"item_code\" = t.\"item_code\" AND w.\"whsle_date\" = t.\"txn_date\"\nJOIN \"BANK_SALES_TRADING\".\"BANK_SALES_TRADING\".\"VEG_CAT\" c ON w.\"item_code\" = c.\"item_code\"\nJOIN \"BANK_SALES_TRADING\".\"BANK_SALES_TRADING\".\"VEG_LOSS_RATE_DF\" l ON w.\"item_code\" = l.\"item_code\"\nWHERE EXTRACT(YEAR FROM TO_DATE(w.\"whsle_date\")) BETWEEN 2020 AND 2023\n    AND t.\"sale\/return\" = 'sale'\nGROUP BY EXTRACT(YEAR FROM TO_DATE(w.\"whsle_date\")), c.\"category_name\"\nORDER BY year, c.\"category_name\""
    },
    {
        "instance_id":"sf_local329",
        "instruction":"How many unique sessions visited the \/regist\/input page and then the \/regist\/confirm page, in that order?",
        "db_id":"LOG",
        "external_knowledge":null,
        "sql":"SELECT COUNT(DISTINCT input_sessions.\"session\") as unique_sessions\nFROM (\n    SELECT DISTINCT \"session\", \"stamp\"\n    FROM \"LOG\".\"LOG\".\"FORM_LOG\"\n    WHERE \"path\" = '\/regist\/input'\n) as input_sessions\nINNER JOIN (\n    SELECT DISTINCT \"session\", \"stamp\"\n    FROM \"LOG\".\"LOG\".\"FORM_LOG\"\n    WHERE \"path\" = '\/regist\/confirm'\n) as confirm_sessions\n    ON input_sessions.\"session\" = confirm_sessions.\"session\"\nWHERE input_sessions.\"stamp\" < confirm_sessions.\"stamp\""
    },
    {
        "instance_id":"sf_local360",
        "instruction":"For each user session in the activity log table, identify the number of events that occurred before the first '\/detail' click or '\/complete' conversion, counting only events that have a non-empty search type. Find the sessions with the minimum count of such pre-click\/pre-conversion events. If multiple sessions share this minimum count, include all of them in the results. Return each qualifying session along with the corresponding path and search type.",
        "db_id":"LOG",
        "external_knowledge":null,
        "sql":"WITH \"events\" AS (\n    SELECT\n        \"session\",\n        \"stamp\",\n        \"path\",\n        \"search_type\"\n    FROM \"LOG\".\"LOG\".\"ACTIVITY_LOG\"\n),\n\n\"first_target\" AS (\n    SELECT\n        \"session\",\n        \"path\",\n        \"search_type\",\n        \"stamp\"      AS \"target_stamp\"\n    FROM (\n        SELECT\n            \"session\",\n            \"path\",\n            \"search_type\",\n            \"stamp\",\n            ROW_NUMBER() OVER (PARTITION BY \"session\" ORDER BY \"stamp\") AS \"rn\"\n        FROM \"events\"\n        WHERE \"path\" ILIKE '%\/detail%' OR \"path\" ILIKE '%\/complete%'\n    ) t\n    WHERE \"rn\" = 1\n),\n\n\"pre_counts\" AS (\n    SELECT\n        ft.\"session\",\n        ft.\"path\"          AS \"target_path\",\n        ft.\"search_type\"   AS \"target_search_type\",\n        COUNT(e.*) AS \"pre_event_count\"\n    FROM \"first_target\" ft\n    LEFT JOIN \"events\" e      \/* LEFT JOIN so zero counts are kept *\/\n      ON  e.\"session\" = ft.\"session\"\n      AND e.\"stamp\"   < ft.\"target_stamp\"               -- strictly before\n      AND e.\"search_type\" IS NOT NULL\n      AND e.\"search_type\" <> ''\n    GROUP BY ft.\"session\", ft.\"path\", ft.\"search_type\"\n),\n\n\"min_count\" AS (\n    SELECT MIN(\"pre_event_count\") AS \"min_cnt\" FROM \"pre_counts\"\n)\n\nSELECT\n    p.\"session\",\n    p.\"target_path\"  AS \"path\",\n    p.\"target_search_type\" AS \"search_type\"\nFROM \"pre_counts\" p\nJOIN \"min_count\"  m  ON p.\"pre_event_count\" = m.\"min_cnt\"\nORDER BY p.\"session\";"
    },
    {
        "instance_id":"sf_local336",
        "instruction":"In the first five laps of the race, how many overtakes occurred in each category\u2014retirements, pit stops, start-related overtakes, and standard on-track passes?",
        "db_id":"F1",
        "external_knowledge":"f1_overtake.md",
        "sql":"WITH \"curr\" AS (\n  SELECT \"race_id\", \"lap\", \"driver_id\", \"position\"\n  FROM \"F1\".\"F1\".\"LAP_POSITIONS\"\n  WHERE \"lap_type\" = 'Race' AND \"lap\" BETWEEN 1 AND 5\n),\n\"prev_race\" AS (\n  SELECT \"race_id\", \"lap\", \"driver_id\", \"position\"\n  FROM \"F1\".\"F1\".\"LAP_POSITIONS\"\n  WHERE \"lap_type\" = 'Race' AND \"lap\" BETWEEN 1 AND 4\n),\n\"grid\" AS (\n  SELECT \"race_id\", \"driver_id\", \"grid\"\n  FROM \"F1\".\"F1\".\"RESULTS\"\n),\n\"pairs_lap1\" AS (\n  SELECT\n    c1.\"race_id\",\n    1 AS \"lap\",\n    c1.\"driver_id\" AS \"overtaker_id\",\n    c2.\"driver_id\" AS \"overtaken_id\",\n    g1.\"grid\" AS \"grid_overtaker\",\n    g2.\"grid\" AS \"grid_overtaken\",\n    c1.\"position\" AS \"curr_pos_overtaker\",\n    c2.\"position\" AS \"curr_pos_overtaken\"\n  FROM \"curr\" c1\n  JOIN \"curr\" c2\n    ON c1.\"race_id\" = c2.\"race_id\"\n   AND c1.\"lap\" = 1 AND c2.\"lap\" = 1\n   AND c1.\"driver_id\" != c2.\"driver_id\"\n  LEFT JOIN \"grid\" g1 ON g1.\"race_id\" = c1.\"race_id\" AND g1.\"driver_id\" = c1.\"driver_id\"\n  LEFT JOIN \"grid\" g2 ON g2.\"race_id\" = c2.\"race_id\" AND g2.\"driver_id\" = c2.\"driver_id\"\n  WHERE g1.\"grid\" IS NOT NULL AND g2.\"grid\" IS NOT NULL\n    AND g1.\"grid\" > g2.\"grid\"\n    AND c1.\"position\" < c2.\"position\"\n),\n\"pairs_lap2to5\" AS (\n  SELECT\n    c1.\"race_id\",\n    c1.\"lap\" AS \"lap\",\n    c1.\"driver_id\" AS \"overtaker_id\",\n    c2.\"driver_id\" AS \"overtaken_id\",\n    g1.\"grid\" AS \"grid_overtaker\",\n    g2.\"grid\" AS \"grid_overtaken\"\n  FROM \"curr\" c1\n  JOIN \"curr\" c2\n    ON c1.\"race_id\" = c2.\"race_id\"\n   AND c1.\"lap\" BETWEEN 2 AND 5\n   AND c2.\"lap\" = c1.\"lap\"\n   AND c1.\"driver_id\" != c2.\"driver_id\"\n  JOIN \"prev_race\" p1 ON p1.\"race_id\" = c1.\"race_id\" AND p1.\"lap\" = c1.\"lap\" - 1 AND p1.\"driver_id\" = c1.\"driver_id\"\n  JOIN \"prev_race\" p2 ON p2.\"race_id\" = c2.\"race_id\" AND p2.\"lap\" = c2.\"lap\" - 1 AND p2.\"driver_id\" = c2.\"driver_id\"\n  LEFT JOIN \"grid\" g1 ON g1.\"race_id\" = c1.\"race_id\" AND g1.\"driver_id\" = c1.\"driver_id\"\n  LEFT JOIN \"grid\" g2 ON g2.\"race_id\" = c2.\"race_id\" AND g2.\"driver_id\" = c2.\"driver_id\"\n  WHERE p1.\"position\" > p2.\"position\"\n    AND c1.\"position\" < c2.\"position\"\n),\n\"all_pairs\" AS (\n  SELECT \"race_id\", \"lap\", \"overtaker_id\", \"overtaken_id\", \"grid_overtaker\", \"grid_overtaken\"\n  FROM \"pairs_lap1\"\n  UNION ALL\n  SELECT \"race_id\", \"lap\", \"overtaker_id\", \"overtaken_id\", \"grid_overtaker\", \"grid_overtaken\"\n  FROM \"pairs_lap2to5\"\n),\n\"classified\" AS (\n  SELECT\n    a.\"race_id\",\n    a.\"lap\",\n    a.\"overtaker_id\",\n    a.\"overtaken_id\",\n    CASE\n      WHEN EXISTS (\n        SELECT 1 FROM \"F1\".\"F1\".\"RETIREMENTS\" r\n        WHERE r.\"race_id\" = a.\"race_id\"\n          AND r.\"driver_id\" = a.\"overtaken_id\"\n          AND r.\"lap\" = a.\"lap\"\n      ) THEN 'R'\n      WHEN EXISTS (\n        SELECT 1 FROM \"F1\".\"F1\".\"PIT_STOPS\" ps\n        WHERE ps.\"race_id\" = a.\"race_id\"\n          AND ps.\"driver_id\" = a.\"overtaken_id\"\n          AND (ps.\"lap\" = a.\"lap\" OR ps.\"lap\" = a.\"lap\" - 1)\n      ) THEN 'P'\n      WHEN a.\"lap\" = 1\n        AND a.\"grid_overtaker\" IS NOT NULL\n        AND a.\"grid_overtaken\" IS NOT NULL\n        AND abs(a.\"grid_overtaker\" - a.\"grid_overtaken\") <= 2 THEN 'S'\n      ELSE 'T'\n    END AS \"category\"\n  FROM \"all_pairs\" a\n)\nSELECT\n  CASE \"category\"\n    WHEN 'R' THEN 'Retirements'\n    WHEN 'P' THEN 'Pit Stops'\n    WHEN 'S' THEN 'Start-Related'\n    ELSE 'On-Track'\n  END AS \"category\",\n  COUNT(*) AS \"overtakes\"\nFROM \"classified\"\nGROUP BY \"category\"\nORDER BY CASE \"category\" WHEN 'Retirements' THEN 1 WHEN 'Pit Stops' THEN 2 WHEN 'Start-Related' THEN 3 ELSE 4 END;"
    },
    {
        "instance_id":"sf_local309",
        "instruction":"For each year, which driver and which constructor scored the most points? I want the full name of each driver.",
        "db_id":"F1",
        "external_knowledge":null,
        "sql":"WITH \"years\" AS (\n  SELECT DISTINCT r.\"year\"\n  FROM \"F1\".\"F1\".\"RACES\" r\n),\n\"final_races\" AS (\n  SELECT r.\"year\", r.\"race_id\"\n  FROM \"F1\".\"F1\".\"RACES\" r\n  JOIN (\n    SELECT \"year\", MAX(\"round\") AS \"max_round\"\n    FROM \"F1\".\"F1\".\"RACES\"\n    GROUP BY \"year\"\n  ) lr\n    ON r.\"year\" = lr.\"year\" AND r.\"round\" = lr.\"max_round\"\n),\n\"driver_top_standings\" AS (\n  SELECT\n    fr.\"year\",\n    ds.\"driver_id\",\n    d.\"full_name\" AS \"driver_full_name\",\n    ds.\"points\"   AS \"driver_points\"\n  FROM \"F1\".\"F1\".\"DRIVER_STANDINGS\" ds\n  JOIN \"final_races\" fr\n    ON ds.\"race_id\" = fr.\"race_id\"\n  JOIN (\n    SELECT fr.\"year\", MAX(ds.\"points\") AS \"max_points\"\n    FROM \"F1\".\"F1\".\"DRIVER_STANDINGS\" ds\n    JOIN \"final_races\" fr ON ds.\"race_id\" = fr.\"race_id\"\n    GROUP BY fr.\"year\"\n  ) md\n    ON fr.\"year\" = md.\"year\" AND ds.\"points\" = md.\"max_points\"\n  LEFT JOIN \"F1\".\"F1\".\"DRIVERS\" d\n    ON ds.\"driver_id\" = d.\"driver_id\"\n),\n\"driver_points_agg\" AS (\n  SELECT\n    r.\"year\" AS \"year\",\n    res.\"driver_id\" AS \"driver_id\",\n    SUM(COALESCE(res.\"points\", 0)) AS \"points\"\n  FROM \"F1\".\"F1\".\"RESULTS\" res\n  JOIN \"F1\".\"F1\".\"RACES\" r\n    ON res.\"race_id\" = r.\"race_id\"\n  GROUP BY r.\"year\", res.\"driver_id\"\n  UNION ALL\n  SELECT\n    r.\"year\" AS \"year\",\n    sr.\"driver_id\" AS \"driver_id\",\n    SUM(COALESCE(sr.\"points\", 0)) AS \"points\"\n  FROM \"F1\".\"F1\".\"SPRINT_RESULTS\" sr\n  JOIN \"F1\".\"F1\".\"RACES\" r\n    ON sr.\"race_id\" = r.\"race_id\"\n  GROUP BY r.\"year\", sr.\"driver_id\"\n),\n\"driver_points_summed\" AS (\n  SELECT \"year\", \"driver_id\", SUM(\"points\") AS \"points\"\n  FROM \"driver_points_agg\"\n  GROUP BY \"year\", \"driver_id\"\n),\n\"driver_top_results\" AS (\n  SELECT\n    dp.\"year\",\n    dp.\"driver_id\",\n    d.\"full_name\" AS \"driver_full_name\",\n    dp.\"points\"   AS \"driver_points\"\n  FROM \"driver_points_summed\" dp\n  JOIN (\n    SELECT \"year\", MAX(\"points\") AS \"max_points\"\n    FROM \"driver_points_summed\"\n    GROUP BY \"year\"\n  ) md\n    ON dp.\"year\" = md.\"year\" AND dp.\"points\" = md.\"max_points\"\n  LEFT JOIN \"F1\".\"F1\".\"DRIVERS\" d\n    ON dp.\"driver_id\" = d.\"driver_id\"\n),\n\"driver_top_final\" AS (\n  SELECT\n    y.\"year\",\n    COALESCE(dts.\"driver_id\", dtr.\"driver_id\")       AS \"driver_id\",\n    COALESCE(dts.\"driver_full_name\", dtr.\"driver_full_name\") AS \"driver_full_name\"\n  FROM \"years\" y\n  LEFT JOIN \"driver_top_standings\" dts\n    ON dts.\"year\" = y.\"year\"\n  LEFT JOIN \"driver_top_results\" dtr\n    ON dtr.\"year\" = y.\"year\" AND dts.\"year\" IS NULL\n),\n\"constructor_top_standings\" AS (\n  SELECT\n    fr.\"year\",\n    cs.\"constructor_id\",\n    c.\"name\" AS \"constructor_name\",\n    cs.\"points\" AS \"constructor_points\"\n  FROM \"F1\".\"F1\".\"CONSTRUCTOR_STANDINGS\" cs\n  JOIN \"final_races\" fr\n    ON cs.\"race_id\" = fr.\"race_id\"\n  JOIN (\n    SELECT fr.\"year\", MAX(cs.\"points\") AS \"max_points\"\n    FROM \"F1\".\"F1\".\"CONSTRUCTOR_STANDINGS\" cs\n    JOIN \"final_races\" fr ON cs.\"race_id\" = fr.\"race_id\"\n    GROUP BY fr.\"year\"\n  ) mc\n    ON fr.\"year\" = mc.\"year\" AND cs.\"points\" = mc.\"max_points\"\n  LEFT JOIN \"F1\".\"F1\".\"CONSTRUCTORS\" c\n    ON cs.\"constructor_id\" = c.\"constructor_id\"\n),\n\"constructor_points_agg\" AS (\n  SELECT\n    r.\"year\" AS \"year\",\n    res.\"constructor_id\" AS \"constructor_id\",\n    SUM(COALESCE(res.\"points\", 0)) AS \"points\"\n  FROM \"F1\".\"F1\".\"RESULTS\" res\n  JOIN \"F1\".\"F1\".\"RACES\" r\n    ON res.\"race_id\" = r.\"race_id\"\n  GROUP BY r.\"year\", res.\"constructor_id\"\n  UNION ALL\n  SELECT\n    r.\"year\" AS \"year\",\n    sr.\"constructor_id\" AS \"constructor_id\",\n    SUM(COALESCE(sr.\"points\", 0)) AS \"points\"\n  FROM \"F1\".\"F1\".\"SPRINT_RESULTS\" sr\n  JOIN \"F1\".\"F1\".\"RACES\" r\n    ON sr.\"race_id\" = r.\"race_id\"\n  GROUP BY r.\"year\", sr.\"constructor_id\"\n),\n\"constructor_points_summed\" AS (\n  SELECT \"year\", \"constructor_id\", SUM(\"points\") AS \"points\"\n  FROM \"constructor_points_agg\"\n  GROUP BY \"year\", \"constructor_id\"\n),\n\"constructor_top_results\" AS (\n  SELECT\n    cp.\"year\",\n    cp.\"constructor_id\",\n    c.\"name\" AS \"constructor_name\",\n    cp.\"points\" AS \"constructor_points\"\n  FROM \"constructor_points_summed\" cp\n  JOIN (\n    SELECT \"year\", MAX(\"points\") AS \"max_points\"\n    FROM \"constructor_points_summed\"\n    GROUP BY \"year\"\n  ) mc\n    ON cp.\"year\" = mc.\"year\" AND cp.\"points\" = mc.\"max_points\"\n  LEFT JOIN \"F1\".\"F1\".\"CONSTRUCTORS\" c\n    ON cp.\"constructor_id\" = c.\"constructor_id\"\n),\n\"constructor_top_final\" AS (\n  SELECT\n    y.\"year\",\n    COALESCE(cts.\"constructor_id\", ctr.\"constructor_id\") AS \"constructor_id\",\n    COALESCE(cts.\"constructor_name\", ctr.\"constructor_name\") AS \"constructor_name\"\n  FROM \"years\" y\n  LEFT JOIN \"constructor_top_standings\" cts\n    ON cts.\"year\" = y.\"year\"\n  LEFT JOIN \"constructor_top_results\" ctr\n    ON ctr.\"year\" = y.\"year\" AND cts.\"year\" IS NULL\n)\nSELECT\n  y.\"year\",\n  dtf.\"driver_full_name\",\n  ctf.\"constructor_name\"\nFROM \"years\" y\nLEFT JOIN \"driver_top_final\" dtf\n  ON dtf.\"year\" = y.\"year\"\nLEFT JOIN \"constructor_top_final\" ctf\n  ON ctf.\"year\" = y.\"year\"\nORDER BY y.\"year\" ASC;"
    },
    {
        "instance_id":"sf_local311",
        "instruction":"Which constructors had the top 3 combined points from their best driver and team, and in which years did they achieve them?",
        "db_id":"F1",
        "external_knowledge":null,
        "sql":"WITH LastRacePerYear AS (\n    SELECT \"year\", MAX(\"race_id\") AS \"last_race_id\"\n    FROM \"F1\".\"F1\".\"RACES\"\n    GROUP BY \"year\"\n),\nFinalConstructorStandings AS (\n    SELECT\n        lr.\"year\",\n        cs.\"constructor_id\",\n        cs.\"points\" AS \"constructor_points\"\n    FROM LastRacePerYear AS lr\n    JOIN \"F1\".\"F1\".\"CONSTRUCTOR_STANDINGS\" AS cs\n        ON lr.\"last_race_id\" = cs.\"race_id\"\n),\nFinalDriverStandings AS (\n    SELECT\n        lr.\"year\",\n        ds.\"driver_id\",\n        ds.\"points\" AS \"driver_points\"\n    FROM LastRacePerYear AS lr\n    JOIN \"F1\".\"F1\".\"DRIVER_STANDINGS\" AS ds\n        ON lr.\"last_race_id\" = ds.\"race_id\"\n),\nDriverPointsPerConstructorYear AS (\n    SELECT\n        r.\"year\",\n        res.\"constructor_id\",\n        res.\"driver_id\",\n        SUM(res.\"points\") AS \"points_for_constructor\"\n    FROM \"F1\".\"F1\".\"RESULTS\" AS res\n    JOIN \"F1\".\"F1\".\"RACES\" AS r ON res.\"race_id\" = r.\"race_id\"\n    GROUP BY r.\"year\", res.\"constructor_id\", res.\"driver_id\"\n),\nBestDriverPerConstructorYear AS (\n    SELECT\n        \"year\",\n        \"constructor_id\",\n        \"driver_id\"\n    FROM (\n        SELECT\n            \"year\",\n            \"constructor_id\",\n            \"driver_id\",\n            ROW_NUMBER() OVER(PARTITION BY \"year\", \"constructor_id\" ORDER BY \"points_for_constructor\" DESC, \"driver_id\" ASC) as \"rn\"\n        FROM DriverPointsPerConstructorYear\n    )\n    WHERE \"rn\" = 1\n)\nSELECT\n    c.\"name\",\n    fcs.\"year\"\nFROM FinalConstructorStandings AS fcs\nJOIN BestDriverPerConstructorYear AS bdpc\n    ON fcs.\"year\" = bdpc.\"year\" AND fcs.\"constructor_id\" = bdpc.\"constructor_id\"\nJOIN FinalDriverStandings AS fds\n    ON bdpc.\"year\" = fds.\"year\" AND bdpc.\"driver_id\" = fds.\"driver_id\"\nJOIN \"F1\".\"F1\".\"CONSTRUCTORS\" AS c\n    ON fcs.\"constructor_id\" = c.\"constructor_id\"\nORDER BY (fcs.\"constructor_points\" + fds.\"driver_points\") DESC\nLIMIT 3;"
    },
    {
        "instance_id":"sf_local354",
        "instruction":"Among Formula 1 drivers who raced during the 1950s, which drivers completed a season in that decade with the same constructor in both the first and the last race they participated in, while also taking part in at least two distinct race rounds during that season?",
        "db_id":"F1",
        "external_knowledge":null,
        "sql":"\nSELECT\n  d.\"full_name\" AS name,\n  t.\"year\" AS year,\n  c.\"name\" AS constructor_name,\n  t.num_unique_rounds AS num_unique_rounds\nFROM (\n  SELECT\n    r.\"driver_id\",\n    ra.\"year\",\n    MIN_BY(r.\"constructor_id\", ra.\"round\") AS first_constructor_id,\n    MAX_BY(r.\"constructor_id\", ra.\"round\") AS last_constructor_id,\n    COUNT(DISTINCT ra.\"round\") AS num_unique_rounds\n  FROM F1.F1.RESULTS r\n  JOIN F1.F1.RACES ra ON r.\"race_id\" = ra.\"race_id\"\n  WHERE ra.\"year\" BETWEEN 1950 AND 1959\n  GROUP BY r.\"driver_id\", ra.\"year\"\n  HAVING\n    COUNT(DISTINCT ra.\"round\") >= 2\n    AND MIN_BY(r.\"constructor_id\", ra.\"round\") = MAX_BY(r.\"constructor_id\", ra.\"round\")\n) t\nJOIN F1.F1.DRIVERS d ON t.\"driver_id\" = d.\"driver_id\"\nJOIN F1.F1.CONSTRUCTORS c ON t.first_constructor_id = c.\"constructor_id\"\nORDER BY name, year\n"
    },
    {
        "instance_id":"sf_local355",
        "instruction":"Calculate the overall average first round and average last round of races missed by Formula 1 drivers across all years. Include only drivers who missed fewer than three races in a given year and who switched teams between their appearances immediately before and after their hiatus (i.e., the constructor ID for the race right before their first missed race must be different from the constructor ID for the race right after their last missed race in that year). Do not group results by year; return just the overall averages across the entire dataset.",
        "db_id":"F1",
        "external_knowledge":null,
        "sql":"WITH \"driver_races\" AS (\n  SELECT\n    \"res\".\"driver_id\" AS \"driver_id\",\n    \"r\".\"year\" AS \"year\",\n    \"r\".\"round\" AS \"round\",\n    \"res\".\"constructor_id\" AS \"constructor_id\",\n    LEAD(\"r\".\"round\") OVER (PARTITION BY \"res\".\"driver_id\", \"r\".\"year\" ORDER BY \"r\".\"round\") AS \"next_round\",\n    LEAD(\"res\".\"constructor_id\") OVER (PARTITION BY \"res\".\"driver_id\", \"r\".\"year\" ORDER BY \"r\".\"round\") AS \"next_constructor\"\n  FROM \"F1\".\"F1\".\"RESULTS\" AS \"res\"\n  JOIN \"F1\".\"F1\".\"RACES\" AS \"r\"\n    ON \"res\".\"race_id\" = \"r\".\"race_id\"\n),\n\"hiatus\" AS (\n  SELECT\n    \"driver_id\",\n    \"year\",\n    \"round\",\n    \"constructor_id\",\n    \"next_round\",\n    \"next_constructor\",\n    \"next_round\" - \"round\" - 1 AS \"races_missed\",\n    \"round\" + 1 AS \"first_missed_round\",\n    \"next_round\" - 1 AS \"last_missed_round\"\n  FROM \"driver_races\"\n  WHERE \"next_round\" IS NOT NULL\n    AND \"next_round\" - \"round\" - 1 > 0\n)\nSELECT\n  AVG(\"first_missed_round\") AS \"avg_first_missed_round\",\n  AVG(\"last_missed_round\") AS \"avg_last_missed_round\"\nFROM \"hiatus\"\nWHERE \"races_missed\" < 3\n  AND \"constructor_id\" IS NOT NULL\n  AND \"next_constructor\" IS NOT NULL\n  AND \"constructor_id\" != \"next_constructor\""
    },
    {
        "instance_id":"sf002",
        "instruction":"As of December 31, 2022, list the top 10 active banks with assets exceeding $10 billion, ranked by the highest percentage of uninsured assets, where the percentage is calculated as one minus the value of the '% Insured (Estimated)' variable from quarterly estimates. Provide the names of these banks and their respective percentages of uninsured assets.",
        "db_id":"FINANCE__ECONOMICS",
        "external_knowledge":null,
        "sql":"WITH \"FilteredData\" AS (\n  SELECT\n    \"ID_RSSD\"::NUMBER AS \"ID_RSSD_NUM\",\n    \"VARIABLE\",\n    \"VALUE\"\n  FROM \"FINANCE__ECONOMICS\".\"CYBERSYN\".\"FINANCIAL_INSTITUTION_TIMESERIES\"\n  WHERE \"DATE\" = '2022-12-31'\n    AND \"VARIABLE\" IN ('ASSET', 'ESTINS')\n),\n\"BankFinancials\" AS (\n  SELECT\n    \"ID_RSSD_NUM\",\n    MAX(CASE WHEN \"VARIABLE\" = 'ASSET' THEN \"VALUE\" END) AS \"TotalAssets\",\n    MAX(CASE WHEN \"VARIABLE\" = 'ESTINS' THEN \"VALUE\" END) AS \"InsuredPercentage\"\n  FROM \"FilteredData\"\n  GROUP BY \"ID_RSSD_NUM\"\n)\nSELECT\n  \"E\".\"NAME\",\n  (1 - \"BF\".\"InsuredPercentage\") * 100 AS \"Uninsured_Assets_Percentage\"\nFROM \"FINANCE__ECONOMICS\".\"CYBERSYN\".\"FINANCIAL_INSTITUTION_ENTITIES\" AS \"E\"\nJOIN \"BankFinancials\" AS \"BF\"\n  ON \"E\".\"ID_RSSD\" = \"BF\".\"ID_RSSD_NUM\"\nWHERE\n  \"E\".\"IS_ACTIVE\" = TRUE\n  AND \"BF\".\"TotalAssets\" > 10000000000\n  AND \"BF\".\"InsuredPercentage\" IS NOT NULL\nORDER BY\n  \"Uninsured_Assets_Percentage\" DESC\nLIMIT 10;"
    },
    {
        "instance_id":"sf044",
        "instruction":"What was the percentage change in post-market close prices for the Magnificent 7 tech companies from January 1 to June 30, 2024?",
        "db_id":"FINANCE__ECONOMICS",
        "external_knowledge":null,
        "sql":"WITH \"tickers\" AS (\n    SELECT column1 AS \"TICKER\"\n    FROM (VALUES \n        ('AAPL'),\n        ('MSFT'),\n        ('GOOGL'),\n        ('AMZN'),\n        ('NVDA'),\n        ('META'),\n        ('TSLA')\n    ) AS \"v\"(column1)\n),\n\"filtered\" AS (\n    SELECT\n        \"s\".\"TICKER\",\n        \"s\".\"DATE\",\n        \"s\".\"VALUE\"\n    FROM \"FINANCE__ECONOMICS\".\"CYBERSYN\".\"STOCK_PRICE_TIMESERIES\" AS \"s\"\n    INNER JOIN \"tickers\" AS \"t\"\n        ON \"s\".\"TICKER\" = \"t\".\"TICKER\"\n    WHERE \"s\".\"VARIABLE\" = 'post-market_close'\n        AND \"s\".\"DATE\" BETWEEN '2024-01-01' AND '2024-06-30'\n),\n\"start_prices\" AS (\n    SELECT\n        \"TICKER\",\n        \"DATE\" AS \"START_DATE\",\n        \"VALUE\" AS \"START_VALUE\"\n    FROM (\n        SELECT\n            \"TICKER\",\n            \"DATE\",\n            \"VALUE\",\n            ROW_NUMBER() OVER (PARTITION BY \"TICKER\" ORDER BY \"DATE\") AS \"RN\"\n        FROM \"filtered\"\n    )\n    WHERE \"RN\" = 1\n),\n\"end_prices\" AS (\n    SELECT\n        \"TICKER\",\n        \"DATE\" AS \"END_DATE\",\n        \"VALUE\" AS \"END_VALUE\"\n    FROM (\n        SELECT\n            \"TICKER\",\n            \"DATE\",\n            \"VALUE\",\n            ROW_NUMBER() OVER (PARTITION BY \"TICKER\" ORDER BY \"DATE\" DESC) AS \"RN\"\n        FROM \"filtered\"\n    )\n    WHERE \"RN\" = 1\n),\n\"split_candidates\" AS (\n    SELECT\n        \"TICKER\",\n        \"DATE\",\n        \"PREV_VALUE\" \/ NULLIF(\"VALUE\", 0) AS \"RATIO\"\n    FROM (\n        SELECT\n            \"TICKER\",\n            \"DATE\",\n            \"VALUE\",\n            LAG(\"VALUE\") OVER (PARTITION BY \"TICKER\" ORDER BY \"DATE\") AS \"PREV_VALUE\"\n        FROM \"filtered\"\n    )\n    WHERE \"PREV_VALUE\" IS NOT NULL\n),\n\"split_factors\" AS (\n    SELECT\n        \"TICKER\",\n        EXP(SUM(LN(ROUND(\"RATIO\")))) AS \"TOTAL_FACTOR\"\n    FROM \"split_candidates\"\n    WHERE \"RATIO\" >= 1.5\n      AND ABS(\"RATIO\" - ROUND(\"RATIO\")) <= 0.2\n    GROUP BY \"TICKER\"\n)\nSELECT\n    \"sp\".\"TICKER\",\n    \"sp\".\"START_DATE\",\n    \"ep\".\"END_DATE\",\n    ROUND(\"sp\".\"START_VALUE\" \/ COALESCE(\"sf\".\"TOTAL_FACTOR\", 1), 2) AS \"ADJUSTED_START_PRICE\",\n    ROUND(\"ep\".\"END_VALUE\", 2) AS \"END_PRICE\",\n    ROUND(((\"ep\".\"END_VALUE\" - (\"sp\".\"START_VALUE\" \/ COALESCE(\"sf\".\"TOTAL_FACTOR\", 1))) \/ (\"sp\".\"START_VALUE\" \/ COALESCE(\"sf\".\"TOTAL_FACTOR\", 1))) * 100, 2) AS \"PERCENT_CHANGE\"\nFROM \"start_prices\" AS \"sp\"\nINNER JOIN \"end_prices\" AS \"ep\"\n    ON \"sp\".\"TICKER\" = \"ep\".\"TICKER\"\nLEFT JOIN \"split_factors\" AS \"sf\"\n    ON \"sp\".\"TICKER\" = \"sf\".\"TICKER\"\nORDER BY \"sp\".\"TICKER\";"
    },
    {
        "instance_id":"sf012",
        "instruction":"Using data from the FEMA National Flood Insurance Program Claim Index, for each year from 2010 through 2019, what were the total building damage amounts and total contents damage amounts reported under the National Flood Insurance Program for the NFIP community named 'City Of New York,' grouped by each year of loss?",
        "db_id":"WEATHER__ENVIRONMENT",
        "external_knowledge":null,
        "sql":"-- Question: For each loss year from 2010 through 2019,\n-- return the total building damage amounts and total contents damage amounts\n-- for NFIP claims where the NFIP community name is exactly 'City Of New York'.\n--\n-- Assumptions \/ reasoning (per guidelines):\n-- 1. Use only the FEMA_NATIONAL_FLOOD_INSURANCE_PROGRAM_CLAIM_INDEX table because\n--    it already contains the required fields (NFIP community name, date of loss,\n--    building damage amount, contents damage amount).\n-- 2. Filter exactly on \"NFIP_COMMUNITY_NAME\" = 'City Of New York' (case-sensitive\n--    equality as in probes) without adding extra communities.\n-- 3. Limit to years 2010\u20132019 inclusive via EXTRACT(year FROM \"DATE_OF_LOSS\").\n-- 4. Aggregate with SUM which automatically ignores NULLs (no additional NULL handling).\n-- 5. Group by the extracted year and order ascending for readability.\n\nSELECT\n    EXTRACT(year FROM \"DATE_OF_LOSS\") AS \"YEAR_OF_LOSS\",\n    SUM(\"BUILDING_DAMAGE_AMOUNT\")  AS \"total_building_damage_amount\",\n    SUM(\"CONTENTS_DAMAGE_AMOUNT\")  AS \"total_contents_damage_amount\"\nFROM \"WEATHER__ENVIRONMENT\".\"CYBERSYN\".\"FEMA_NATIONAL_FLOOD_INSURANCE_PROGRAM_CLAIM_INDEX\"\nWHERE \"NFIP_COMMUNITY_NAME\" = 'City Of New York'\n  AND \"DATE_OF_LOSS\" IS NOT NULL\n  AND EXTRACT(year FROM \"DATE_OF_LOSS\") BETWEEN 2010 AND 2019\nGROUP BY EXTRACT(year FROM \"DATE_OF_LOSS\")\nORDER BY \"YEAR_OF_LOSS\";"
    },
    {
        "instance_id":"sf018",
        "instruction":"Examine user engagement with push notifications within a specified one-hour window on June 1, 2023.",
        "db_id":"BRAZE_USER_EVENT_DEMO_DATASET",
        "external_knowledge":"PushNotificationAnalysis.md",
        "sql":"WITH all_events AS (\n  -- SEND events\n  SELECT \n    \"APP_GROUP_ID\",\n    \"CAMPAIGN_ID\",\n    \"USER_ID\",\n    COALESCE(\"MESSAGE_VARIATION_ID\", \"MESSAGE_VARIATION_API_ID\") AS \"MESSAGE_VARIATION_ID_USED\",\n    \"PLATFORM\",\n    \"AD_TRACKING_ENABLED\",\n    NULL AS \"CARRIER\",\n    NULL AS \"BROWSER\",\n    NULL AS \"DEVICE_MODEL\",\n    'send' AS event_type\n  FROM BRAZE_USER_EVENT_DEMO_DATASET.PUBLIC.USERS_MESSAGES_PUSHNOTIFICATION_SEND_VIEW\n  WHERE \"TIME\" BETWEEN 1685606400 AND 1685610000\n  \n  UNION ALL\n  \n  -- BOUNCE events\n  SELECT \n    \"APP_GROUP_ID\",\n    \"CAMPAIGN_ID\",\n    \"USER_ID\",\n    COALESCE(\"MESSAGE_VARIATION_ID\", \"MESSAGE_VARIATION_API_ID\") AS \"MESSAGE_VARIATION_ID_USED\",\n    \"PLATFORM\",\n    \"AD_TRACKING_ENABLED\",\n    NULL AS \"CARRIER\",\n    NULL AS \"BROWSER\",\n    NULL AS \"DEVICE_MODEL\",\n    'bounce' AS event_type\n  FROM BRAZE_USER_EVENT_DEMO_DATASET.PUBLIC.USERS_MESSAGES_PUSHNOTIFICATION_BOUNCE_VIEW\n  WHERE \"TIME\" BETWEEN 1685606400 AND 1685610000\n  \n  UNION ALL\n  \n  -- OPEN events\n  SELECT \n    \"APP_GROUP_ID\",\n    \"CAMPAIGN_ID\",\n    \"USER_ID\",\n    COALESCE(\"MESSAGE_VARIATION_ID\", \"MESSAGE_VARIATION_API_ID\") AS \"MESSAGE_VARIATION_ID_USED\",\n    \"PLATFORM\",\n    \"AD_TRACKING_ENABLED\",\n    \"CARRIER\",\n    \"BROWSER\",\n    \"DEVICE_MODEL\",\n    'open' AS event_type\n  FROM BRAZE_USER_EVENT_DEMO_DATASET.PUBLIC.USERS_MESSAGES_PUSHNOTIFICATION_OPEN_VIEW\n  WHERE \"TIME\" BETWEEN 1685606400 AND 1685610000\n  \n  UNION ALL\n  \n  -- INFLUENCEDOPEN events\n  SELECT \n    \"APP_GROUP_ID\",\n    \"CAMPAIGN_ID\",\n    \"USER_ID\",\n    COALESCE(\"MESSAGE_VARIATION_ID\", \"MESSAGE_VARIATION_API_ID\") AS \"MESSAGE_VARIATION_ID_USED\",\n    \"PLATFORM\",\n    NULL AS \"AD_TRACKING_ENABLED\",\n    \"CARRIER\",\n    \"BROWSER\",\n    \"DEVICE_MODEL\",\n    'influenced_open' AS event_type\n  FROM BRAZE_USER_EVENT_DEMO_DATASET.PUBLIC.USERS_MESSAGES_PUSHNOTIFICATION_INFLUENCEDOPEN_VIEW\n  WHERE \"TIME\" BETWEEN 1685606400 AND 1685610000\n)\n\nSELECT \n  \"APP_GROUP_ID\",\n  \"CAMPAIGN_ID\",\n  \"USER_ID\",\n  \"MESSAGE_VARIATION_ID_USED\" AS \"MESSAGE_VARIATION_ID\",\n  \"PLATFORM\",\n  \"AD_TRACKING_ENABLED\",\n  \"CARRIER\",\n  \"BROWSER\",\n  \"DEVICE_MODEL\",\n  SUM(CASE WHEN event_type = 'send' THEN 1 ELSE 0 END) AS \"push_notification_sends\",\n  COUNT(DISTINCT CASE WHEN event_type = 'send' THEN \"USER_ID\" ELSE NULL END) AS \"unique_push_notification_sends\",\n  SUM(CASE WHEN event_type = 'bounce' THEN 1 ELSE 0 END) AS \"push_notification_bounced\",\n  COUNT(DISTINCT CASE WHEN event_type = 'bounce' THEN \"USER_ID\" ELSE NULL END) AS \"unique_push_notification_bounced\",\n  SUM(CASE WHEN event_type = 'open' THEN 1 ELSE 0 END) AS \"push_notification_open\",\n  COUNT(DISTINCT CASE WHEN event_type = 'open' THEN \"USER_ID\" ELSE NULL END) AS \"unique_push_notification_opened\",\n  SUM(CASE WHEN event_type = 'influenced_open' THEN 1 ELSE 0 END) AS \"push_notification_influenced_open\",\n  COUNT(DISTINCT CASE WHEN event_type = 'influenced_open' THEN \"USER_ID\" ELSE NULL END) AS \"unique_push_notification_influenced_open\"\nFROM all_events\nGROUP BY \n  \"APP_GROUP_ID\",\n  \"CAMPAIGN_ID\",\n  \"USER_ID\",\n  \"MESSAGE_VARIATION_ID_USED\",\n  \"PLATFORM\",\n  \"AD_TRACKING_ENABLED\",\n  \"CARRIER\",\n  \"BROWSER\",\n  \"DEVICE_MODEL\"\nORDER BY \n  \"APP_GROUP_ID\",\n  \"CAMPAIGN_ID\",\n  \"USER_ID\"\nLIMIT 20"
    },
    {
        "instance_id":"sf040",
        "instruction":"Find the top 10 northernmost addresses in Florida's largest zip code area. What are their address numbers, street names, and types?",
        "db_id":"US_ADDRESSES__POI",
        "external_knowledge":null,
        "sql":"WITH \"FL_ZIPS\" AS (\n    SELECT\n        \"RELATED_GEO_ID\" AS \"ZIP_GEO_ID\"\n    FROM \"US_ADDRESSES__POI\".\"CYBERSYN\".\"GEOGRAPHY_RELATIONSHIPS\"\n    WHERE \"GEO_ID\" = 'geoId\/12'\n      AND \"RELATIONSHIP_TYPE\" = 'Contains'\n      AND \"RELATED_LEVEL\" = 'CensusZipCodeTabulationArea'\n),\n\"ZIP_GEOMS\" AS (\n    SELECT\n        f.\"ZIP_GEO_ID\" AS \"GEO_ID\",\n        TO_GEOGRAPHY(c.\"VALUE\") AS \"GEOM\"\n    FROM \"FL_ZIPS\" f\n    JOIN \"US_ADDRESSES__POI\".\"CYBERSYN\".\"GEOGRAPHY_CHARACTERISTICS\" c\n      ON c.\"GEO_ID\" = f.\"ZIP_GEO_ID\"\n     AND c.\"RELATIONSHIP_TYPE\" = 'coordinates_wkt'\n     AND c.\"VALUE\" IS NOT NULL\n),\n\"ZIP_AREAS\" AS (\n    SELECT\n        \"GEO_ID\",\n        ST_AREA(ST_UNION_AGG(\"GEOM\")) AS \"AREA_M2\"\n    FROM \"ZIP_GEOMS\"\n    GROUP BY \"GEO_ID\"\n),\n\"LARGEST_FL_ZIP\" AS (\n    SELECT \"GEO_ID\" AS \"ZIP_GEO_ID\"\n    FROM \"ZIP_AREAS\"\n    ORDER BY \"AREA_M2\" DESC\n    LIMIT 1\n)\nSELECT\n    a.\"NUMBER\" AS \"ADDRESS_NUMBER\",\n    a.\"STREET\" AS \"STREET_NAME\",\n    a.\"STREET_TYPE\" AS \"STREET_TYPE\"\nFROM \"US_ADDRESSES__POI\".\"CYBERSYN\".\"US_ADDRESSES\" a\nJOIN \"LARGEST_FL_ZIP\" z\n  ON a.\"ID_ZIP\" = z.\"ZIP_GEO_ID\"\nWHERE a.\"STATE\" = 'FL'\n  AND a.\"LATITUDE\" IS NOT NULL\n  AND a.\"NUMBER\" IS NOT NULL\n  AND REGEXP_LIKE(a.\"NUMBER\", '^[0-9]+$')\n  AND a.\"STREET\" IS NOT NULL AND TRIM(a.\"STREET\") != ''\n  AND a.\"STREET_TYPE\" IS NOT NULL AND TRIM(a.\"STREET_TYPE\") != ''\nORDER BY a.\"LATITUDE\" DESC, a.\"LONGITUDE\" DESC\nLIMIT 10;"
    },
    {
        "instance_id":"sf011",
        "instruction":"Determine the population distribution within each block group relative to its census tract in New York State using 2021 ACS data. Include block group ID, census value, state county tract ID, total tract population, and the population ratio of each block group.",
        "db_id":"CENSUS_GALAXY__ZIP_CODE_TO_BLOCK_GROUP_SAMPLE",
        "external_knowledge":null,
        "sql":"WITH BlockGroupAndTractPop AS (\n  SELECT\n    T1.\"BlockGroupID\",\n    T1.\"StateCountyTractID\",\n    T2.\"CensusValue\" AS \"BlockGroupPopulation\",\n    SUM(T2.\"CensusValue\") OVER (PARTITION BY T1.\"StateCountyTractID\") AS \"TractPopulation\"\n  FROM \"CENSUS_GALAXY__ZIP_CODE_TO_BLOCK_GROUP_SAMPLE\".\"PUBLIC\".\"Dim_CensusGeography\" AS T1\n  JOIN \"CENSUS_GALAXY__ZIP_CODE_TO_BLOCK_GROUP_SAMPLE\".\"PUBLIC\".\"Fact_CensusValues_ACS2021\" AS T2\n    ON T1.\"BlockGroupID\" = T2.\"BlockGroupID\"\n  WHERE\n    T1.\"StateName\" = 'New York' AND T2.\"MetricID\" = 'B01003_001E'\n)\nSELECT\n  \"BlockGroupID\",\n  \"BlockGroupPopulation\" AS \"census_value\",\n  \"StateCountyTractID\",\n  \"TractPopulation\" AS \"total_tract_population\",\n  \"BlockGroupPopulation\" \/ \"TractPopulation\" AS \"population_ratio\"\nFROM BlockGroupAndTractPop\nWHERE \"TractPopulation\" > 0"
    },
    {
        "instance_id":"sf014",
        "instruction":"What is the New York State ZIP code with the highest number of commuters traveling over one hour, according to 2021 ACS data? Include the zip code, the total commuters, state benchmark for this duration, and state population.",
        "db_id":"CENSUS_GALAXY__AIML_MODEL_DATA_ENRICHMENT_SAMPLE",
        "external_knowledge":null,
        "sql":"WITH ZipCommuters AS (\n    SELECT\n        f.\"ZipCode\",\n        SUM(f.\"CensusValueByZip\") AS \"TotalCommutersOver1Hour\"\n    FROM CENSUS_GALAXY__AIML_MODEL_DATA_ENRICHMENT_SAMPLE.PUBLIC.\"Fact_CensusValues_ACS2021_ByZip\" AS f\n    JOIN CENSUS_GALAXY__AIML_MODEL_DATA_ENRICHMENT_SAMPLE.PUBLIC.\"LU_GeographyExpanded\" AS l ON f.\"ZipCode\" = l.\"ZipCode\"\n    WHERE l.\"PreferredStateAbbrev\" = 'NY'\n      AND f.\"MetricID\" IN ('B08303_012E', 'B08303_013E')\n    GROUP BY f.\"ZipCode\"\n),\nStateBenchmark AS (\n    SELECT\n        SUM(\"StateBenchmarkValue\") AS \"StateBenchmarkOver1Hour\"\n    FROM CENSUS_GALAXY__AIML_MODEL_DATA_ENRICHMENT_SAMPLE.PUBLIC.\"Fact_StateBenchmark_ACS2021\"\n    WHERE \"StateAbbrev\" = 'NY'\n      AND \"MetricID\" IN ('B08303_012E', 'B08303_013E')\n),\nStatePopulation AS (\n    SELECT\n        MAX(\"TotalStatePopulation\") AS \"StatePopulation\"\n    FROM CENSUS_GALAXY__AIML_MODEL_DATA_ENRICHMENT_SAMPLE.PUBLIC.\"Fact_StateBenchmark_ACS2021\"\n    WHERE \"StateAbbrev\" = 'NY'\n)\nSELECT\n    zc.\"ZipCode\" AS \"zip_code\",\n    zc.\"TotalCommutersOver1Hour\" AS \"total_commuters_over_one_hour\",\n    sb.\"StateBenchmarkOver1Hour\" AS \"state_benchmark_for_this_duration\",\n    sp.\"StatePopulation\" AS \"state_population\"\nFROM ZipCommuters zc\nCROSS JOIN StateBenchmark sb\nCROSS JOIN StatePopulation sp\nORDER BY \"total_commuters_over_one_hour\" DESC\nLIMIT 1;"
    }
]